{"id":"meld-eq0","title":"Meld v1: Multi-Model Plan Convergence Tool","description":"# Meld v1 Epic\n\n## Vision\nMeld is a Python CLI tool that orchestrates multi-model plan convergence. Given a task description, it uses Claude as the \"Melder\" to generate an initial plan, then collects parallel feedback from three AI advisors (Claude CLI, Gemini CLI, Codex CLI), synthesizes the feedback, and iterates until convergence or max rounds.\n\n## Why This Matters\n- **Model Diversity**: Different AI models have different strengths, blind spots, and perspectives. Combining their feedback produces more robust plans than any single model.\n- **Convergence as Quality Signal**: When multiple models agree a plan is complete, confidence is higher. Disagreement surfaces risks and edge cases.\n- **Human-in-the-Loop**: The DEFERRED items mechanism ensures humans make final calls on contested decisions.\n\n## Key Design Goals\n1. **Advisor Adapter Layer**: Encapsulate per-CLI quirks (flags, output parsing) so CLI drift doesn't break the core loop\n2. **Session Persistence**: Crash-safe artifacts enable debugging, `--resume`, and meet PRD requirement \"no data loss if interrupted\"\n3. **Structured Feedback**: Lightly structured advisor responses improve synthesis quality and convergence accuracy\n4. **Real-time Visibility**: Textual TUI shows all four models working in parallel with streaming output\n\n## Architecture Summary\n- **Language**: Python 3.10+ (required for Textual/Rich; modern async support)\n- **TUI Framework**: Textual (purpose-built for terminal apps; handles layout, streaming, async natively)\n- **CLI Invocation**: asyncio.create_subprocess_exec (non-blocking parallel execution; stdout streaming)\n- **Output Format**: Markdown + optional JSON (Markdown for humans, JSON for CI/scripting)\n- **Configuration**: CLI flags only (no config files for v1; keeps it simple)\n\n## Implementation Phases\n1. **Foundation**: Project scaffolding, CLI, session manager, provider interface\n2. **Core Logic**: All provider adapters, Melder, Advisor pool, convergence detection\n3. **TUI**: Textual app with 4-panel layout, streaming, status indicators\n4. **Polish**: Output formatter, JSON output, resume, graceful degradation\n\n## Success Criteria\n- All CLIs invoked successfully when available\n- Preflight detects missing/broken CLIs with actionable messages\n- TUI renders without flicker with throttled updates\n- Convergence detection: no false positives; OPEN_ITEMS \u003e 0 always blocks\n- Graceful degradation: works with 2/3 advisors\n- Session artifacts enable crash recovery via --resume\n\n## Dependencies\n### External (user must have installed)\n- `claude` CLI - Anthropic's Claude Code CLI (authenticated)\n- `gemini` CLI - Google's Gemini CLI (authenticated)\n- `codex` CLI - OpenAI's Codex CLI (authenticated)\n- Python 3.10+\n\n### Python Packages\n- `textual\u003e=0.50.0` - TUI framework\n- `rich\u003e=13.0.0` - Terminal formatting (Textual dependency)\n\n## Deferred to v1.1\n- Event-driven architecture (event bus for TUI/logging separation)\n- Context size guardrails (--max-context-chars, PRD summarization)\n- Semantic oscillation detection\n- Advisor specialization hints\n- Cost tracking and token usage estimation","status":"open","priority":1,"issue_type":"epic","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T22:58:31.635092-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T22:58:31.635092-06:00","labels":["epic","python","tui","v1"]}
{"id":"meld-eq0.1","title":"Task 1: Project Setup \u0026 CLI Foundation","description":"# Task 1: Project Setup \u0026 CLI Foundation\n\n## Overview\nEstablish the Python project structure and implement the CLI entry point with all input modes. This is the foundational layer that everything else builds upon.\n\n## Why This Task Exists\nBefore any business logic can be implemented, we need:\n1. A properly configured Python project with modern tooling (pyproject.toml, proper packaging)\n2. A CLI interface that handles all input modes the user might want\n3. Directory structure that matches the architectural vision\n\n## Design Decisions Made\n\n### Project Structure Choice\nUsing flat `meld/` package structure rather than `src/meld/` because:\n- Simpler for a standalone CLI tool\n- Direct import paths (`from meld.cli import ...`)\n- Standard pattern for single-package projects\n\n### CLI Framework: argparse (stdlib)\nChose argparse over click/typer because:\n- Zero additional dependencies\n- Sufficient for our needs (2 subcommands, ~15 flags)\n- Better control over help formatting\n- No magic decorators to understand\n\n### Input Mode Priority (when multiple provided)\n1. Positional arg (highest priority)\n2. --file flag\n3. stdin (auto-detected when piped)\n\nRationale: Explicit arguments should win over implicit (stdin).\n\n## Technical Requirements\n\n### Directory Structure\n```\nmeld/\n├── __init__.py          # Version, package metadata\n├── __main__.py          # Entry point for python -m meld\n├── cli.py               # argparse setup, subcommands\n├── orchestrator.py      # Main loop (stub for now)\n├── melder.py            # Claude integration (stub)\n├── advisors.py          # Advisor pool (stub)\n├── session.py           # Session management (stub)\n├── preflight.py         # Preflight checks (stub)\n├── tui.py               # Textual app (stub)\n├── output.py            # Output formatting (stub)\n├── signals.py           # Signal handling (stub)\n├── prompts.py           # Prompt templates (stub)\n├── providers/           # Provider adapters\n│   ├── __init__.py\n│   ├── base.py          # Abstract base (stub)\n│   ├── claude.py        # Claude adapter (stub)\n│   ├── gemini.py        # Gemini adapter (stub)\n│   └── openai.py        # OpenAI adapter (stub)\n└── models.py            # Data models/types\n```\n\n### pyproject.toml Requirements\n- Build system: hatchling (modern, fast, minimal config)\n- Python: \u003e=3.10\n- Dependencies: textual\u003e=0.50.0, rich\u003e=13.0.0\n- Entry point: `meld = meld.cli:main`\n- Dev dependencies: pytest, pytest-asyncio, ruff\n\n### CLI Interface\n```bash\n# Main command with task\nmeld run \"Build an auth system\"\n\n# Read from file\nmeld run --file task.txt\n\n# Pipe from stdin\necho \"Build auth\" | meld run\n\n# With PRD context\nmeld run \"Build auth\" --prd requirements.md\n\n# Run options\nmeld run \"Task\" --rounds 5 --timeout 600\nmeld run \"Task\" --output plan.md --json-output summary.json\nmeld run \"Task\" --quiet  # No TUI, stdout output\nmeld run \"Task\" --verbose  # Include raw advisor outputs\n\n# Session management\nmeld run --resume 2026-01-16T02-47-17Z-abc123\nmeld run \"Task\" --run-dir ./custom-runs/\n\n# Skip preflight\nmeld run \"Task\" --skip-preflight\n\n# Doctor subcommand\nmeld doctor\n```\n\n## Acceptance Criteria\n- [ ] `pip install -e .` works from project root\n- [ ] `meld --help` shows usage with all flags documented\n- [ ] `meld run \"task\"` accepts positional argument\n- [ ] `meld run --file task.txt` reads from file\n- [ ] `echo \"task\" | meld run` reads from stdin\n- [ ] `meld run --prd doc.md` loads PRD content\n- [ ] `meld doctor` runs (even if stub)\n- [ ] All flags parse correctly (no errors)\n- [ ] Version displayed with `meld --version`\n\n## Notes for Implementation\n- Start with just the CLI parsing, not the actual execution\n- Stubs should print \"Not implemented yet\" and exit cleanly\n- Use type hints throughout (Python 3.10+ syntax)\n- Include docstrings for public functions","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T22:59:44.858502-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T22:59:44.858502-06:00","labels":["cli","foundation","phase-1"],"dependencies":[{"issue_id":"meld-eq0.1","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T22:59:44.864126-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.1.1","title":"1.1: Create pyproject.toml with modern Python packaging","description":"# Subtask 1.1: Create pyproject.toml\n\n## What to Create\nA modern Python project configuration using pyproject.toml with hatchling build system.\n\n## Why hatchling?\n- Modern, fast, PEP 517/518 compliant\n- Minimal configuration needed\n- Good defaults for package discovery\n- Active development and community support\n\n## File Content Requirements\n\n```toml\n[build-system]\nrequires = [\"hatchling\"]\nbuild-backend = \"hatchling.build\"\n\n[project]\nname = \"meld\"\nversion = \"0.1.0\"\ndescription = \"Multi-model plan convergence orchestrator\"\nreadme = \"README.md\"\nlicense = \"MIT\"\nrequires-python = \"\u003e=3.10\"\nauthors = [\n    { name = \"Your Name\", email = \"you@example.com\" }\n]\nkeywords = [\"ai\", \"cli\", \"planning\", \"convergence\"]\nclassifiers = [\n    \"Development Status :: 3 - Alpha\",\n    \"Environment :: Console\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Software Development :: Quality Assurance\",\n]\ndependencies = [\n    \"textual\u003e=0.50.0\",\n    \"rich\u003e=13.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest\u003e=7.0\",\n    \"pytest-asyncio\u003e=0.21\",\n    \"ruff\u003e=0.1.0\",\n]\n\n[project.scripts]\nmeld = \"meld.cli:main\"\n\n[project.urls]\nHomepage = \"https://github.com/yourname/meld\"\nDocumentation = \"https://github.com/yourname/meld#readme\"\nRepository = \"https://github.com/yourname/meld\"\n\n[tool.hatch.build.targets.wheel]\npackages = [\"meld\"]\n\n[tool.ruff]\nline-length = 100\ntarget-version = \"py310\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"UP\", \"B\"]\n\n[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\n```\n\n## Acceptance Criteria\n- [ ] File exists at project root\n- [ ] `pip install -e .` succeeds\n- [ ] `pip install -e \".[dev]\"` installs dev dependencies\n- [ ] `meld` command is available after install\n\n## Considerations\n- Version starts at 0.1.0 (pre-release, unstable API)\n- MIT license for maximum flexibility\n- Dev dependencies separate to keep production install slim","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:00:09.253817-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:00:09.253817-06:00","labels":["foundation","packaging"],"dependencies":[{"issue_id":"meld-eq0.1.1","depends_on_id":"meld-eq0.1","type":"parent-child","created_at":"2026-01-15T23:00:09.25725-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.1.2","title":"1.2: Create directory structure with stub modules","description":"# Subtask 1.2: Create Directory Structure\n\n## What to Create\nThe complete directory structure with stub implementations for all modules.\n\n## Directory Layout\n```\nmeld/\n├── __init__.py          # Package init with version\n├── __main__.py          # python -m meld support\n├── cli.py               # CLI entry point (stub)\n├── orchestrator.py      # Main loop\n├── melder.py            # Claude Melder\n├── advisors.py          # Advisor pool\n├── session.py           # Session management\n├── preflight.py         # Preflight checks\n├── tui.py               # Textual TUI\n├── output.py            # Output formatting\n├── signals.py           # Signal handling\n├── prompts.py           # Prompt templates\n├── models.py            # Data classes/types\n└── providers/\n    ├── __init__.py\n    ├── base.py          # Abstract base adapter\n    ├── claude.py        # Claude adapter\n    ├── gemini.py        # Gemini adapter\n    └── openai.py        # OpenAI/Codex adapter\n\ntests/\n├── __init__.py\n├── test_cli.py          # CLI tests (stub)\n└── conftest.py          # pytest fixtures\n```\n\n## Stub Implementation Pattern\nEach module should have a docstring explaining its purpose and any placeholder code:\n\n```python\n\"\"\"\nmeld/orchestrator.py - Main convergence loop.\n\nThis module coordinates the Melder and Advisors through\nthe plan → feedback → synthesis cycle until convergence\nor max rounds reached.\n\"\"\"\n\nasync def run_convergence_loop(task: str, prd: str | None = None) -\u003e str:\n    \"\"\"Run the main convergence loop.\n    \n    Args:\n        task: The task description to plan for\n        prd: Optional PRD content for context\n        \n    Returns:\n        The final converged plan as markdown\n    \"\"\"\n    raise NotImplementedError(\"Orchestrator not yet implemented\")\n```\n\n## __init__.py Content\n```python\n\"\"\"Meld: Multi-model plan convergence orchestrator.\"\"\"\n\n__version__ = \"0.1.0\"\n```\n\n## __main__.py Content\n```python\n\"\"\"Allow running as python -m meld.\"\"\"\nfrom meld.cli import main\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Why All Stubs Upfront?\n1. **Import validation**: Ensures module structure is correct\n2. **IDE support**: Autocomplete works immediately\n3. **Parallel development**: Multiple developers can work on different modules\n4. **Test scaffolding**: Tests can be written against interfaces\n\n## Acceptance Criteria\n- [ ] All directories exist\n- [ ] All .py files exist with docstrings\n- [ ] `from meld import __version__` works\n- [ ] `python -m meld` runs (even if just prints help)\n- [ ] `from meld.providers.base import ...` imports work\n- [ ] No circular import issues","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:00:39.684833-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:00:39.684833-06:00","labels":["foundation","structure"],"dependencies":[{"issue_id":"meld-eq0.1.2","depends_on_id":"meld-eq0.1","type":"parent-child","created_at":"2026-01-15T23:00:39.68786-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.1.2","depends_on_id":"meld-eq0.1.1","type":"blocks","created_at":"2026-01-15T23:41:42.758331-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.1.3","title":"1.3: Implement CLI argument parsing with argparse","description":"# Subtask 1.3: CLI Argument Parsing\n\n## What to Implement\nFull CLI argument parsing in `meld/cli.py` using argparse.\n\n## CLI Structure\n\n### Subcommands\n1. `run` (default) - Execute a meld session\n2. `doctor` - Check CLI availability and auth\n\n### Run Subcommand Flags\n\n| Flag | Type | Default | Description |\n|------|------|---------|-------------|\n| `TASK` | positional | - | Task description |\n| `--file`, `-f` | str | - | Read task from file |\n| `--prd` | str | - | Load PRD file for context |\n| `--rounds` | int | 5 | Maximum iteration rounds |\n| `--timeout` | int | 600 | Per-advisor timeout (seconds) |\n| `--output`, `-o` | str | - | Write final plan to file |\n| `--json-output` | str | - | Write JSON summary to file |\n| `--quiet`, `-q` | bool | False | No TUI, final plan to stdout |\n| `--verbose` | bool | False | Include raw advisor outputs |\n| `--resume` | str | - | Resume from run ID |\n| `--run-dir` | str | .meld/runs/ | Session artifact directory |\n| `--skip-preflight` | bool | False | Skip CLI validation |\n\n### Implementation Pattern\n\n```python\nimport argparse\nimport sys\nfrom meld import __version__\n\ndef create_parser() -\u003e argparse.ArgumentParser:\n    parser = argparse.ArgumentParser(\n        prog=\"meld\",\n        description=\"Multi-model plan convergence orchestrator\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog=\"\"\"\nExamples:\n  meld run \"Build an authentication system\"\n  meld run --file task.txt --prd requirements.md\n  echo \"Build auth\" | meld run\n  meld doctor\n\"\"\"\n    )\n    parser.add_argument(\"--version\", action=\"version\", version=f\"%(prog)s {__version__}\")\n    \n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Available commands\")\n    \n    # Run subcommand\n    run_parser = subparsers.add_parser(\"run\", help=\"Execute a meld session\")\n    # ... add all flags\n    \n    # Doctor subcommand\n    doctor_parser = subparsers.add_parser(\"doctor\", help=\"Check CLI availability\")\n    \n    return parser\n\ndef main() -\u003e int:\n    parser = create_parser()\n    args = parser.parse_args()\n    \n    # Handle default command (run if no subcommand given)\n    if args.command is None:\n        args.command = \"run\"\n    \n    # Route to appropriate handler\n    if args.command == \"run\":\n        return run_command(args)\n    elif args.command == \"doctor\":\n        return doctor_command(args)\n    \n    return 0\n```\n\n## Input Mode Logic\n\n```python\ndef get_task_input(args) -\u003e str:\n    \"\"\"Get task from args, file, or stdin (in priority order).\"\"\"\n    # 1. Positional argument wins\n    if hasattr(args, 'task') and args.task:\n        return args.task\n    \n    # 2. File flag\n    if hasattr(args, 'file') and args.file:\n        with open(args.file, 'r') as f:\n            return f.read().strip()\n    \n    # 3. Stdin if piped\n    if not sys.stdin.isatty():\n        return sys.stdin.read().strip()\n    \n    # 4. No input provided\n    raise ValueError(\"No task provided. Use positional arg, --file, or pipe.\")\n```\n\n## Validation Rules\n1. `--resume` is mutually exclusive with task input\n2. `--rounds` must be \u003e= 1 and \u003c= 20\n3. `--timeout` must be \u003e= 30 and \u003c= 3600\n4. `--output` and `--json-output` directories must exist\n\n## Acceptance Criteria\n- [ ] `meld --help` shows all options\n- [ ] `meld run --help` shows run-specific options\n- [ ] `meld doctor --help` shows doctor info\n- [ ] Positional task argument works\n- [ ] `--file` reads task from file\n- [ ] Stdin detection works when piped\n- [ ] All flags parse without error\n- [ ] Invalid flag combinations rejected with clear error\n\n## Notes\n- Use argparse's built-in validation where possible\n- Custom validation in handler functions for complex rules\n- Help text should be concise but complete","notes":"## ADDITIONAL REQUIREMENT: --no-save Flag (FR8)\n\nPRD Requirement FR8: '--no-save: do not write run artifacts to disk (disables resume)'\n\n### Flag Specification\n```\n--no-save    Do not persist session artifacts to disk\n             Disables: session directory creation, artifact writing, resume capability\n             Use for: sensitive tasks where you don't want local state\n             Mutually exclusive with: --resume\n```\n\n### Implementation Addition\n```python\nparser.add_argument(\n    '--no-save',\n    action='store_true',\n    default=False,\n    help='Do not persist session artifacts to disk (disables resume)',\n)\n\n# Validation\nif args.no_save and args.resume:\n    parser.error('--no-save and --resume are mutually exclusive')\n```\n\n### Acceptance Criteria Addition\n- [ ] --no-save flag parsed correctly\n- [ ] Mutually exclusive with --resume (validation error if both)\n- [ ] When set, SessionManager operates in memory-only mode\n- [ ] No .meld/runs directory created when --no-save is set","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:01:16.138441-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:04:09.338448-06:00","labels":["argparse","cli","foundation"],"dependencies":[{"issue_id":"meld-eq0.1.3","depends_on_id":"meld-eq0.1","type":"parent-child","created_at":"2026-01-15T23:01:16.14204-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.1.3","depends_on_id":"meld-eq0.1.2","type":"blocks","created_at":"2026-01-15T23:41:42.837124-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.1.4","title":"1.4: Implement input mode handling (arg/file/stdin/PRD)","description":"# Subtask 1.4: Input Mode Handling\n\n## What to Implement\nRobust handling of all input modes: positional argument, file, stdin, and PRD loading.\n\n## Input Priority\nWhen multiple sources are available:\n1. **Positional arg** (explicit intent)\n2. **--file** (explicit intent)\n3. **stdin** (implicit, only if piped)\n\n## Implementation\n\n```python\nimport sys\nfrom pathlib import Path\n\nclass InputError(Exception):\n    \"\"\"Raised when task input cannot be obtained.\"\"\"\n    pass\n\ndef get_task_input(args) -\u003e str:\n    \"\"\"\n    Get task from args, file, or stdin.\n    \n    Priority:\n    1. Positional argument (args.task)\n    2. File (args.file)\n    3. Stdin (only if piped, not interactive)\n    \n    Raises:\n        InputError: If no input available or file not found\n    \"\"\"\n    # Positional argument\n    if getattr(args, 'task', None):\n        return args.task.strip()\n    \n    # File input\n    if getattr(args, 'file', None):\n        path = Path(args.file)\n        if not path.exists():\n            raise InputError(f\"Task file not found: {args.file}\")\n        if not path.is_file():\n            raise InputError(f\"Not a file: {args.file}\")\n        return path.read_text().strip()\n    \n    # Stdin (only if piped)\n    if not sys.stdin.isatty():\n        content = sys.stdin.read().strip()\n        if not content:\n            raise InputError(\"Empty input from stdin\")\n        return content\n    \n    # No input available\n    raise InputError(\n        \"No task provided. Use one of:\\n\"\n        \"  meld run \\\"your task description\\\"\\n\"\n        \"  meld run --file task.txt\\n\"\n        \"  echo \\\"task\\\" | meld run\"\n    )\n\ndef load_prd(args) -\u003e str | None:\n    \"\"\"\n    Load PRD content if --prd flag provided.\n    \n    Returns:\n        PRD content as string, or None if not provided\n        \n    Raises:\n        InputError: If PRD file not found or unreadable\n    \"\"\"\n    if not getattr(args, 'prd', None):\n        return None\n    \n    path = Path(args.prd)\n    if not path.exists():\n        raise InputError(f\"PRD file not found: {args.prd}\")\n    if not path.is_file():\n        raise InputError(f\"Not a file: {args.prd}\")\n    \n    content = path.read_text().strip()\n    if not content:\n        raise InputError(f\"PRD file is empty: {args.prd}\")\n    \n    return content\n```\n\n## Edge Cases to Handle\n\n### 1. Empty Input\n```python\n# Task is whitespace only\nif not task.strip():\n    raise InputError(\"Task description cannot be empty\")\n```\n\n### 2. Very Long Input\n```python\n# v1 doesn't enforce limits, but warn for very long tasks\nMAX_TASK_CHARS = 50000\nif len(task) \u003e MAX_TASK_CHARS:\n    print(f\"Warning: Task is very long ({len(task)} chars). Consider summarizing.\", \n          file=sys.stderr)\n```\n\n### 3. Binary File Detection\n```python\n# Detect binary content (null bytes)\nif '\\x00' in content:\n    raise InputError(f\"File appears to be binary: {path}\")\n```\n\n### 4. Encoding Issues\n```python\n# Explicit UTF-8 with error handling\ntry:\n    content = path.read_text(encoding='utf-8')\nexcept UnicodeDecodeError:\n    raise InputError(f\"File is not valid UTF-8: {path}\")\n```\n\n## Resume Mode\nWhen `--resume` is provided:\n- Task input flags are ignored\n- Task is loaded from session directory\n- Validate run ID exists\n\n```python\ndef handle_resume_mode(args) -\u003e tuple[str, str | None]:\n    \"\"\"\n    Handle --resume flag.\n    \n    Returns:\n        (task, prd) loaded from session\n    \"\"\"\n    if not args.resume:\n        return None, None\n    \n    run_dir = Path(args.run_dir) / args.resume\n    if not run_dir.exists():\n        raise InputError(f\"Session not found: {args.resume}\")\n    \n    task_file = run_dir / 'task.md'\n    if not task_file.exists():\n        raise InputError(f\"Session corrupt - missing task.md: {args.resume}\")\n    \n    task = task_file.read_text().strip()\n    \n    prd_file = run_dir / 'prd.md'\n    prd = prd_file.read_text().strip() if prd_file.exists() else None\n    \n    return task, prd\n```\n\n## Acceptance Criteria\n- [ ] Positional arg: `meld run \"task\"` works\n- [ ] File input: `meld run --file task.txt` works\n- [ ] Stdin: `echo \"task\" | meld run` works\n- [ ] PRD loading: `--prd` loads content correctly\n- [ ] Missing file: Clear error message\n- [ ] Empty input: Clear error message\n- [ ] Binary file: Detected and rejected\n- [ ] Resume mode: Loads from session dir\n- [ ] Priority: Positional beats file beats stdin\n\n## Error Message Quality\nError messages should:\n1. State what went wrong\n2. Show the path/value that caused it\n3. Suggest how to fix it\n\nExample:\n```\n❌ Task file not found: requirements.txt\n   Check the path or use: meld run \"your task\"\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:02:33.377264-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:02:33.377264-06:00","labels":["cli","foundation","input"],"dependencies":[{"issue_id":"meld-eq0.1.4","depends_on_id":"meld-eq0.1","type":"parent-child","created_at":"2026-01-15T23:02:33.38112-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.1.4","depends_on_id":"meld-eq0.1.3","type":"blocks","created_at":"2026-01-15T23:41:42.91155-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.10","title":"Task 10: Output Formatter \u0026 Final Deliverables","description":"# Task 10: Output Formatter \u0026 Final Deliverables\n\n## Overview\nGenerate final output documents including the converged plan, run report, decision log, and optional JSON summary for CI/scripting.\n\n## Output Artifacts\n\n### 1. Final Plan (Markdown)\nThe converged implementation plan ready for use:\n- Clean markdown\n- All sections from last synthesis\n- No internal metadata\n\n### 2. Run Report\nSummary of what happened during the session:\n- Round-by-round summary\n- Key changes each round\n- Decision log\n- Advisor participation\n\n### 3. JSON Summary (for CI)\nMachine-readable output:\n- Convergence status\n- Round count\n- Advisor availability\n- Exit code meaning\n\n## Design Decisions\n\n### Markdown for Humans\nDefault output is markdown because:\n- Renders in any viewer\n- Works with docs tools\n- Human-readable as plain text\n\n### JSON for Machines\nOptional `--json-output` for:\n- CI/CD pipelines\n- Scripting\n- Integration with other tools\n\n### Verbose Mode\n`--verbose` includes raw advisor outputs for:\n- Debugging\n- Understanding model reasoning\n- Audit trails\n\n## Technical Requirements\n\n### OutputFormatter Class\n```python\nclass OutputFormatter:\n    def format_final_output(\n        self,\n        result: OrchestrationResult,\n        verbose: bool = False,\n    ) -\u003e str:\n        \"\"\"Generate final markdown document.\"\"\"\n    \n    def format_json_summary(\n        self,\n        result: OrchestrationResult,\n    ) -\u003e dict:\n        \"\"\"Generate JSON summary.\"\"\"\n    \n    def format_run_report(\n        self,\n        session: SessionManager,\n    ) -\u003e str:\n        \"\"\"Generate run report.\"\"\"\n```\n\n### Final Document Structure\n```markdown\n# Implementation Plan\n\n{final_plan_content}\n\n---\n\n## Run Report\n\n**Session:** {session_id}\n**Status:** Converged after 3 rounds / Max rounds reached\n**Advisors:** Claude ✓, Gemini ✓, Codex ✗\n\n### Round Summary\n\n| Round | Changes | Key Updates |\n|-------|---------|-------------|\n| 1 | 8 | Added auth flow, updated DB schema |\n| 2 | 3 | Refined error handling |\n| 3 | 0 | No changes - converged |\n\n### Decision Log\n\n{decision_log_content}\n\n---\nGenerated by Meld v0.1.0\n```\n\n### JSON Schema\n```json\n{\n  \"version\": \"0.1.0\",\n  \"session_id\": \"2026-01-16T02-47-17Z-abc123\",\n  \"status\": \"converged\",\n  \"rounds_completed\": 3,\n  \"max_rounds\": 5,\n  \"duration_seconds\": 272,\n  \"advisors\": {\n    \"claude\": {\"participated\": true, \"rounds\": [1, 2, 3]},\n    \"gemini\": {\"participated\": true, \"rounds\": [1, 2, 3]},\n    \"codex\": {\"participated\": false, \"error\": \"CLI not found\"}\n  },\n  \"convergence\": {\n    \"converged\": true,\n    \"open_items\": 0,\n    \"final_diff_ratio\": 0.02\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] Final plan markdown generated correctly\n- [ ] Run report includes all required sections\n- [ ] Decision log included\n- [ ] JSON summary has correct schema\n- [ ] Verbose mode includes advisor outputs\n- [ ] Output writes to file with --output\n- [ ] JSON writes to file with --json-output\n- [ ] Stdout output works correctly","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:34:32.215691-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:34:32.215691-06:00","labels":["formatting","output","phase-4","polish"],"dependencies":[{"issue_id":"meld-eq0.10","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:34:32.219202-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.10","depends_on_id":"meld-eq0.8","type":"blocks","created_at":"2026-01-15T23:45:06.555656-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.10.1","title":"10.1: Implement markdown output formatter","description":"# Subtask 10.1: Markdown Output Formatter\n\n## What to Implement\nGenerate the final markdown output document.\n\n## Location\n`meld/output.py`\n\n## Implementation\n\n```python\nfrom dataclasses import dataclass\nfrom meld.orchestrator import OrchestrationResult\nfrom meld.session import SessionManager\nfrom meld import __version__\n\n\nclass OutputFormatter:\n    \"\"\"\n    Formats final output documents.\n    \n    Generates:\n    - Markdown final plan with run report\n    - JSON summary for CI\n    \"\"\"\n    \n    def __init__(self, session: SessionManager):\n        self.session = session\n    \n    def format_final_output(\n        self,\n        result: OrchestrationResult,\n        verbose: bool = False,\n    ) -\u003e str:\n        \"\"\"\n        Generate final markdown document.\n        \n        Args:\n            result: OrchestrationResult from orchestrator\n            verbose: Include raw advisor outputs\n            \n        Returns:\n            Complete markdown document\n        \"\"\"\n        sections = []\n        \n        # Main plan\n        sections.append('# Implementation Plan\\n')\n        sections.append(result.final_plan)\n        sections.append('\\n---\\n')\n        \n        # Run report\n        sections.append(self._format_run_report(result))\n        \n        # Decision log\n        if result.decision_log:\n            sections.append('\\n### Decision Log\\n')\n            sections.append(result.decision_log)\n        \n        # Verbose: raw advisor outputs\n        if verbose:\n            sections.append('\\n---\\n')\n            sections.append(self._format_advisor_outputs())\n        \n        # Footer\n        sections.append(f'\\n---\\n_Generated by Meld v{__version__}_\\n')\n        \n        return '\\n'.join(sections)\n    \n    def _format_run_report(self, result: OrchestrationResult) -\u003e str:\n        \"\"\"Format the run report section.\"\"\"\n        lines = []\n        \n        lines.append('## Run Report\\n')\n        \n        # Session info\n        lines.append(f'**Session:** `{self.session.state.id}`\\n')\n        \n        # Status\n        if result.converged:\n            status = f'Converged after {result.rounds_completed} rounds'\n        else:\n            status = f'Max rounds ({result.rounds_completed}) reached without convergence'\n        lines.append(f'**Status:** {status}\\n')\n        \n        # Advisors\n        advisor_status = []\n        for name, participated in result.advisor_participation.items():\n            icon = '✓' if participated else '✗'\n            advisor_status.append(f'{name.title()} {icon}')\n        lines.append(f'**Advisors:** {', '.join(advisor_status)}\\n')\n        \n        # Warnings\n        if result.warnings:\n            lines.append('\\n**Warnings:**\\n')\n            for warning in result.warnings:\n                lines.append(f'- {warning}\\n')\n        \n        # Round summary table\n        lines.append('\\n### Round Summary\\n')\n        lines.append(self._format_round_table())\n        \n        return ''.join(lines)\n    \n    def _format_round_table(self) -\u003e str:\n        \"\"\"Format round-by-round summary table.\"\"\"\n        lines = []\n        lines.append('| Round | Status | Notes |\\n')\n        lines.append('|-------|--------|-------|\\n')\n        \n        # Read from session events\n        events = self._read_events()\n        \n        for event in events:\n            if event.get('event') == 'convergence_checked':\n                round_num = event.get('round', '?')\n                converged = event.get('converged', False)\n                rationale = event.get('rationale', '')\n                \n                status = 'Converged' if converged else 'Continuing'\n                lines.append(f'| {round_num} | {status} | {rationale} |\\n')\n        \n        return ''.join(lines)\n    \n    def _format_advisor_outputs(self) -\u003e str:\n        \"\"\"Format raw advisor outputs for verbose mode.\"\"\"\n        lines = []\n        lines.append('## Raw Advisor Outputs\\n')\n        \n        # Find all advisor files\n        for advisor in ['claude', 'gemini', 'codex']:\n            files = sorted(self.session.session_dir.glob(f'advisor.{advisor}.round*.md'))\n            \n            if files:\n                lines.append(f'\\n### {advisor.title()}\\n')\n                \n                for f in files:\n                    round_num = f.stem.split('round')[1]\n                    content = f.read_text()\n                    \n                    lines.append(f'\\n#### Round {round_num}\\n')\n                    lines.append(content)\n        \n        return ''.join(lines)\n    \n    def _read_events(self) -\u003e list[dict]:\n        \"\"\"Read events from session.\"\"\"\n        import json\n        events_path = self.session.session_dir / 'events.jsonl'\n        \n        events = []\n        if events_path.exists():\n            for line in events_path.read_text().strip().split('\\n'):\n                if line:\n                    events.append(json.loads(line))\n        \n        return events\n```\n\n## Acceptance Criteria\n- [ ] Final plan formatted correctly\n- [ ] Run report included\n- [ ] Advisor participation shown\n- [ ] Round summary table generated\n- [ ] Decision log included\n- [ ] Verbose mode includes raw outputs\n- [ ] Footer with version","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:35:18.963573-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:35:18.963573-06:00","labels":["markdown","output","polish"],"dependencies":[{"issue_id":"meld-eq0.10.1","depends_on_id":"meld-eq0.10","type":"parent-child","created_at":"2026-01-15T23:35:18.966911-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.10.1","depends_on_id":"meld-eq0.8.1","type":"blocks","created_at":"2026-01-15T23:45:07.012357-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.10.2","title":"10.2: Implement JSON output for CI/scripting","description":"# Subtask 10.2: JSON Output for CI\n\n## What to Implement\nMachine-readable JSON summary for CI/CD and scripting.\n\n## Location\n`meld/output.py` (continued)\n\n## Implementation\n\n```python\nimport json\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\n\nclass OutputFormatter:\n    # ... existing code ...\n    \n    def format_json_summary(\n        self,\n        result: OrchestrationResult,\n    ) -\u003e dict:\n        \"\"\"\n        Generate JSON summary for CI/scripting.\n        \n        Returns:\n            Dict suitable for json.dumps()\n        \"\"\"\n        return {\n            'version': __version__,\n            'session_id': self.session.state.id,\n            'timestamp': datetime.now(timezone.utc).isoformat(),\n            \n            # Status\n            'status': 'converged' if result.converged else 'max_rounds',\n            'exit_code': 0 if result.converged else 1,\n            \n            # Rounds\n            'rounds_completed': result.rounds_completed,\n            'max_rounds': self.session.state.config.max_rounds,\n            \n            # Duration\n            'duration_seconds': self._calculate_duration(),\n            \n            # Advisors\n            'advisors': self._format_advisor_json(result),\n            \n            # Convergence details\n            'convergence': {\n                'converged': result.converged,\n                'open_items': self._get_final_open_items(),\n                'final_diff_ratio': self._get_final_diff_ratio(),\n            },\n            \n            # Warnings\n            'warnings': result.warnings,\n            \n            # Files\n            'artifacts': {\n                'session_dir': str(self.session.session_dir),\n                'final_plan': str(self.session.session_dir / 'final-plan.md'),\n            }\n        }\n    \n    def _format_advisor_json(self, result: OrchestrationResult) -\u003e dict:\n        \"\"\"Format advisor status for JSON.\"\"\"\n        advisors = {}\n        \n        for name in ['claude', 'gemini', 'codex']:\n            participated = result.advisor_participation.get(name, False)\n            \n            # Find which rounds they participated in\n            rounds = []\n            for f in self.session.session_dir.glob(f'advisor.{name}.round*.md'):\n                round_num = int(f.stem.split('round')[1])\n                rounds.append(round_num)\n            \n            advisors[name] = {\n                'participated': participated,\n                'rounds': sorted(rounds),\n            }\n            \n            # Add error info if failed\n            if not participated:\n                error = self._get_advisor_error(name)\n                if error:\n                    advisors[name]['error'] = error\n        \n        return advisors\n    \n    def _calculate_duration(self) -\u003e int:\n        \"\"\"Calculate session duration in seconds.\"\"\"\n        started = self.session.state.started\n        updated = self.session.state.updated\n        \n        if started and updated:\n            start_dt = datetime.fromisoformat(started.replace('Z', '+00:00'))\n            end_dt = datetime.fromisoformat(updated.replace('Z', '+00:00'))\n            return int((end_dt - start_dt).total_seconds())\n        \n        return 0\n    \n    def _get_final_open_items(self) -\u003e int:\n        \"\"\"Get open items count from final convergence check.\"\"\"\n        events = self._read_events()\n        \n        for event in reversed(events):\n            if event.get('event') == 'convergence_checked':\n                # Parse from stored data\n                return event.get('open_items', 0)\n        \n        return 0\n    \n    def _get_final_diff_ratio(self) -\u003e float:\n        \"\"\"Get diff ratio from final round.\"\"\"\n        events = self._read_events()\n        \n        for event in reversed(events):\n            if event.get('event') == 'convergence_checked':\n                return event.get('diff_ratio', 0.0)\n        \n        return 0.0\n    \n    def _get_advisor_error(self, name: str) -\u003e str | None:\n        \"\"\"Get error message for failed advisor.\"\"\"\n        events = self._read_events()\n        \n        for event in events:\n            if (event.get('event') == 'advisor_failed' and \n                event.get('advisor') == name):\n                return event.get('error', 'Unknown error')\n        \n        return None\n    \n    def write_json_output(\n        self,\n        result: OrchestrationResult,\n        output_path: Path | str,\n    ) -\u003e None:\n        \"\"\"Write JSON summary to file.\"\"\"\n        summary = self.format_json_summary(result)\n        \n        path = Path(output_path)\n        path.write_text(json.dumps(summary, indent=2))\n\n\ndef write_output(\n    formatter: OutputFormatter,\n    result: OrchestrationResult,\n    output_path: Path | str | None = None,\n    json_path: Path | str | None = None,\n    verbose: bool = False,\n    quiet: bool = False,\n) -\u003e None:\n    \"\"\"\n    Write all output files.\n    \n    Args:\n        formatter: OutputFormatter instance\n        result: OrchestrationResult\n        output_path: Where to write markdown (None = stdout)\n        json_path: Where to write JSON (None = skip)\n        verbose: Include raw advisor outputs\n        quiet: Minimal output\n    \"\"\"\n    # Markdown output\n    markdown = formatter.format_final_output(result, verbose=verbose)\n    \n    if output_path:\n        Path(output_path).write_text(markdown)\n    elif not quiet:\n        print(markdown)\n    elif quiet:\n        # Quiet mode: just the plan to stdout\n        print(result.final_plan)\n    \n    # JSON output\n    if json_path:\n        formatter.write_json_output(result, json_path)\n```\n\n## JSON Schema Details\n\n```json\n{\n  \"version\": \"0.1.0\",\n  \"session_id\": \"2026-01-16T02-47-17Z-abc123\",\n  \"timestamp\": \"2026-01-16T03:02:17Z\",\n  \"status\": \"converged\",\n  \"exit_code\": 0,\n  \"rounds_completed\": 3,\n  \"max_rounds\": 5,\n  \"duration_seconds\": 272,\n  \"advisors\": {\n    \"claude\": {\"participated\": true, \"rounds\": [1, 2, 3]},\n    \"gemini\": {\"participated\": true, \"rounds\": [1, 2, 3]},\n    \"codex\": {\"participated\": false, \"error\": \"CLI not found\"}\n  },\n  \"convergence\": {\n    \"converged\": true,\n    \"open_items\": 0,\n    \"final_diff_ratio\": 0.02\n  },\n  \"warnings\": [],\n  \"artifacts\": {\n    \"session_dir\": \".meld/runs/2026-01-16T02-47-17Z-abc123\",\n    \"final_plan\": \".meld/runs/2026-01-16T02-47-17Z-abc123/final-plan.md\"\n  }\n}\n```\n\n## Acceptance Criteria\n- [ ] JSON schema complete and valid\n- [ ] Exit code correct (0 = converged, 1 = max rounds)\n- [ ] Advisor participation tracked\n- [ ] Duration calculated correctly\n- [ ] Writes to file correctly\n- [ ] Indented for readability","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:36:49.047932-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:36:49.047932-06:00","labels":["ci","json","output","polish"],"dependencies":[{"issue_id":"meld-eq0.10.2","depends_on_id":"meld-eq0.10","type":"parent-child","created_at":"2026-01-15T23:36:49.051406-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.10.2","depends_on_id":"meld-eq0.10.1","type":"blocks","created_at":"2026-01-15T23:45:07.256435-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.10.3","title":"10.3: Implement quiet mode for scripting","description":"# Subtask 10.3: Quiet Mode\n\n## What to Implement\nNon-interactive mode for scripting and CI pipelines.\n\n## Behavior\n\n### Default (TUI) Mode\n- Full TUI with 4 panels\n- Real-time streaming\n- Interactive experience\n\n### Quiet Mode (`-q` / `--quiet`)\n- No TUI\n- Progress to stderr\n- Final plan to stdout (unless `--output`)\n- Exit codes for status\n\n## Exit Codes\n\n| Code | Meaning |\n|------|---------|\n| 0 | Converged successfully |\n| 1 | Max rounds reached (not converged) |\n| 2 | Preflight failed (missing CLIs) |\n| 3 | All advisors failed |\n| 4 | Melder failed |\n| 5 | Interrupted (Ctrl+C) |\n\n## Implementation\n\n```python\nimport sys\nfrom pathlib import Path\n\n\nasync def run_quiet_mode(\n    task: str,\n    prd: str | None,\n    max_rounds: int,\n    timeout: float,\n    output_path: Path | None,\n    json_path: Path | None,\n    verbose: bool,\n) -\u003e int:\n    \"\"\"\n    Run meld in quiet (non-TUI) mode.\n    \n    Args:\n        task: Task description\n        prd: Optional PRD content\n        max_rounds: Maximum rounds\n        timeout: Per-advisor timeout\n        output_path: Where to write final plan\n        json_path: Where to write JSON summary\n        verbose: Include raw advisor outputs\n        \n    Returns:\n        Exit code\n    \"\"\"\n    # Print progress to stderr\n    def progress(msg: str) -\u003e None:\n        print(msg, file=sys.stderr, flush=True)\n    \n    # Create session\n    progress('Starting meld session...')\n    \n    session = SessionManager.create_new(\n        Path('.meld/runs'),\n        SessionConfig(timeout=int(timeout), max_rounds=max_rounds),\n    )\n    \n    progress(f'Session: {session.state.id}')\n    \n    # Create components\n    claude_adapter = ClaudeAdapter()\n    adapters = [claude_adapter, GeminiAdapter(), CodexAdapter()]\n    \n    melder = Melder(claude_adapter, session)\n    pool = AdvisorPool(adapters, session)\n    \n    orchestrator = Orchestrator(\n        session=session,\n        melder=melder,\n        advisor_pool=pool,\n        max_rounds=max_rounds,\n        timeout=timeout,\n    )\n    \n    # Set up progress callback\n    def on_event(event):\n        match event.type:\n            case 'phase_changed':\n                phase = event.data.get('phase')\n                round_num = event.data.get('round', 0)\n                if phase == 'planning':\n                    progress('Generating initial plan...')\n                elif phase == 'feedback':\n                    progress(f'Round {round_num}/{max_rounds}: Collecting feedback...')\n                elif phase == 'synthesis':\n                    progress(f'Round {round_num}/{max_rounds}: Synthesizing...')\n            \n            case 'convergence_checked':\n                converged = event.data.get('converged')\n                if converged:\n                    progress('✓ Converged\\!')\n            \n            case 'all_advisors_failed':\n                progress('✗ All advisors failed')\n    \n    orchestrator.set_event_callback(on_event)\n    \n    try:\n        # Run orchestration\n        result = await orchestrator.run(task, prd)\n        \n        # Determine exit code\n        if not result.advisor_participation:\n            exit_code = 3  # All advisors failed\n        elif not result.final_plan:\n            exit_code = 4  # Melder failed\n        elif result.converged:\n            exit_code = 0  # Success\n        else:\n            exit_code = 1  # Max rounds\n        \n        # Generate output\n        formatter = OutputFormatter(session)\n        \n        if output_path:\n            # Write to file\n            output = formatter.format_final_output(result, verbose=verbose)\n            output_path.write_text(output)\n            progress(f'Output written to: {output_path}')\n        else:\n            # Print final plan to stdout\n            print(result.final_plan)\n        \n        if json_path:\n            formatter.write_json_output(result, json_path)\n            progress(f'JSON summary written to: {json_path}')\n        \n        return exit_code\n        \n    except Exception as e:\n        progress(f'Error: {e}')\n        return 4\n\n\n# In cli.py\ndef run_command(args) -\u003e int:\n    if args.quiet:\n        return asyncio.run(run_quiet_mode(\n            task=get_task_input(args),\n            prd=load_prd(args),\n            max_rounds=args.rounds,\n            timeout=args.timeout,\n            output_path=Path(args.output) if args.output else None,\n            json_path=Path(args.json_output) if args.json_output else None,\n            verbose=args.verbose,\n        ))\n    else:\n        return run_tui_mode(...)\n```\n\n## Usage Examples\n\n```bash\n# Basic quiet mode\nmeld -q \"Build auth system\" \u003e plan.md\n\n# With explicit output file\nmeld -q \"Build auth\" --output plan.md\n\n# Check convergence status\nmeld -q \"Build auth\" \u0026\u0026 echo \"Converged\\!\" || echo \"Needs work\"\n\n# With JSON for CI\nmeld -q \"Build auth\" --output plan.md --json-output result.json\n\n# Capture progress too\nmeld -q \"Build auth\" \u003e plan.md 2\u003e progress.log\n```\n\n## Acceptance Criteria\n- [ ] No TUI when -q flag used\n- [ ] Progress messages to stderr\n- [ ] Final plan to stdout (or file)\n- [ ] Exit codes match specification\n- [ ] JSON output works correctly\n- [ ] Can be used in shell scripts\n- [ ] Works with pipes and redirects","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:37:37.111869-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:37:37.111869-06:00","labels":["polish","quiet","scripting"],"dependencies":[{"issue_id":"meld-eq0.10.3","depends_on_id":"meld-eq0.10","type":"parent-child","created_at":"2026-01-15T23:37:37.115134-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.10.3","depends_on_id":"meld-eq0.10.1","type":"blocks","created_at":"2026-01-15T23:45:07.37157-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.10.3","depends_on_id":"meld-eq0.10.2","type":"blocks","created_at":"2026-01-15T23:45:07.471225-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.11","title":"Task 11: Comprehensive Testing Suite","description":"# Task 11: Comprehensive Testing Suite\n\n## Overview\nEstablish a complete testing infrastructure with unit tests, integration tests, and end-to-end test scripts with detailed logging to ensure all components work correctly.\n\n## Why Dedicated Testing Task?\nTesting was mentioned in individual tasks but needs centralized infrastructure:\n1. **Pytest setup** with fixtures for all components\n2. **Mock adapters** for CI testing without real CLIs\n3. **Integration tests** for component interactions\n4. **E2E scripts** with detailed logging for validation\n5. **Performance benchmarks** for parallel execution\n\n## Design Principles\n\n### Test in Isolation\n- Unit tests mock all external dependencies\n- Integration tests use mock adapters\n- E2E tests can optionally use real CLIs\n\n### Comprehensive Logging\nAll test runs produce detailed logs:\n- Test outcomes with timing\n- Mock adapter invocations\n- Session artifacts for debugging\n- Performance metrics\n\n## Technical Requirements\n\n### Test Directory Structure\n```\ntests/\n├── __init__.py\n├── conftest.py           # Shared fixtures\n├── unit/\n│   ├── test_cli.py\n│   ├── test_session.py\n│   ├── test_adapters.py\n│   ├── test_melder.py\n│   ├── test_advisors.py\n│   ├── test_convergence.py\n│   └── test_orchestrator.py\n├── integration/\n│   ├── test_session_manager.py\n│   ├── test_advisor_pool.py\n│   └── test_full_loop.py\n├── e2e/\n│   ├── test_happy_path.py\n│   ├── test_failure_scenarios.py\n│   └── test_resume.py\n└── mocks/\n    ├── __init__.py\n    └── mock_adapter.py\n```\n\n### Coverage Targets\n- Unit tests: \u003e90% coverage on core logic\n- Integration: All component interactions\n- E2E: Happy path + major failure modes\n\n## Acceptance Criteria\n- [ ] pytest infrastructure complete\n- [ ] All components have unit tests\n- [ ] MockAdapter enables CI testing\n- [ ] Integration tests verify component interactions\n- [ ] E2E tests validate full workflows\n- [ ] Test logging is comprehensive\n- [ ] `pytest` runs all tests successfully\n- [ ] CI-compatible (no real CLIs required)\n\n## Dependencies\nDepends on Tasks 1-10 being implemented.","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:51:13.984705-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:51:13.984705-06:00","labels":["phase-4","quality","testing"],"dependencies":[{"issue_id":"meld-eq0.11","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:51:13.988285-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.11.1","title":"11.1: Set up pytest infrastructure and fixtures","description":"# Subtask 11.1: Pytest Infrastructure\n\n## What to Implement\nComplete pytest setup with shared fixtures for all test types.\n\n## Location\n`tests/conftest.py` and `pyproject.toml` test configuration\n\n## Implementation\n\n### pyproject.toml additions\n```toml\n[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\ntestpaths = [\"tests\"]\naddopts = [\n    \"-v\",\n    \"--tb=short\",\n    \"--log-cli-level=INFO\",\n    \"--log-file=test-results.log\",\n    \"--log-file-level=DEBUG\",\n]\nmarkers = [\n    \"unit: Unit tests (fast, no I/O)\",\n    \"integration: Integration tests (may use filesystem)\",\n    \"e2e: End-to-end tests (full system)\",\n    \"slow: Slow tests (\u003e10s)\",\n    \"requires_cli: Tests requiring real CLI (skip in CI)\",\n]\n\n[tool.coverage.run]\nsource = [\"meld\"]\nbranch = true\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"raise NotImplementedError\",\n    \"if TYPE_CHECKING:\",\n]\n```\n\n### conftest.py Fixtures\n```python\nimport pytest\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import AsyncMock\n\nfrom meld.session import SessionManager, SessionConfig\nfrom meld.providers.base import ProviderAdapter\nfrom meld.models import AdvisorResult, StreamEvent, StreamEventType\n\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Temporary directory for test files.\"\"\"\n    with tempfile.TemporaryDirectory() as d:\n        yield Path(d)\n\n\n@pytest.fixture\ndef session_dir(temp_dir):\n    \"\"\"Session directory for tests.\"\"\"\n    session_dir = temp_dir / \"session-test-123\"\n    session_dir.mkdir()\n    return session_dir\n\n\n@pytest.fixture\ndef session_config():\n    \"\"\"Default session configuration.\"\"\"\n    return SessionConfig(\n        timeout=60,\n        max_rounds=3,\n    )\n\n\n@pytest.fixture\ndef session_manager(temp_dir, session_config):\n    \"\"\"Fresh session manager for each test.\"\"\"\n    return SessionManager.create_new(temp_dir, session_config)\n\n\n@pytest.fixture\ndef mock_adapter():\n    \"\"\"Mock adapter that returns configurable responses.\"\"\"\n    from tests.mocks.mock_adapter import MockAdapter\n    return MockAdapter()\n\n\n@pytest.fixture\ndef mock_claude_adapter(mock_adapter):\n    \"\"\"Mock adapter configured as Claude.\"\"\"\n    mock_adapter.name = \"claude\"\n    return mock_adapter\n\n\n@pytest.fixture\ndef mock_gemini_adapter(mock_adapter):\n    \"\"\"Mock adapter configured as Gemini.\"\"\"\n    adapter = MockAdapter()\n    adapter.name = \"gemini\"\n    return adapter\n\n\n@pytest.fixture\ndef mock_codex_adapter(mock_adapter):\n    \"\"\"Mock adapter configured as Codex.\"\"\"\n    adapter = MockAdapter()\n    adapter.name = \"codex\"\n    return adapter\n\n\n@pytest.fixture\ndef all_mock_adapters(mock_claude_adapter, mock_gemini_adapter, mock_codex_adapter):\n    \"\"\"All three mock adapters.\"\"\"\n    return [mock_claude_adapter, mock_gemini_adapter, mock_codex_adapter]\n\n\n@pytest.fixture\ndef sample_task():\n    \"\"\"Sample task for testing.\"\"\"\n    return \"Build a user authentication system with JWT tokens\"\n\n\n@pytest.fixture\ndef sample_prd():\n    \"\"\"Sample PRD content.\"\"\"\n    return \"\"\"\n# Authentication PRD\n\n## Requirements\n- Support email/password login\n- Issue JWT tokens\n- Token refresh mechanism\n- Password reset flow\n\"\"\"\n\n\n@pytest.fixture\ndef sample_plan():\n    \"\"\"Sample plan content.\"\"\"\n    return \"\"\"\n# Implementation Plan\n\n## Overview\nBuild JWT-based auth with refresh tokens.\n\n## Steps\n1. Create user model\n2. Implement login endpoint\n3. Add token refresh\n\"\"\"\n\n\n@pytest.fixture\ndef sample_feedback():\n    \"\"\"Sample advisor feedback.\"\"\"\n    return {\n        \"claude\": \"## Summary\\\\n- Plan looks good\\\\n\\\\n## Improvements\\\\n- Add rate limiting\",\n        \"gemini\": \"## Summary\\\\n- Solid approach\\\\n\\\\n## Must-Fix\\\\n- Add input validation\",\n        \"codex\": \"## Summary\\\\n- LGTM\\\\n\\\\n## Missing\\\\n- Error handling for token expiry\",\n    }\n\n\n# Logging fixtures\n@pytest.fixture(autouse=True)\ndef configure_logging(caplog):\n    \"\"\"Configure logging for all tests.\"\"\"\n    import logging\n    caplog.set_level(logging.DEBUG)\n\n\n@pytest.fixture\ndef log_file(temp_dir):\n    \"\"\"Log file for detailed test logging.\"\"\"\n    log_path = temp_dir / \"test.log\"\n    return log_path\n```\n\n## Acceptance Criteria\n- [ ] pytest runs without configuration errors\n- [ ] All fixtures importable\n- [ ] Markers work for test selection\n- [ ] Log output captured correctly\n- [ ] Coverage reporting works\n- [ ] async tests work with asyncio_mode","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:51:38.653603-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:51:38.653603-06:00","labels":["infrastructure","testing"],"dependencies":[{"issue_id":"meld-eq0.11.1","depends_on_id":"meld-eq0.11","type":"parent-child","created_at":"2026-01-15T23:51:38.658876-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.11.1","depends_on_id":"meld-eq0.1","type":"blocks","created_at":"2026-01-16T00:01:57.631022-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.11.2","title":"11.2: Implement MockAdapter for testing","description":"# Subtask 11.2: MockAdapter Implementation\n\n## What to Implement\nA configurable mock adapter for testing without real CLI dependencies.\n\n## Location\n`tests/mocks/mock_adapter.py`\n\n## Why MockAdapter?\n- CI environments dont have claude/gemini/codex installed\n- Need to test timeout, failure, and retry logic\n- Need deterministic responses for validation\n- Need to simulate streaming behavior\n\n## Implementation\n\n```python\nimport asyncio\nfrom typing import AsyncIterator\nfrom dataclasses import dataclass, field\n\nfrom meld.providers.base import ProviderAdapter\nfrom meld.models import (\n    AdvisorResult,\n    AdvisorError,\n    StreamEvent,\n    StreamEventType,\n    AvailabilityResult,\n)\n\n\n@dataclass\nclass MockResponse:\n    \"\"\"Configurable response for MockAdapter.\"\"\"\n    content: str = \"Mock response content\"\n    success: bool = True\n    error: AdvisorError | None = None\n    error_message: str = \"\"\n    delay_ms: int = 100  # Simulated delay\n    stream_chunks: list[str] | None = None  # For streaming\n\n\nclass MockAdapter(ProviderAdapter):\n    \"\"\"\n    Configurable mock adapter for testing.\n    \n    Allows configuration of:\n    - Response content\n    - Success/failure\n    - Error types\n    - Delays (for timeout testing)\n    - Streaming behavior\n    - Call tracking\n    \n    Example:\n        adapter = MockAdapter()\n        adapter.set_response(MockResponse(content=\"Test plan\"))\n        result = await adapter.run(\"prompt\")\n        assert result.success\n        assert adapter.call_count == 1\n    \"\"\"\n    \n    name: str = \"mock\"\n    \n    def __init__(self, name: str = \"mock\"):\n        self.name = name\n        self._responses: list[MockResponse] = []\n        self._default_response = MockResponse()\n        self._call_count = 0\n        self._calls: list[dict] = []\n        self._available = True\n        self._authenticated = True\n        self._version = \"1.0.0-mock\"\n    \n    # Configuration methods\n    def set_response(self, response: MockResponse) -\u003e None:\n        \"\"\"Set single response (used for all calls).\"\"\"\n        self._default_response = response\n    \n    def queue_responses(self, responses: list[MockResponse]) -\u003e None:\n        \"\"\"Queue multiple responses (used in order).\"\"\"\n        self._responses = list(responses)\n    \n    def set_available(self, available: bool, authenticated: bool = True) -\u003e None:\n        \"\"\"Configure availability check response.\"\"\"\n        self._available = available\n        self._authenticated = authenticated\n    \n    def set_fail_count(self, count: int, error: AdvisorError = AdvisorError.TIMEOUT) -\u003e None:\n        \"\"\"Configure to fail N times then succeed.\"\"\"\n        fail_response = MockResponse(\n            success=False,\n            error=error,\n            error_message=f\"Mock {error.name} error\",\n        )\n        success_response = MockResponse()\n        \n        self._responses = [fail_response] * count + [success_response]\n    \n    # Tracking methods\n    @property\n    def call_count(self) -\u003e int:\n        return self._call_count\n    \n    @property\n    def calls(self) -\u003e list[dict]:\n        return self._calls.copy()\n    \n    def reset(self) -\u003e None:\n        \"\"\"Reset call tracking.\"\"\"\n        self._call_count = 0\n        self._calls = []\n    \n    # ProviderAdapter implementation\n    def _get_next_response(self) -\u003e MockResponse:\n        \"\"\"Get next response from queue or default.\"\"\"\n        if self._responses:\n            return self._responses.pop(0)\n        return self._default_response\n    \n    def build_command(self, prompt: str) -\u003e list[str]:\n        \"\"\"Return fake command (for interface compliance).\"\"\"\n        return [\"mock-cli\", \"-p\", prompt]\n    \n    def parse_output(self, stdout: str, stderr: str) -\u003e str:\n        \"\"\"Return stdout as-is.\"\"\"\n        return stdout.strip()\n    \n    def categorize_error(\n        self,\n        returncode: int,\n        stdout: str,\n        stderr: str,\n    ) -\u003e tuple[AdvisorError, str]:\n        \"\"\"Return configured error.\"\"\"\n        return AdvisorError.INTERNAL_ERROR, \"Mock error\"\n    \n    async def check_available(self) -\u003e AvailabilityResult:\n        \"\"\"Return configured availability.\"\"\"\n        return AvailabilityResult(\n            provider=self.name,\n            available=self._available,\n            authenticated=self._authenticated if self._available else None,\n            version=self._version if self._available else \"\",\n            error=\"\" if self._available else \"Mock not available\",\n            install_hint=\"pip install mock-cli\",\n        )\n    \n    async def run(\n        self,\n        prompt: str,\n        timeout: float = 600.0,\n        cwd=None,\n    ) -\u003e AdvisorResult:\n        \"\"\"Run mock with configured response.\"\"\"\n        self._call_count += 1\n        response = self._get_next_response()\n        \n        # Record call\n        self._calls.append({\n            \"prompt\": prompt,\n            \"timeout\": timeout,\n            \"timestamp\": asyncio.get_event_loop().time(),\n        })\n        \n        # Simulate delay\n        if response.delay_ms \u003e 0:\n            delay = response.delay_ms / 1000.0\n            if delay \u003e timeout:\n                # Simulate timeout\n                await asyncio.sleep(timeout)\n                return AdvisorResult.failure_result(\n                    provider=self.name,\n                    error=AdvisorError.TIMEOUT,\n                    error_message=f\"Exceeded {timeout}s timeout\",\n                    duration_ms=int(timeout * 1000),\n                )\n            await asyncio.sleep(delay)\n        \n        duration_ms = response.delay_ms\n        \n        if response.success:\n            return AdvisorResult.success_result(\n                provider=self.name,\n                content=response.content,\n                duration_ms=duration_ms,\n            )\n        else:\n            return AdvisorResult.failure_result(\n                provider=self.name,\n                error=response.error or AdvisorError.INTERNAL_ERROR,\n                error_message=response.error_message,\n                duration_ms=duration_ms,\n            )\n    \n    async def stream(\n        self,\n        prompt: str,\n        timeout: float = 600.0,\n        cwd=None,\n    ) -\u003e AsyncIterator[StreamEvent]:\n        \"\"\"Stream mock response in chunks.\"\"\"\n        self._call_count += 1\n        response = self._get_next_response()\n        \n        self._calls.append({\n            \"prompt\": prompt,\n            \"timeout\": timeout,\n            \"streaming\": True,\n        })\n        \n        start_time = asyncio.get_event_loop().time()\n        \n        # Get chunks\n        if response.stream_chunks:\n            chunks = response.stream_chunks\n        else:\n            # Split content into word chunks\n            words = response.content.split()\n            chunks = [w + \" \" for w in words]\n        \n        # Stream chunks with delays\n        chunk_delay = (response.delay_ms / 1000.0) / max(len(chunks), 1)\n        \n        for i, chunk in enumerate(chunks):\n            elapsed = asyncio.get_event_loop().time() - start_time\n            \n            if elapsed \u003e timeout:\n                yield StreamEvent.error(f\"Timeout after {elapsed:.1f}s\", elapsed)\n                return\n            \n            await asyncio.sleep(chunk_delay)\n            elapsed = asyncio.get_event_loop().time() - start_time\n            yield StreamEvent.text(chunk, elapsed)\n        \n        final_elapsed = asyncio.get_event_loop().time() - start_time\n        \n        if response.success:\n            yield StreamEvent.done(final_elapsed)\n        else:\n            yield StreamEvent.error(response.error_message, final_elapsed)\n\n\n# Preset configurations for common test scenarios\nclass MockAdapterPresets:\n    \"\"\"Factory for common mock configurations.\"\"\"\n    \n    @staticmethod\n    def success(name: str = \"mock\", content: str = \"Success response\") -\u003e MockAdapter:\n        adapter = MockAdapter(name)\n        adapter.set_response(MockResponse(content=content))\n        return adapter\n    \n    @staticmethod\n    def failure(\n        name: str = \"mock\",\n        error: AdvisorError = AdvisorError.INTERNAL_ERROR,\n    ) -\u003e MockAdapter:\n        adapter = MockAdapter(name)\n        adapter.set_response(MockResponse(\n            success=False,\n            error=error,\n            error_message=f\"Mock {error.name}\",\n        ))\n        return adapter\n    \n    @staticmethod\n    def timeout(name: str = \"mock\", delay_ms: int = 10000) -\u003e MockAdapter:\n        adapter = MockAdapter(name)\n        adapter.set_response(MockResponse(delay_ms=delay_ms))\n        return adapter\n    \n    @staticmethod\n    def retry_then_succeed(\n        name: str = \"mock\",\n        fail_count: int = 2,\n        error: AdvisorError = AdvisorError.RATE_LIMITED,\n    ) -\u003e MockAdapter:\n        adapter = MockAdapter(name)\n        adapter.set_fail_count(fail_count, error)\n        return adapter\n    \n    @staticmethod\n    def unavailable(name: str = \"mock\") -\u003e MockAdapter:\n        adapter = MockAdapter(name)\n        adapter.set_available(False)\n        return adapter\n```\n\n## Acceptance Criteria\n- [ ] MockAdapter implements full ProviderAdapter interface\n- [ ] Configurable success/failure responses\n- [ ] Supports response queuing for retry tests\n- [ ] Tracks all calls for assertion\n- [ ] Simulates delays for timeout testing\n- [ ] Streaming works correctly\n- [ ] Presets cover common test scenarios","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:52:17.156051-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:52:17.156051-06:00","labels":["mocks","testing"],"dependencies":[{"issue_id":"meld-eq0.11.2","depends_on_id":"meld-eq0.11","type":"parent-child","created_at":"2026-01-15T23:52:17.176887-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.11.2","depends_on_id":"meld-eq0.3","type":"blocks","created_at":"2026-01-16T00:01:59.451254-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.11.3","title":"11.3: Implement unit and integration tests","description":"# Subtask 11.3: Unit and Integration Tests\n\n## What to Implement\nComprehensive unit tests for all core components and integration tests for component interactions.\n\n## Location\n`tests/unit/` and `tests/integration/`\n\n## Unit Tests Required\n\n### test_cli.py\n```python\nimport pytest\nfrom meld.cli import create_parser, get_task_input\n\nclass TestCLIParsing:\n    def test_run_subcommand_default(self):\n        parser = create_parser()\n        args = parser.parse_args([\"run\", \"Build auth\"])\n        assert args.command == \"run\"\n        assert args.task == \"Build auth\"\n    \n    def test_file_input(self, temp_dir):\n        task_file = temp_dir / \"task.txt\"\n        task_file.write_text(\"Build auth system\")\n        args = mock_args(file=str(task_file))\n        assert get_task_input(args) == \"Build auth system\"\n    \n    def test_rounds_validation(self):\n        parser = create_parser()\n        args = parser.parse_args([\"run\", \"task\", \"--rounds\", \"10\"])\n        assert args.rounds == 10\n    \n    def test_doctor_subcommand(self):\n        parser = create_parser()\n        args = parser.parse_args([\"doctor\"])\n        assert args.command == \"doctor\"\n```\n\n### test_session.py\n```python\nimport pytest\nfrom meld.session import SessionManager, SessionStatus, generate_session_id\n\nclass TestSessionID:\n    def test_uniqueness(self):\n        ids = [generate_session_id() for _ in range(1000)]\n        assert len(ids) == len(set(ids))\n    \n    def test_format(self):\n        sid = generate_session_id()\n        import re\n        pattern = r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}Z-[a-f0-9]{6}$\"\n        assert re.match(pattern, sid)\n\nclass TestSessionManager:\n    def test_create_new(self, temp_dir, session_config):\n        manager = SessionManager.create_new(temp_dir, session_config)\n        assert manager.state.status == SessionStatus.PENDING\n        assert (temp_dir / manager.state.id / \"session.json\").exists()\n    \n    def test_update_status(self, session_manager):\n        session_manager.update_status(SessionStatus.IN_PROGRESS, current_round=1)\n        assert session_manager.state.status == SessionStatus.IN_PROGRESS\n        assert session_manager.state.current_round == 1\n    \n    def test_write_artifact(self, session_manager):\n        session_manager.write_artifact(\"test.md\", \"content\")\n        path = session_manager.session_dir / \"test.md\"\n        assert path.exists()\n        assert path.read_text() == \"content\"\n    \n    def test_atomic_write_survives_crash(self, session_manager, monkeypatch):\n        # Write initial content\n        session_manager.write_artifact(\"test.md\", \"original\")\n        \n        # Mock rename to fail mid-write\n        def failing_rename(*args):\n            raise OSError(\"Simulated crash\")\n        \n        monkeypatch.setattr(\"os.rename\", failing_rename)\n        \n        with pytest.raises(OSError):\n            session_manager.write_artifact(\"test.md\", \"new content\")\n        \n        # Original should still exist\n        assert (session_manager.session_dir / \"test.md\").read_text() == \"original\"\n```\n\n### test_adapters.py\n```python\nimport pytest\nfrom meld.providers.claude import ClaudeAdapter\nfrom meld.providers.gemini import GeminiAdapter\nfrom meld.providers.openai import CodexAdapter\nfrom meld.models import AdvisorError\n\nclass TestClaudeAdapter:\n    def test_build_command(self):\n        adapter = ClaudeAdapter()\n        cmd = adapter.build_command(\"Hello\")\n        assert cmd[0] == \"claude\"\n        assert \"-p\" in cmd\n        assert \"--permission-mode\" in cmd\n    \n    def test_categorize_auth_error(self):\n        adapter = ClaudeAdapter()\n        error, msg = adapter.categorize_error(1, \"\", \"not logged in\")\n        assert error == AdvisorError.AUTH_FAILED\n    \n    def test_categorize_rate_limit(self):\n        adapter = ClaudeAdapter()\n        error, msg = adapter.categorize_error(1, \"\", \"rate limit exceeded\")\n        assert error == AdvisorError.RATE_LIMITED\n\nclass TestGeminiAdapter:\n    def test_build_command(self):\n        adapter = GeminiAdapter()\n        cmd = adapter.build_command(\"Hello\")\n        assert \"gemini\" in cmd[0]\n        assert \"--sandbox\" in cmd\n\nclass TestCodexAdapter:\n    def test_build_command(self):\n        adapter = CodexAdapter()\n        cmd = adapter.build_command(\"Hello\")\n        assert \"codex\" in cmd[0]\n        assert \"exec\" in cmd\n```\n\n### test_convergence.py\n```python\nimport pytest\nfrom meld.convergence import (\n    parse_convergence_json,\n    detect_convergence,\n    compute_diff_ratio,\n)\n\nclass TestConvergenceParsing:\n    def test_parse_json_block(self):\n        response = \"\"\"\n## Convergence Assessment\n```json\n{\"status\": \"CONVERGED\", \"changes_made\": 2, \"open_items\": 0, \"rationale\": \"Done\"}\n```\n\"\"\"\n        assessment = parse_convergence_json(response)\n        assert assessment.status == \"CONVERGED\"\n        assert assessment.open_items == 0\n    \n    def test_parse_malformed_json(self):\n        response = \"No JSON here\"\n        assert parse_convergence_json(response) is None\n\nclass TestConvergenceDetection:\n    def test_round_1_never_converges(self):\n        response = {status:","notes":"## COVERAGE TARGETS\n\n### Critical Path Modules (\u003e85% coverage required)\n- meld/convergence.py - convergence detection logic\n- meld/orchestrator.py - main loop and state machine\n- meld/session.py - session management and resume\n- meld/melder.py - plan generation and synthesis\n\n### Standard Modules (\u003e70% coverage required)\n- meld/providers/*.py - adapter implementations\n- meld/preflight.py - preflight checks\n- meld/output.py - formatters\n\n### UI Modules (\u003e50% coverage required)\n- meld/tui/*.py - TUI components (harder to test)\n\n### Test Configuration\n```toml\n[tool.coverage.report]\nfail_under = 70  # Overall minimum\n\n[tool.coverage.html]\ndirectory = \"coverage_html\"\n```\n\n### Acceptance Criteria Addition\n- [ ] Coverage report generated on test run\n- [ ] Critical path modules have \u003e85% coverage\n- [ ] Overall coverage \u003e70%\n- [ ] CI fails if coverage drops below thresholds","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:52:56.769574-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:05:03.064006-06:00","labels":["integration","testing","unit-tests"],"dependencies":[{"issue_id":"meld-eq0.11.3","depends_on_id":"meld-eq0.11","type":"parent-child","created_at":"2026-01-15T23:52:56.776814-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.11.3","depends_on_id":"meld-eq0.11.2","type":"blocks","created_at":"2026-01-15T23:54:14.904584-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.11.4","title":"11.4: Implement E2E test scripts with detailed logging","description":"# Subtask 11.4: End-to-End Tests with Detailed Logging\n\n## What to Implement\nComplete E2E test scripts that validate the full system with comprehensive logging for debugging.\n\n## Location\n`tests/e2e/` and `tests/e2e/run_e2e.py`\n\n## Why E2E Tests?\n- Validate complete workflows from CLI to output\n- Catch integration issues unit tests miss\n- Provide confidence for releases\n- Detailed logs help debug failures\n\n## E2E Test Scenarios\n\n### 1. Happy Path Test\n```python\n\"\"\"tests/e2e/test_happy_path.py\"\"\"\nimport pytest\nimport logging\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\nclass TestHappyPath:\n    @pytest.mark.e2e\n    @pytest.mark.asyncio\n    async def test_full_convergence_flow(\n        self,\n        temp_dir,\n        all_mock_adapters,\n        caplog,\n    ):\n        \"\"\"\n        Test complete flow: task -\u003e plan -\u003e feedback -\u003e synthesis -\u003e convergence\n        \n        Expected:\n        - Session created with unique ID\n        - Initial plan generated\n        - All advisors provide feedback\n        - Synthesis incorporates feedback\n        - Convergence detected within max_rounds\n        - All artifacts written correctly\n        \"\"\"\n        caplog.set_level(logging.DEBUG)\n        \n        logger.info(\"=\" * 60)\n        logger.info(\"E2E TEST: Full Convergence Flow\")\n        logger.info(\"=\" * 60)\n        \n        # Configure mocks for convergence scenario\n        configure_convergence_mocks(all_mock_adapters)\n        \n        task = \"Build a REST API for user management\"\n        \n        logger.info(f\"Task: {task}\")\n        logger.info(f\"Working directory: {temp_dir}\")\n        \n        # Run orchestration\n        result = await run_meld_session(\n            task=task,\n            run_dir=temp_dir,\n            adapters=all_mock_adapters,\n            max_rounds=5,\n        )\n        \n        # Log result\n        logger.info(\"-\" * 40)\n        logger.info(\"RESULTS:\")\n        logger.info(f\"  Converged: {result.converged}\")\n        logger.info(f\"  Rounds: {result.rounds_completed}\")\n        logger.info(f\"  Advisors: {result.advisor_participation}\")\n        \n        # Assertions with logging\n        assert_with_log(result.converged, \"Session should converge\")\n        assert_with_log(result.rounds_completed \u003c= 5, \"Should converge within max rounds\")\n        \n        # Verify artifacts\n        session_dir = result.session_dir\n        logger.info(f\"Session dir: {session_dir}\")\n        \n        artifacts = list(session_dir.glob(\"*\"))\n        logger.info(f\"Artifacts: {[a.name for a in artifacts]}\")\n        \n        assert_with_log(\n            (session_dir / \"task.md\").exists(),\n            \"task.md should exist\"\n        )\n        assert_with_log(\n            (session_dir / \"final-plan.md\").exists(),\n            \"final-plan.md should exist\"\n        )\n        assert_with_log(\n            (session_dir / \"session.json\").exists(),\n            \"session.json should exist\"\n        )\n        \n        logger.info(\"=\" * 60)\n        logger.info(\"E2E TEST PASSED: Full Convergence Flow\")\n        logger.info(\"=\" * 60)\n```\n\n### 2. Failure Scenarios\n```python\n\"\"\"tests/e2e/test_failure_scenarios.py\"\"\"\nimport pytest\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TestFailureScenarios:\n    @pytest.mark.e2e\n    @pytest.mark.asyncio\n    async def test_all_advisors_fail(self, temp_dir, caplog):\n        \"\"\"Test graceful degradation when all advisors fail.\"\"\"\n        caplog.set_level(logging.DEBUG)\n        \n        logger.info(\"E2E TEST: All Advisors Fail\")\n        \n        # Configure all mocks to fail\n        adapters = [\n            MockAdapterPresets.failure(\"claude\", AdvisorError.TIMEOUT),\n            MockAdapterPresets.failure(\"gemini\", AdvisorError.AUTH_FAILED),\n            MockAdapterPresets.failure(\"codex\", AdvisorError.NETWORK_ERROR),\n        ]\n        \n        result = await run_meld_session(\n            task=\"Test task\",\n            run_dir=temp_dir,\n            adapters=adapters,\n        )\n        \n        logger.info(f\"Result: converged={result.converged}, warnings={result.warnings}\")\n        \n        assert not result.converged\n        assert len(result.warnings) \u003e 0\n        logger.info(\"E2E TEST PASSED: Handled all advisors failing\")\n    \n    @pytest.mark.e2e\n    @pytest.mark.asyncio\n    async def test_one_advisor_fails_continues(self, temp_dir, caplog):\n        \"\"\"Test that session continues with 2/3 advisors.\"\"\"\n        caplog.set_level(logging.DEBUG)\n        \n        logger.info(\"E2E TEST: Partial Advisor Failure\")\n        \n        adapters = [\n            MockAdapterPresets.success(\"claude\"),\n            MockAdapterPresets.failure(\"gemini\", AdvisorError.CLI_NOT_FOUND),\n            MockAdapterPresets.success(\"codex\"),\n        ]\n        \n        result = await run_meld_session(\n            task=\"Test task\",\n            run_dir=temp_dir,\n            adapters=adapters,\n        )\n        \n        logger.info(f\"Participation: {result.advisor_participation}\")\n        \n        assert result.advisor_participation[\"claude\"] == True\n        assert result.advisor_participation[\"gemini\"] == False\n        assert result.advisor_participation[\"codex\"] == True\n        logger.info(\"E2E TEST PASSED: Continued with 2/3 advisors\")\n    \n    @pytest.mark.e2e\n    @pytest.mark.asyncio\n    async def test_max_rounds_reached(self, temp_dir, caplog):\n        \"\"\"Test behavior when max rounds reached without convergence.\"\"\"\n        caplog.set_level(logging.DEBUG)\n        \n        logger.info(\"E2E TEST: Max Rounds Without Convergence\")\n        \n        # Configure mocks to never converge\n        adapters = create_non_converging_mocks()\n        \n        result = await run_meld_session(\n            task=\"Complex task that wont converge\",\n            run_dir=temp_dir,\n            adapters=adapters,\n            max_rounds=3,\n        )\n        \n        logger.info(f\"Rounds: {result.rounds_completed}, Converged: {result.converged}\")\n        \n        assert not result.converged\n        assert result.rounds_completed == 3\n        assert (result.session_dir / \"final-plan.md\").exists()\n        logger.info(\"E2E TEST PASSED: Max rounds handled correctly\")\n```\n\n### 3. Resume Test\n```python\n\"\"\"tests/e2e/test_resume.py\"\"\"\nimport pytest\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass TestResume:\n    @pytest.mark.e2e\n    @pytest.mark.asyncio\n    async def test_resume_from_interrupt(self, temp_dir, caplog):\n        \"\"\"Test resuming an interrupted session.\"\"\"\n        caplog.set_level(logging.DEBUG)\n        \n        logger.info(\"E2E TEST: Resume From Interrupt\")\n        \n        # Start a session and simulate interrupt after round 1\n        session_id = await start_and_interrupt_session(\n            temp_dir,\n            interrupt_after_round=1,\n        )\n        \n        logger.info(f\"Interrupted session: {session_id}\")\n        \n        # Verify session marked as interrupted\n        session = load_session(temp_dir, session_id)\n        assert session.state.status == SessionStatus.INTERRUPTED\n        logger.info(f\"Session status: {session.state.status}\")\n        \n        # Resume session\n        result = await resume_meld_session(\n            run_dir=temp_dir,\n            session_id=session_id,\n        )\n        \n        logger.info(f\"Resumed result: converged={result.converged}\")\n        \n        # Should continue from round 2\n        assert result.rounds_completed \u003e 1\n        logger.info(\"E2E TEST PASSED: Resume from interrupt\")\n```\n\n## Test Runner Script\n```python\n\"\"\"tests/e2e/run_e2e.py\"\"\"\n#!/usr/bin/env python3\n\"\"\"\nE2E Test Runner with Detailed Logging\n\nUsage:\n    python -m tests.e2e.run_e2e\n    python -m tests.e2e.run_e2e --verbose\n    python -m tests.e2e.run_e2e --scenario happy_path\n\"\"\"\nimport argparse\nimport logging\nimport sys\nfrom datetime import datetime\nfrom pathlib import Path\n\ndef setup_logging(log_dir: Path, verbose: bool = False):\n    \"\"\"Configure comprehensive logging.\"\"\"\n    log_dir.mkdir(exist_ok=True)\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = log_dir / f\"e2e_test_{timestamp}.log\"\n    \n    # Detailed format for file\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setLevel(logging.DEBUG)\n    file_handler.setFormatter(logging.Formatter(\n        \"%(asctime)s | %(levelname)-8s | %(name)s | %(message)s\"\n    ))\n    \n    # Simpler format for console\n    console_handler = logging.StreamHandler()\n    console_handler.setLevel(logging.INFO if not verbose else logging.DEBUG)\n    console_handler.setFormatter(logging.Formatter(\n        \"%(levelname)-8s | %(message)s\"\n    ))\n    \n    root = logging.getLogger()\n    root.setLevel(logging.DEBUG)\n    root.addHandler(file_handler)\n    root.addHandler(console_handler)\n    \n    return log_file\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Run E2E tests\")\n    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\")\n    parser.add_argument(\"--scenario\", choices=[\"happy_path\", \"failures\", \"resume\", \"all\"], default=\"all\")\n    parser.add_argument(\"--log-dir\", default=\"test-logs\")\n    args = parser.parse_args()\n    \n    log_dir = Path(args.log_dir)\n    log_file = setup_logging(log_dir, args.verbose)\n    \n    logger = logging.getLogger(__name__)\n    logger.info(\"=\" * 60)\n    logger.info(\"MELD E2E TEST SUITE\")\n    logger.info(f\"Log file: {log_file}\")\n    logger.info(\"=\" * 60)\n    \n    # Run pytest with appropriate markers\n    import pytest\n    \n    pytest_args = [\n        \"tests/e2e/\",\n        \"-v\",\n        \"--tb=long\",\n        f\"--log-file={log_file}\",\n        \"--log-file-level=DEBUG\",\n    ]\n    \n    if args.scenario != \"all\":\n        pytest_args.append(f\"-k={args.scenario}\")\n    \n    exit_code = pytest.main(pytest_args)\n    \n    logger.info(\"=\" * 60)\n    logger.info(f\"TEST SUITE COMPLETE - Exit code: {exit_code}\")\n    logger.info(f\"Full logs: {log_file}\")\n    logger.info(\"=\" * 60)\n    \n    return exit_code\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n```\n\n## Logging Utilities\n```python\n\"\"\"tests/e2e/logging_utils.py\"\"\"\nimport logging\nimport functools\nfrom contextlib import contextmanager\n\nlogger = logging.getLogger(__name__)\n\ndef assert_with_log(condition: bool, message: str):\n    \"\"\"Assert with logging.\"\"\"\n    if condition:\n        logger.info(f\"✓ PASS: {message}\")\n    else:\n        logger.error(f\"✗ FAIL: {message}\")\n    assert condition, message\n\n@contextmanager\ndef log_section(name: str):\n    \"\"\"Context manager for logging test sections.\"\"\"\n    logger.info(\"-\" * 40)\n    logger.info(f\"SECTION: {name}\")\n    logger.info(\"-\" * 40)\n    try:\n        yield\n    finally:\n        logger.info(f\"END SECTION: {name}\")\n\ndef log_test(func):\n    \"\"\"Decorator to log test execution.\"\"\"\n    @functools.wraps(func)\n    async def wrapper(*args, **kwargs):\n        logger.info(f\"\u003e\u003e\u003e START TEST: {func.__name__}\")\n        try:\n            result = await func(*args, **kwargs)\n            logger.info(f\"\u003c\u003c\u003c PASS TEST: {func.__name__}\")\n            return result\n        except Exception as e:\n            logger.error(f\"\u003c\u003c\u003c FAIL TEST: {func.__name__}: {e}\")\n            raise\n    return wrapper\n```\n\n## Acceptance Criteria\n- [ ] Happy path E2E test passes\n- [ ] Failure scenario tests pass\n- [ ] Resume test passes\n- [ ] All tests produce detailed logs\n- [ ] Log files include timestamps\n- [ ] Can run specific scenarios\n- [ ] Test runner script works\n- [ ] Logs capture full execution trace\n- [ ] Tests run without real CLIs","notes":"## REQUIRED E2E TEST SCENARIOS\n\nAll scenarios must be implemented with detailed logging.\n\n### 1. Happy Path - Full Convergence\n```python\n@pytest.mark.e2e\nasync def test_full_convergence():\n    \"\"\"\n    Task -\u003e Initial plan -\u003e Feedback rounds -\u003e Convergence\n    Validates: Session created, all artifacts written, exit code 0\n    \"\"\"\n```\n\n### 2. Partial Advisor Failure\n```python\n@pytest.mark.e2e\nasync def test_partial_advisor_failure():\n    \"\"\"\n    1 advisor fails (e.g., timeout), session continues with 2\n    Validates: Warning logged, convergence still possible\n    \"\"\"\n```\n\n### 3. Total Advisor Failure\n```python\n@pytest.mark.e2e\nasync def test_all_advisors_fail():\n    \"\"\"\n    All 3 advisors fail, graceful degradation\n    Validates: Exit code 3, best plan so far saved\n    \"\"\"\n```\n\n### 4. Max Rounds Reached\n```python\n@pytest.mark.e2e\nasync def test_max_rounds_without_convergence():\n    \"\"\"\n    Reaches max rounds without converging\n    Validates: Exit code 1, final plan saved, round count correct\n    \"\"\"\n```\n\n### 5. Resume from Interrupt\n```python\n@pytest.mark.e2e\nasync def test_resume_from_interrupt():\n    \"\"\"\n    Start session, interrupt after round 1, resume, complete\n    Validates: Session restored, skips completed work\n    \"\"\"\n```\n\n### 6. Quiet Mode Output\n```python\n@pytest.mark.e2e\nasync def test_quiet_mode():\n    \"\"\"\n    Run with -q flag, verify stdout/stderr separation\n    Validates: Progress to stderr, plan to stdout, no TUI\n    \"\"\"\n```\n\n### 7. JSON Output Generation\n```python\n@pytest.mark.e2e\nasync def test_json_output():\n    \"\"\"\n    Run with --json-output, verify schema\n    Validates: Valid JSON, correct schema, all fields present\n    \"\"\"\n```\n\n### 8. Verbose Mode\n```python\n@pytest.mark.e2e\nasync def test_verbose_mode():\n    \"\"\"\n    Run with --verbose, verify raw outputs included\n    Validates: Advisor outputs in final document\n    \"\"\"\n```\n\n### 9. Preflight Failure\n```python\n@pytest.mark.e2e\nasync def test_preflight_failure():\n    \"\"\"\n    Missing CLIs cause preflight failure\n    Validates: Exit code 2, clear error message\n    \"\"\"\n```\n\n### 10. Graceful Shutdown (Ctrl+C)\n```python\n@pytest.mark.e2e\nasync def test_graceful_shutdown():\n    \"\"\"\n    Send SIGINT during run, verify clean shutdown\n    Validates: Exit code 5, session marked interrupted, subprocesses killed\n    \"\"\"\n```\n\n### 11. Secret Redaction\n```python\n@pytest.mark.e2e\nasync def test_secret_redaction():\n    \"\"\"\n    Task contains API key, verify not in artifacts\n    Validates: Secrets redacted in all output files\n    \"\"\"\n```\n\n### 12. Oscillation Detection\n```python\n@pytest.mark.e2e\nasync def test_oscillation_detection():\n    \"\"\"\n    Configure advisors to create cycling feedback\n    Validates: Oscillation detected, 'Needs Human Decision' output\n    \"\"\"\n```\n\n### Test Logging Requirements\nEach test MUST:\n- Log test name and purpose at start\n- Log each major step with timestamps\n- Log all API/CLI invocations\n- Log assertion details on failure\n- Write full logs to test-results.log\n- Output summary to console","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:53:43.267129-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:04:50.999521-06:00","labels":["e2e","logging","testing"],"dependencies":[{"issue_id":"meld-eq0.11.4","depends_on_id":"meld-eq0.11","type":"parent-child","created_at":"2026-01-15T23:53:43.281256-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.11.4","depends_on_id":"meld-eq0.11.3","type":"blocks","created_at":"2026-01-15T23:54:15.205915-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.2","title":"Task 2: Session Manager \u0026 Persistence","description":"# Task 2: Session Manager \u0026 Persistence\n\n## Overview\nImplement the session management system that creates unique run directories, writes artifacts incrementally, and supports crash recovery via `--resume`.\n\n## Why Session Persistence Matters\n1. **Crash Recovery**: If the process dies mid-round, users can resume without re-running completed work\n2. **Debugging**: All artifacts are preserved for inspection and debugging\n3. **Audit Trail**: Complete history of what each advisor said and how the plan evolved\n4. **PRD Requirement**: \"No data loss if interrupted\"\n\n## Design Decisions\n\n### Session ID Format\n`{ISO-timestamp}-{random-6-chars}`\nExample: `2026-01-16T02-47-17Z-abc123`\n\nRationale:\n- Timestamp prefix enables sorting by creation time\n- Random suffix prevents collisions\n- Colons replaced with dashes for filesystem compatibility\n\n### Artifact Structure\n```\n.meld/\n  runs/\n    2026-01-16T02-47-17Z-abc123/\n      session.json          # Run metadata, status, config\n      task.md               # Original task input\n      prd.md                # PRD content (if provided)\n      plan.round0.md        # Initial plan (before feedback)\n      plan.round1.md        # Plan after round 1\n      advisor.claude.round1.md\n      advisor.gemini.round1.md\n      advisor.codex.round1.md\n      plan.round2.md\n      advisor.claude.round2.md\n      ...\n      final-plan.md         # Final output (on completion)\n      events.jsonl          # Structured event log\n```\n\n### Write Strategy: Incremental + Atomic\n1. Write artifacts immediately after each operation completes\n2. Use atomic writes (write to temp, then rename)\n3. Update session.json after each checkpoint\n\n### Why JSONL for Events?\n- Append-only format survives crashes\n- Each line is independently parseable\n- Easy to tail -f for debugging\n- Common format for log analysis tools\n\n## Technical Requirements\n\n### Session State Machine\n```\nPENDING → IN_PROGRESS → COMPLETED\n                 ↓\n            INTERRUPTED → (resume) → IN_PROGRESS\n                 ↓\n               FAILED\n```\n\n### session.json Schema\n```json\n{\n  \"id\": \"2026-01-16T02-47-17Z-abc123\",\n  \"status\": \"in_progress\",\n  \"current_round\": 2,\n  \"interrupted_at\": null,\n  \"max_rounds\": 5,\n  \"started\": \"2026-01-16T02:47:17Z\",\n  \"updated\": \"2026-01-16T02:52:00Z\",\n  \"config\": {\n    \"timeout\": 600,\n    \"prd_file\": \"requirements.md\"\n  },\n  \"advisors\": {\n    \"claude\": \"completed\",\n    \"gemini\": \"failed\",\n    \"codex\": \"completed\"\n  },\n  \"convergence\": {\n    \"status\": \"continuing\",\n    \"open_items\": 2,\n    \"diff_ratio\": 0.08\n  }\n}\n```\n\n### Key Operations\n1. `create_session()` - Initialize new run directory\n2. `load_session(run_id)` - Load existing session for resume\n3. `write_artifact(name, content)` - Write file atomically\n4. `update_status(status, **fields)` - Update session.json\n5. `append_event(event)` - Append to events.jsonl\n6. `get_last_checkpoint()` - Find resume point\n\n## Acceptance Criteria\n- [ ] Run directory created with unique ID\n- [ ] task.md and prd.md written on session start\n- [ ] plan.roundN.md written after each plan generation\n- [ ] advisor.*.roundN.md written after each advisor response\n- [ ] session.json updated after each operation\n- [ ] events.jsonl appended to correctly\n- [ ] Resume loads session state correctly\n- [ ] Atomic writes prevent corruption\n- [ ] Directory permissions set correctly (0o755)\n\n## Edge Cases\n1. **Disk full**: Detect and report clearly\n2. **Permission denied**: Check at session start\n3. **Concurrent access**: Use file locking\n4. **Unicode filenames**: Avoid in run IDs","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:03:29.874725-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:03:29.874725-06:00","labels":["foundation","persistence","phase-1","session"],"dependencies":[{"issue_id":"meld-eq0.2","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:03:29.878171-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.2","depends_on_id":"meld-eq0.1","type":"blocks","created_at":"2026-01-15T23:42:59.275229-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.2.1","title":"2.1: Implement session ID generation and directory creation","description":"# Subtask 2.1: Session ID \u0026 Directory Creation\n\n## What to Implement\nGenerate unique session IDs and create the run directory structure.\n\n## Session ID Format\n`{ISO-timestamp}-{random-6-chars}`\n\n### Implementation\n\n```python\nimport secrets\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\ndef generate_session_id() -\u003e str:\n    \"\"\"\n    Generate unique session ID.\n    \n    Format: YYYY-MM-DDTHH-MM-SSZ-xxxxxx\n    Example: 2026-01-16T02-47-17Z-abc123\n    \n    Uses:\n    - UTC timestamp for consistency\n    - Dashes instead of colons for filesystem compatibility\n    - 6 random chars (36^6 = 2 billion combinations)\n    \"\"\"\n    timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H-%M-%SZ')\n    random_suffix = secrets.token_hex(3)  # 6 hex chars\n    return f\"{timestamp}-{random_suffix}\"\n\ndef create_session_directory(\n    session_id: str,\n    base_dir: Path | str = '.meld/runs'\n) -\u003e Path:\n    \"\"\"\n    Create session directory with proper structure.\n    \n    Args:\n        session_id: Unique session identifier\n        base_dir: Base directory for runs (default: .meld/runs)\n        \n    Returns:\n        Path to created session directory\n        \n    Raises:\n        OSError: If directory creation fails\n    \"\"\"\n    base = Path(base_dir)\n    session_dir = base / session_id\n    \n    # Create with parents and explicit permissions\n    session_dir.mkdir(parents=True, exist_ok=False, mode=0o755)\n    \n    return session_dir\n```\n\n## Directory Structure Created\n```\n.meld/\n  runs/\n    {session_id}/    # Empty initially, populated by other operations\n```\n\n## Error Handling\n\n### Directory Already Exists\nShould never happen due to random suffix, but handle gracefully:\n```python\nif session_dir.exists():\n    # Regenerate with new random suffix\n    return create_session_directory(generate_session_id(), base_dir)\n```\n\n### Permission Denied\n```python\ntry:\n    session_dir.mkdir(...)\nexcept PermissionError:\n    raise SessionError(f\"Cannot create session directory: permission denied at {base_dir}\")\n```\n\n### Disk Full\n```python\nexcept OSError as e:\n    if e.errno == errno.ENOSPC:\n        raise SessionError(\"Cannot create session: disk full\")\n    raise\n```\n\n## Acceptance Criteria\n- [ ] Session IDs are unique (test with 1000 rapid generations)\n- [ ] Timestamp portion is valid ISO format\n- [ ] Random suffix is 6 hex characters\n- [ ] Directory created with 755 permissions\n- [ ] Parent directories created if needed\n- [ ] Clear error on permission denied\n- [ ] Clear error on disk full\n\n## Testing Strategy\n```python\ndef test_session_id_uniqueness():\n    ids = [generate_session_id() for _ in range(1000)]\n    assert len(ids) == len(set(ids)), \"Duplicate session IDs generated\"\n\ndef test_session_id_format():\n    sid = generate_session_id()\n    # Should match: 2026-01-16T02-47-17Z-abc123\n    import re\n    pattern = r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}Z-[a-f0-9]{6}$'\n    assert re.match(pattern, sid)\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:05:00.743615-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:05:00.743615-06:00","labels":["foundation","session"],"dependencies":[{"issue_id":"meld-eq0.2.1","depends_on_id":"meld-eq0.2","type":"parent-child","created_at":"2026-01-15T23:05:00.744432-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.2.2","title":"2.2: Implement atomic file writing for artifacts","description":"# Subtask 2.2: Atomic File Writing\n\n## What to Implement\nSafe, atomic file writing that prevents corruption from crashes or concurrent access.\n\n## Why Atomic Writes?\nIf the process crashes while writing:\n- **Non-atomic**: File may be partially written, corrupted\n- **Atomic**: Either old content exists or new content exists, never partial\n\n## Implementation Pattern\n\n```python\nimport os\nimport tempfile\nfrom pathlib import Path\n\ndef write_artifact(\n    session_dir: Path,\n    filename: str,\n    content: str,\n    encoding: str = 'utf-8'\n) -\u003e Path:\n    \"\"\"\n    Write artifact file atomically.\n    \n    Uses write-to-temp-then-rename pattern for atomicity.\n    \n    Args:\n        session_dir: Session directory path\n        filename: Name of file to write\n        content: Content to write\n        encoding: Text encoding (default: utf-8)\n        \n    Returns:\n        Path to written file\n        \n    Raises:\n        OSError: If write fails\n    \"\"\"\n    target_path = session_dir / filename\n    \n    # Create temp file in same directory (same filesystem for atomic rename)\n    fd, temp_path = tempfile.mkstemp(\n        dir=session_dir,\n        prefix=f'.{filename}.',\n        suffix='.tmp'\n    )\n    \n    try:\n        # Write content to temp file\n        with os.fdopen(fd, 'w', encoding=encoding) as f:\n            f.write(content)\n            f.flush()\n            os.fsync(f.fileno())  # Force write to disk\n        \n        # Atomic rename\n        os.rename(temp_path, target_path)\n        \n        return target_path\n        \n    except Exception:\n        # Clean up temp file on failure\n        try:\n            os.unlink(temp_path)\n        except OSError:\n            pass\n        raise\n```\n\n## Artifact Types\n\n### Text Artifacts\n- `task.md`: Original task input\n- `prd.md`: PRD content\n- `plan.round{N}.md`: Plan after round N\n- `advisor.{name}.round{N}.md`: Advisor feedback\n- `final-plan.md`: Final output\n\n### JSON Artifacts\n- `session.json`: Session metadata\n\n### JSONL Artifacts (append-only)\n- `events.jsonl`: Event log\n\n## Append-Only JSONL Writing\n\n```python\nimport json\nimport fcntl\n\ndef append_event(session_dir: Path, event: dict) -\u003e None:\n    \"\"\"\n    Append event to events.jsonl with file locking.\n    \n    Each event is a single JSON line. File locking prevents\n    concurrent append corruption.\n    \n    Args:\n        session_dir: Session directory path\n        event: Event dictionary to append\n    \"\"\"\n    events_path = session_dir / 'events.jsonl'\n    \n    # Add timestamp if not present\n    if 'timestamp' not in event:\n        event['timestamp'] = datetime.now(timezone.utc).isoformat()\n    \n    line = json.dumps(event, ensure_ascii=False) + '\\n'\n    \n    with open(events_path, 'a', encoding='utf-8') as f:\n        # Acquire exclusive lock\n        fcntl.flock(f.fileno(), fcntl.LOCK_EX)\n        try:\n            f.write(line)\n            f.flush()\n            os.fsync(f.fileno())\n        finally:\n            fcntl.flock(f.fileno(), fcntl.LOCK_UN)\n```\n\n## Event Schema\n\n```json\n{\"timestamp\": \"2026-01-16T02:47:17Z\", \"event\": \"session_started\", \"session_id\": \"...\"}\n{\"timestamp\": \"2026-01-16T02:47:20Z\", \"event\": \"plan_generated\", \"round\": 0}\n{\"timestamp\": \"2026-01-16T02:48:00Z\", \"event\": \"advisor_started\", \"advisor\": \"claude\", \"round\": 1}\n{\"timestamp\": \"2026-01-16T02:49:30Z\", \"event\": \"advisor_completed\", \"advisor\": \"claude\", \"round\": 1, \"duration_ms\": 90000}\n{\"timestamp\": \"2026-01-16T02:49:35Z\", \"event\": \"advisor_failed\", \"advisor\": \"gemini\", \"round\": 1, \"error\": \"timeout\"}\n```\n\n## Acceptance Criteria\n- [ ] Files written atomically (verify with crash simulation)\n- [ ] Temp files cleaned up on failure\n- [ ] fsync forces data to disk\n- [ ] events.jsonl is append-only\n- [ ] File locking prevents corruption\n- [ ] UTF-8 encoding used consistently\n- [ ] No partial writes possible\n\n## Testing Strategy\n```python\ndef test_atomic_write_on_crash():\n    \"\"\"Simulate crash during write, verify no corruption.\"\"\"\n    # Write initial content\n    write_artifact(session_dir, 'test.md', 'original content')\n    \n    # Simulate crash during second write (mock os.rename to fail)\n    with patch('os.rename', side_effect=OSError(\"Simulated crash\")):\n        with pytest.raises(OSError):\n            write_artifact(session_dir, 'test.md', 'new content')\n    \n    # Original should still exist\n    assert (session_dir / 'test.md').read_text() == 'original content'\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:06:00.395037-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:06:00.395037-06:00","labels":["foundation","io","session"],"dependencies":[{"issue_id":"meld-eq0.2.2","depends_on_id":"meld-eq0.2","type":"parent-child","created_at":"2026-01-15T23:06:00.395814-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.2.2","depends_on_id":"meld-eq0.2.1","type":"blocks","created_at":"2026-01-15T23:42:59.411116-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.2.3","title":"2.3: Implement session.json management","description":"# Subtask 2.3: session.json Management\n\n## What to Implement\nCreate and update the session.json metadata file that tracks run state.\n\n## session.json Schema\n\n```python\nfrom dataclasses import dataclass, field, asdict\nfrom datetime import datetime\nfrom enum import Enum\nfrom typing import Any\nimport json\n\nclass SessionStatus(str, Enum):\n    PENDING = 'pending'\n    IN_PROGRESS = 'in_progress'\n    COMPLETED = 'completed'\n    FAILED = 'failed'\n    INTERRUPTED = 'interrupted'\n\nclass AdvisorStatus(str, Enum):\n    PENDING = 'pending'\n    RUNNING = 'running'\n    COMPLETED = 'completed'\n    FAILED = 'failed'\n    SKIPPED = 'skipped'\n\nclass InterruptedAt(str, Enum):\n    PLANNING = 'planning'\n    FEEDBACK = 'feedback'\n    SYNTHESIS = 'synthesis'\n\n@dataclass\nclass SessionConfig:\n    timeout: int = 600\n    max_rounds: int = 5\n    prd_file: str | None = None\n    skip_preflight: bool = False\n    quiet: bool = False\n    verbose: bool = False\n\n@dataclass\nclass ConvergenceState:\n    status: str = 'continuing'  # 'converged' or 'continuing'\n    open_items: int = 0\n    changes_made: int = 0\n    diff_ratio: float = 0.0\n    rationale: str = ''\n\n@dataclass\nclass SessionState:\n    id: str\n    status: SessionStatus = SessionStatus.PENDING\n    current_round: int = 0\n    interrupted_at: InterruptedAt | None = None\n    started: str = ''  # ISO datetime\n    updated: str = ''  # ISO datetime\n    config: SessionConfig = field(default_factory=SessionConfig)\n    advisors: dict[str, AdvisorStatus] = field(default_factory=lambda: {\n        'claude': AdvisorStatus.PENDING,\n        'gemini': AdvisorStatus.PENDING,\n        'codex': AdvisorStatus.PENDING,\n    })\n    convergence: ConvergenceState = field(default_factory=ConvergenceState)\n    \n    def to_dict(self) -\u003e dict[str, Any]:\n        \"\"\"Convert to JSON-serializable dict.\"\"\"\n        d = asdict(self)\n        # Convert enums to strings\n        d['status'] = self.status.value\n        d['interrupted_at'] = self.interrupted_at.value if self.interrupted_at else None\n        d['advisors'] = {k: v.value for k, v in self.advisors.items()}\n        return d\n    \n    @classmethod\n    def from_dict(cls, d: dict) -\u003e 'SessionState':\n        \"\"\"Create from dict (for loading).\"\"\"\n        # Convert strings back to enums\n        d['status'] = SessionStatus(d['status'])\n        if d.get('interrupted_at'):\n            d['interrupted_at'] = InterruptedAt(d['interrupted_at'])\n        d['advisors'] = {k: AdvisorStatus(v) for k, v in d['advisors'].items()}\n        d['config'] = SessionConfig(**d['config'])\n        d['convergence'] = ConvergenceState(**d['convergence'])\n        return cls(**d)\n```\n\n## Session Manager Class\n\n```python\nclass SessionManager:\n    \"\"\"Manages session lifecycle and persistence.\"\"\"\n    \n    def __init__(self, session_dir: Path, state: SessionState):\n        self.session_dir = session_dir\n        self.state = state\n        self._json_path = session_dir / 'session.json'\n    \n    @classmethod\n    def create_new(\n        cls,\n        base_dir: Path,\n        config: SessionConfig,\n    ) -\u003e 'SessionManager':\n        \"\"\"Create a new session.\"\"\"\n        session_id = generate_session_id()\n        session_dir = create_session_directory(session_id, base_dir)\n        \n        now = datetime.now(timezone.utc).isoformat()\n        state = SessionState(\n            id=session_id,\n            status=SessionStatus.PENDING,\n            started=now,\n            updated=now,\n            config=config,\n        )\n        \n        manager = cls(session_dir, state)\n        manager._write_state()\n        return manager\n    \n    @classmethod\n    def load(cls, base_dir: Path, session_id: str) -\u003e 'SessionManager':\n        \"\"\"Load existing session for resume.\"\"\"\n        session_dir = base_dir / session_id\n        json_path = session_dir / 'session.json'\n        \n        if not json_path.exists():\n            raise SessionError(f\"Session not found: {session_id}\")\n        \n        with open(json_path, 'r') as f:\n            data = json.load(f)\n        \n        state = SessionState.from_dict(data)\n        return cls(session_dir, state)\n    \n    def update_status(self, status: SessionStatus, **kwargs) -\u003e None:\n        \"\"\"Update session status and any additional fields.\"\"\"\n        self.state.status = status\n        self.state.updated = datetime.now(timezone.utc).isoformat()\n        \n        for key, value in kwargs.items():\n            if hasattr(self.state, key):\n                setattr(self.state, key, value)\n        \n        self._write_state()\n    \n    def _write_state(self) -\u003e None:\n        \"\"\"Write session.json atomically.\"\"\"\n        content = json.dumps(self.state.to_dict(), indent=2)\n        write_artifact(self.session_dir, 'session.json', content)\n```\n\n## State Transitions\n\n```\ncreate_new() → PENDING\nstart_run() → IN_PROGRESS\ncomplete_round() → IN_PROGRESS (update current_round)\nmark_converged() → COMPLETED\nmark_failed(error) → FAILED\nhandle_interrupt() → INTERRUPTED (set interrupted_at)\nresume() → IN_PROGRESS (clear interrupted_at)\n```\n\n## Acceptance Criteria\n- [ ] session.json created on new session\n- [ ] All fields serialize correctly\n- [ ] Status transitions work correctly\n- [ ] Updated timestamp changes on each update\n- [ ] Load from existing session works\n- [ ] Invalid session ID raises clear error\n- [ ] Dataclasses round-trip correctly (save → load)\n\n## Testing Strategy\n```python\ndef test_session_round_trip():\n    \"\"\"Session state survives save and load.\"\"\"\n    manager = SessionManager.create_new(tmp_dir, SessionConfig())\n    session_id = manager.state.id\n    \n    manager.update_status(SessionStatus.IN_PROGRESS, current_round=2)\n    \n    # Reload\n    loaded = SessionManager.load(tmp_dir, session_id)\n    assert loaded.state.status == SessionStatus.IN_PROGRESS\n    assert loaded.state.current_round == 2\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:06:48.792881-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:06:48.792881-06:00","labels":["foundation","session","state"],"dependencies":[{"issue_id":"meld-eq0.2.3","depends_on_id":"meld-eq0.2","type":"parent-child","created_at":"2026-01-15T23:06:48.795245-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.2.3","depends_on_id":"meld-eq0.2.2","type":"blocks","created_at":"2026-01-15T23:42:59.678875-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.2.4","title":"2.4: Implement --resume checkpoint recovery","description":"# Subtask 2.4: Resume/Checkpoint Recovery\n\n## What to Implement\nLogic to resume an interrupted session from the last completed checkpoint.\n\n## Resume Strategy\n\n### Checkpoint Granularity\nA checkpoint is saved after:\n1. Initial plan generation (round 0)\n2. All advisors complete for a round\n3. Synthesis complete for a round\n\n### Resume Points\nThe session can resume from:\n- **planning**: Re-generate initial plan\n- **feedback**: Re-run advisor feedback for current round\n- **synthesis**: Re-synthesize feedback into plan\n\n### Finding Resume Point\n\n```python\nfrom pathlib import Path\nfrom typing import Tuple\n\ndef find_resume_point(session_dir: Path) -\u003e Tuple[str, int]:\n    \"\"\"\n    Determine where to resume based on artifacts present.\n    \n    Returns:\n        (phase, round_number) - where to resume\n        \n    Logic:\n    1. Find highest round number with plan.roundN.md\n    2. Check if all advisors completed for that round\n    3. Return appropriate resume point\n    \"\"\"\n    # Find all plan files\n    plan_files = sorted(session_dir.glob('plan.round*.md'))\n    \n    if not plan_files:\n        # No plans yet, start from beginning\n        return ('planning', 0)\n    \n    # Get highest round with plan\n    latest_plan = plan_files[-1]\n    round_num = int(latest_plan.stem.split('round')[1])\n    \n    # Check if advisors completed for next round\n    advisors = ['claude', 'gemini', 'codex']\n    next_round = round_num + 1\n    \n    advisor_files = [\n        session_dir / f'advisor.{name}.round{next_round}.md'\n        for name in advisors\n    ]\n    \n    # Count how many advisor files exist\n    existing_advisors = sum(1 for f in advisor_files if f.exists())\n    \n    if existing_advisors == 0:\n        # No advisor feedback for next round, need to get feedback\n        return ('feedback', next_round)\n    elif existing_advisors \u003c 3:\n        # Partial advisor feedback, resume feedback (will handle existing)\n        return ('feedback', next_round)\n    else:\n        # All advisor feedback exists, need synthesis\n        return ('synthesis', next_round)\n```\n\n## Resume Handler\n\n```python\nclass ResumeHandler:\n    \"\"\"Handles session resume logic.\"\"\"\n    \n    def __init__(self, session_manager: SessionManager):\n        self.manager = session_manager\n        self.session_dir = session_manager.session_dir\n    \n    def prepare_resume(self) -\u003e dict:\n        \"\"\"\n        Prepare session for resumption.\n        \n        Returns:\n            dict with resume context:\n            {\n                'phase': 'feedback',\n                'round': 2,\n                'task': 'original task...',\n                'prd': 'prd content...' or None,\n                'current_plan': 'latest plan content...',\n                'existing_feedback': {'claude': '...', 'gemini': None, 'codex': '...'},\n            }\n        \"\"\"\n        phase, round_num = find_resume_point(self.session_dir)\n        \n        # Load task and PRD\n        task = (self.session_dir / 'task.md').read_text()\n        prd_path = self.session_dir / 'prd.md'\n        prd = prd_path.read_text() if prd_path.exists() else None\n        \n        # Load current plan (last completed)\n        if round_num == 0 and phase == 'planning':\n            current_plan = None\n        else:\n            plan_round = round_num - 1 if phase == 'feedback' else round_num\n            current_plan = (self.session_dir / f'plan.round{plan_round}.md').read_text()\n        \n        # Load existing feedback for this round (if any)\n        existing_feedback = {}\n        if phase in ('feedback', 'synthesis'):\n            for advisor in ['claude', 'gemini', 'codex']:\n                path = self.session_dir / f'advisor.{advisor}.round{round_num}.md'\n                existing_feedback[advisor] = path.read_text() if path.exists() else None\n        \n        # Update session state\n        self.manager.state.interrupted_at = None  # Clear interrupt\n        self.manager.state.status = SessionStatus.IN_PROGRESS\n        self.manager.state.current_round = round_num\n        self.manager._write_state()\n        \n        return {\n            'phase': phase,\n            'round': round_num,\n            'task': task,\n            'prd': prd,\n            'current_plan': current_plan,\n            'existing_feedback': existing_feedback,\n        }\n```\n\n## Validation Before Resume\n\n```python\ndef validate_session_for_resume(session_dir: Path) -\u003e list[str]:\n    \"\"\"\n    Check session is valid for resume.\n    \n    Returns:\n        List of validation errors (empty if valid)\n    \"\"\"\n    errors = []\n    \n    # Must have session.json\n    if not (session_dir / 'session.json').exists():\n        errors.append('Missing session.json - session may be corrupted')\n    \n    # Must have task.md\n    if not (session_dir / 'task.md').exists():\n        errors.append('Missing task.md - cannot resume without original task')\n    \n    # Load session state\n    try:\n        with open(session_dir / 'session.json') as f:\n            state = json.load(f)\n    except json.JSONDecodeError:\n        errors.append('Corrupted session.json - cannot parse')\n        return errors\n    \n    # Check status allows resume\n    status = state.get('status')\n    if status == 'completed':\n        errors.append('Session already completed - nothing to resume')\n    \n    return errors\n```\n\n## Acceptance Criteria\n- [ ] Correctly identifies resume point from artifacts\n- [ ] Loads task and PRD from session\n- [ ] Loads current plan state\n- [ ] Identifies which advisors completed\n- [ ] Clears interrupted_at flag on resume\n- [ ] Validates session before attempting resume\n- [ ] Clear error if session corrupted\n- [ ] Clear error if session already complete\n\n## Edge Cases\n1. **No artifacts**: Resume from beginning\n2. **Partial advisor files**: Resume feedback, skip completed advisors\n3. **Corrupted session.json**: Error with recovery suggestion\n4. **Already completed**: Error indicating nothing to resume","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:08:09.95309-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:08:09.95309-06:00","labels":["foundation","resume","session"],"dependencies":[{"issue_id":"meld-eq0.2.4","depends_on_id":"meld-eq0.2","type":"parent-child","created_at":"2026-01-15T23:08:09.956486-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.2.4","depends_on_id":"meld-eq0.2.3","type":"blocks","created_at":"2026-01-15T23:42:59.926568-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.2.5","title":"2.5: Implement secret redaction for artifacts","description":"# Subtask 2.5: Secret Redaction for Artifacts\n\n## Overview\nImplement automatic redaction of common secret patterns before writing artifacts to disk.\n\n## Why This Matters (NFR4)\nPRD Requirement: Persisted artifacts redact common secret patterns (API keys, tokens) by default\n\nUsers may paste sensitive information in tasks or PRDs. Advisor responses might echo these back. We must not persist secrets to disk.\n\n## Secret Patterns to Detect\n\n### API Keys and Tokens\n- OpenAI: sk-... patterns\n- Anthropic: sk-ant-... patterns\n- AWS: AKIA... access keys\n- Google: AIza... API keys\n- GitHub: ghp_, gho_, ghu_, ghs_, ghr_ tokens\n- Generic: api_key=, token=, password=, secret= patterns\n\n## Implementation\n\n### Redaction Function\nApply regex-based redaction before any file write. Replace matches with [REDACTED:TYPE].\n\n### Integration Points\n- Apply to write_artifact() by default\n- Skip for session.json (metadata only)\n- Make redaction opt-out via parameter\n\n## Acceptance Criteria\n- [ ] API key patterns detected and redacted (OpenAI, Anthropic, AWS, Google, GitHub)\n- [ ] Generic patterns caught (api_key=, token=, password=, secret=)\n- [ ] Redaction applied to task.md, prd.md, advisor outputs, plans\n- [ ] session.json NOT redacted (metadata only)\n- [ ] Pattern matching is case-insensitive\n- [ ] False positive rate is acceptable\n\n## Testing Strategy\nUnit tests for each pattern type plus integration test ensuring no secrets leak to disk.","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-16T00:02:47.455276-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:02:47.455276-06:00","labels":["foundation","security","session"],"dependencies":[{"issue_id":"meld-eq0.2.5","depends_on_id":"meld-eq0.2","type":"parent-child","created_at":"2026-01-16T00:02:47.468033-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.2.5","depends_on_id":"meld-eq0.2.2","type":"blocks","created_at":"2026-01-16T00:02:55.402098-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.3","title":"Task 3: Provider Adapter Interface","description":"# Task 3: Provider Adapter Interface\n\n## Overview\nDesign and implement the abstract base class for provider adapters that encapsulates CLI-specific behavior for Claude, Gemini, and Codex CLIs.\n\n## Why an Adapter Layer?\n\n### Problem: CLI Drift\nEach AI CLI tool:\n- Has different flags and options\n- May change between versions\n- Has different output formats\n- Handles errors differently\n- Has different authentication mechanisms\n\n### Solution: Adapter Pattern\nThe adapter pattern:\n1. **Encapsulates** CLI-specific details in one place\n2. **Normalizes** output to a consistent format\n3. **Isolates** changes when CLIs update\n4. **Enables** testing with mock adapters\n5. **Supports** adding new providers easily\n\n## Design Principles\n\n### Single Responsibility\nEach adapter handles exactly one CLI:\n- Command construction\n- Execution and streaming\n- Output parsing\n- Error categorization\n\n### Consistent Interface\nAll adapters expose the same async interface:\n```python\nasync def run(prompt: str) -\u003e AdvisorResult\nasync def stream(prompt: str) -\u003e AsyncIterator[StreamEvent]\nasync def check_available() -\u003e AvailabilityResult\n```\n\n### Streaming First\nAll adapters stream output for real-time TUI updates.\nFull response is assembled from stream for artifact storage.\n\n## Technical Requirements\n\n### AdvisorResult Data Model\n```python\n@dataclass\nclass AdvisorResult:\n    \"\"\"Result from an advisor invocation.\"\"\"\n    provider: str          # 'claude', 'gemini', 'codex'\n    success: bool          # Whether invocation succeeded\n    content: str           # Full response text\n    duration_ms: int       # Execution time\n    error: AdvisorError | None  # Error info if failed\n    metadata: dict         # Provider-specific metadata\n```\n\n### StreamEvent Data Model\n```python\n@dataclass\nclass StreamEvent:\n    \"\"\"Event from streaming output.\"\"\"\n    type: StreamEventType  # 'text', 'error', 'done'\n    content: str           # Text chunk or error message\n    timestamp: float       # Time since start (seconds)\n```\n\n### AdvisorError Categories\n```python\nclass AdvisorError(Enum):\n    CLI_NOT_FOUND = auto()     # CLI binary not installed\n    AUTH_FAILED = auto()       # Not authenticated\n    TIMEOUT = auto()           # Response took too long\n    RATE_LIMITED = auto()      # API quota exceeded\n    NETWORK_ERROR = auto()     # Connection issues\n    PARSE_ERROR = auto()       # Unexpected output format\n    INTERNAL_ERROR = auto()    # CLI internal error\n```\n\n### Base Adapter Class\n```python\nfrom abc import ABC, abstractmethod\n\nclass ProviderAdapter(ABC):\n    \"\"\"Abstract base class for AI CLI adapters.\"\"\"\n    \n    name: str  # 'claude', 'gemini', 'codex'\n    \n    @abstractmethod\n    async def run(\n        self,\n        prompt: str,\n        timeout: float = 600.0,\n        cwd: Path | None = None,\n    ) -\u003e AdvisorResult:\n        \"\"\"\n        Run CLI with prompt and return full result.\n        \n        Args:\n            prompt: The prompt to send\n            timeout: Maximum execution time in seconds\n            cwd: Working directory for CLI (default: current)\n            \n        Returns:\n            AdvisorResult with response or error\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def stream(\n        self,\n        prompt: str,\n        timeout: float = 600.0,\n        cwd: Path | None = None,\n    ) -\u003e AsyncIterator[StreamEvent]:\n        \"\"\"\n        Stream CLI output for real-time display.\n        \n        Yields:\n            StreamEvent objects as output arrives\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def check_available(self) -\u003e AvailabilityResult:\n        \"\"\"\n        Check if CLI is installed and authenticated.\n        \n        Returns:\n            AvailabilityResult with status and any issues\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def build_command(self, prompt: str) -\u003e list[str]:\n        \"\"\"\n        Build CLI command from prompt.\n        \n        Returns:\n            Command as list of strings for subprocess\n        \"\"\"\n        pass\n```\n\n## Subprocess Runner\n\n```python\nasync def run_subprocess(\n    cmd: list[str],\n    timeout: float,\n    cwd: Path | None = None,\n) -\u003e tuple[str, str, int]:\n    \"\"\"\n    Run subprocess with timeout and output capture.\n    \n    Returns:\n        (stdout, stderr, return_code)\n    \"\"\"\n    process = await asyncio.create_subprocess_exec(\n        *cmd,\n        cwd=cwd or os.getcwd(),\n        stdout=asyncio.subprocess.PIPE,\n        stderr=asyncio.subprocess.PIPE,\n    )\n    \n    try:\n        stdout, stderr = await asyncio.wait_for(\n            process.communicate(),\n            timeout=timeout\n        )\n        return stdout.decode(), stderr.decode(), process.returncode\n    except asyncio.TimeoutError:\n        process.kill()\n        await process.wait()\n        raise TimeoutError(f\"Process exceeded {timeout}s timeout\")\n```\n\n## Acceptance Criteria\n- [ ] ProviderAdapter ABC defined with all methods\n- [ ] AdvisorResult dataclass complete\n- [ ] StreamEvent dataclass complete\n- [ ] AdvisorError enum covers all cases\n- [ ] run_subprocess handles timeout correctly\n- [ ] All types have proper type hints\n- [ ] Docstrings explain behavior\n\n## Notes for Implementation\n- Use `typing.AsyncIterator` for stream return type\n- Use `abc.ABC` and `@abstractmethod` for base class\n- Prefer `asyncio.create_subprocess_exec` over `shell=True`\n- Always set explicit `cwd` for reproducibility","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:09:38.110548-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:09:38.110548-06:00","labels":["adapter","foundation","interface","phase-1"],"dependencies":[{"issue_id":"meld-eq0.3","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:09:38.118605-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.3","depends_on_id":"meld-eq0.1","type":"blocks","created_at":"2026-01-15T23:43:39.495095-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.3.1","title":"3.1: Define data models (AdvisorResult, StreamEvent, errors)","description":"# Subtask 3.1: Provider Data Models\n\n## What to Implement\nAll data classes and enums used by the provider adapter interface.\n\n## Location\n`meld/models.py` - Central location for all data models.\n\n## Implementations\n\n### AdvisorError Enum\n```python\nfrom enum import Enum, auto\n\nclass AdvisorError(Enum):\n    \"\"\"Categories of errors from advisor CLI invocations.\n    \n    These categories determine retry behavior and user messaging.\n    \"\"\"\n    CLI_NOT_FOUND = auto()     # CLI binary not installed\n    AUTH_FAILED = auto()       # Not authenticated to API\n    TIMEOUT = auto()           # Response exceeded time limit\n    RATE_LIMITED = auto()      # API quota exceeded\n    NETWORK_ERROR = auto()     # Connection/DNS issues\n    PARSE_ERROR = auto()       # Unexpected output format\n    INTERNAL_ERROR = auto()    # CLI internal failure\n    UNKNOWN = auto()           # Unrecognized error\n```\n\n### StreamEventType Enum\n```python\nclass StreamEventType(Enum):\n    \"\"\"Types of events in streaming output.\"\"\"\n    TEXT = 'text'       # Regular text chunk\n    ERROR = 'error'     # Error message\n    DONE = 'done'       # Stream complete\n    STATUS = 'status'   # Status update (e.g., \"thinking...\")\n```\n\n### StreamEvent Dataclass\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass StreamEvent:\n    \"\"\"\n    Event from streaming CLI output.\n    \n    Used by TUI to display real-time progress.\n    \n    Attributes:\n        type: Type of event (text, error, done)\n        content: Text content or error message\n        timestamp: Seconds since stream started\n    \"\"\"\n    type: StreamEventType\n    content: str\n    timestamp: float  # Time since start in seconds\n    \n    @classmethod\n    def text(cls, content: str, timestamp: float) -\u003e 'StreamEvent':\n        return cls(StreamEventType.TEXT, content, timestamp)\n    \n    @classmethod\n    def error(cls, message: str, timestamp: float) -\u003e 'StreamEvent':\n        return cls(StreamEventType.ERROR, message, timestamp)\n    \n    @classmethod\n    def done(cls, timestamp: float) -\u003e 'StreamEvent':\n        return cls(StreamEventType.DONE, '', timestamp)\n```\n\n### AdvisorResult Dataclass\n```python\n@dataclass\nclass AdvisorResult:\n    \"\"\"\n    Result from an advisor CLI invocation.\n    \n    Captures both successful responses and failures\n    in a uniform structure.\n    \n    Attributes:\n        provider: Name of the provider ('claude', 'gemini', 'codex')\n        success: Whether invocation succeeded\n        content: Full response text (empty on failure)\n        duration_ms: Execution time in milliseconds\n        error: Error category if failed\n        error_message: Human-readable error description\n        metadata: Provider-specific additional data\n    \"\"\"\n    provider: str\n    success: bool\n    content: str\n    duration_ms: int\n    error: AdvisorError | None = None\n    error_message: str = ''\n    metadata: dict = field(default_factory=dict)\n    \n    @classmethod\n    def success_result(\n        cls,\n        provider: str,\n        content: str,\n        duration_ms: int,\n        metadata: dict | None = None,\n    ) -\u003e 'AdvisorResult':\n        \"\"\"Create a successful result.\"\"\"\n        return cls(\n            provider=provider,\n            success=True,\n            content=content,\n            duration_ms=duration_ms,\n            metadata=metadata or {},\n        )\n    \n    @classmethod\n    def failure_result(\n        cls,\n        provider: str,\n        error: AdvisorError,\n        error_message: str,\n        duration_ms: int,\n    ) -\u003e 'AdvisorResult':\n        \"\"\"Create a failure result.\"\"\"\n        return cls(\n            provider=provider,\n            success=False,\n            content='',\n            duration_ms=duration_ms,\n            error=error,\n            error_message=error_message,\n        )\n```\n\n### AvailabilityResult Dataclass\n```python\n@dataclass\nclass AvailabilityResult:\n    \"\"\"\n    Result from checking CLI availability.\n    \n    Used by preflight checks and meld doctor.\n    \n    Attributes:\n        provider: Name of the provider\n        available: Whether CLI is installed and accessible\n        authenticated: Whether auth is configured (if checkable)\n        version: CLI version string (if available)\n        error: Error if not available\n        install_hint: How to install if missing\n    \"\"\"\n    provider: str\n    available: bool\n    authenticated: bool | None = None  # None if can't check\n    version: str = ''\n    error: str = ''\n    install_hint: str = ''\n```\n\n## Acceptance Criteria\n- [ ] All enums defined with appropriate values\n- [ ] All dataclasses have complete type hints\n- [ ] Factory methods work correctly\n- [ ] Docstrings explain purpose and usage\n- [ ] Can import from meld.models\n- [ ] No circular imports\n\n## Testing Strategy\n```python\ndef test_advisor_result_success():\n    result = AdvisorResult.success_result(\n        provider='claude',\n        content='Plan content here',\n        duration_ms=5000,\n    )\n    assert result.success\n    assert result.error is None\n\ndef test_advisor_result_failure():\n    result = AdvisorResult.failure_result(\n        provider='gemini',\n        error=AdvisorError.TIMEOUT,\n        error_message='Exceeded 600s timeout',\n        duration_ms=600000,\n    )\n    assert not result.success\n    assert result.error == AdvisorError.TIMEOUT\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:10:25.105587-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:10:25.105587-06:00","labels":["foundation","models","types"],"dependencies":[{"issue_id":"meld-eq0.3.1","depends_on_id":"meld-eq0.3","type":"parent-child","created_at":"2026-01-15T23:10:25.109581-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.3.2","title":"3.2: Implement ProviderAdapter abstract base class","description":"# Subtask 3.2: ProviderAdapter Abstract Base Class\n\n## What to Implement\nThe abstract base class that all provider adapters must implement.\n\n## Location\n`meld/providers/base.py`\n\n## Implementation\n\n```python\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import AsyncIterator\nimport asyncio\nimport os\nimport time\n\nfrom meld.models import (\n    AdvisorResult,\n    AdvisorError,\n    StreamEvent,\n    StreamEventType,\n    AvailabilityResult,\n)\n\n\nclass ProviderAdapter(ABC):\n    \"\"\"\n    Abstract base class for AI CLI provider adapters.\n    \n    Each adapter encapsulates the specifics of one CLI tool:\n    - Command construction with correct flags\n    - Output parsing and normalization\n    - Error categorization\n    - Streaming support\n    \n    Subclasses must implement:\n    - name: Provider identifier\n    - build_command(): Construct CLI command\n    - parse_output(): Parse CLI output\n    - categorize_error(): Map errors to categories\n    - check_available(): Verify CLI is ready\n    \n    The base class provides:\n    - run(): Full execution with result\n    - stream(): Streaming execution\n    - Subprocess management and timeout handling\n    \n    Example:\n        class ClaudeAdapter(ProviderAdapter):\n            name = 'claude'\n            \n            def build_command(self, prompt: str) -\u003e list[str]:\n                return ['claude', '-p', prompt, '--output-format', 'text']\n    \"\"\"\n    \n    name: str  # Must be set by subclass\n    \n    @abstractmethod\n    def build_command(self, prompt: str) -\u003e list[str]:\n        \"\"\"\n        Build CLI command for the given prompt.\n        \n        Args:\n            prompt: The prompt text to send to the AI\n            \n        Returns:\n            List of command parts for subprocess\n            \n        Example:\n            ['claude', '-p', 'What is 2+2?', '--output-format', 'text']\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def parse_output(self, stdout: str, stderr: str) -\u003e str:\n        \"\"\"\n        Parse CLI output to extract response content.\n        \n        Args:\n            stdout: Standard output from CLI\n            stderr: Standard error from CLI\n            \n        Returns:\n            Cleaned response text\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def categorize_error(\n        self,\n        returncode: int,\n        stdout: str,\n        stderr: str,\n    ) -\u003e tuple[AdvisorError, str]:\n        \"\"\"\n        Categorize CLI error based on output.\n        \n        Args:\n            returncode: Exit code from CLI\n            stdout: Standard output\n            stderr: Standard error\n            \n        Returns:\n            (error_category, human_readable_message)\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    async def check_available(self) -\u003e AvailabilityResult:\n        \"\"\"\n        Check if CLI is installed and authenticated.\n        \n        Should perform minimal check (not a full API call).\n        \n        Returns:\n            AvailabilityResult with status\n        \"\"\"\n        pass\n    \n    async def run(\n        self,\n        prompt: str,\n        timeout: float = 600.0,\n        cwd: Path | None = None,\n    ) -\u003e AdvisorResult:\n        \"\"\"\n        Run CLI with prompt and return full result.\n        \n        Handles subprocess execution, timeout, and error categorization.\n        \n        Args:\n            prompt: The prompt to send\n            timeout: Maximum execution time in seconds\n            cwd: Working directory (default: current)\n            \n        Returns:\n            AdvisorResult with response or error info\n        \"\"\"\n        cmd = self.build_command(prompt)\n        start_time = time.monotonic()\n        \n        try:\n            process = await asyncio.create_subprocess_exec(\n                *cmd,\n                cwd=str(cwd) if cwd else None,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n            )\n            \n            stdout_bytes, stderr_bytes = await asyncio.wait_for(\n                process.communicate(),\n                timeout=timeout,\n            )\n            \n            duration_ms = int((time.monotonic() - start_time) * 1000)\n            stdout = stdout_bytes.decode('utf-8', errors='replace')\n            stderr = stderr_bytes.decode('utf-8', errors='replace')\n            \n            if process.returncode == 0:\n                content = self.parse_output(stdout, stderr)\n                return AdvisorResult.success_result(\n                    provider=self.name,\n                    content=content,\n                    duration_ms=duration_ms,\n                )\n            else:\n                error, message = self.categorize_error(\n                    process.returncode, stdout, stderr\n                )\n                return AdvisorResult.failure_result(\n                    provider=self.name,\n                    error=error,\n                    error_message=message,\n                    duration_ms=duration_ms,\n                )\n                \n        except asyncio.TimeoutError:\n            duration_ms = int((time.monotonic() - start_time) * 1000)\n            return AdvisorResult.failure_result(\n                provider=self.name,\n                error=AdvisorError.TIMEOUT,\n                error_message=f'Exceeded {timeout}s timeout',\n                duration_ms=duration_ms,\n            )\n        except FileNotFoundError:\n            duration_ms = int((time.monotonic() - start_time) * 1000)\n            return AdvisorResult.failure_result(\n                provider=self.name,\n                error=AdvisorError.CLI_NOT_FOUND,\n                error_message=f'{self.name} CLI not found in PATH',\n                duration_ms=duration_ms,\n            )\n    \n    async def stream(\n        self,\n        prompt: str,\n        timeout: float = 600.0,\n        cwd: Path | None = None,\n    ) -\u003e AsyncIterator[StreamEvent]:\n        \"\"\"\n        Stream CLI output for real-time display.\n        \n        Yields StreamEvent objects as output arrives.\n        Final event will be type=DONE or type=ERROR.\n        \n        Args:\n            prompt: The prompt to send\n            timeout: Maximum execution time\n            cwd: Working directory\n            \n        Yields:\n            StreamEvent objects\n        \"\"\"\n        cmd = self.build_command(prompt)\n        start_time = time.monotonic()\n        \n        def elapsed() -\u003e float:\n            return time.monotonic() - start_time\n        \n        try:\n            process = await asyncio.create_subprocess_exec(\n                *cmd,\n                cwd=str(cwd) if cwd else None,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n            )\n            \n            # Read stdout line by line\n            async def read_stream():\n                while True:\n                    line = await asyncio.wait_for(\n                        process.stdout.readline(),\n                        timeout=timeout - elapsed(),\n                    )\n                    if not line:\n                        break\n                    yield StreamEvent.text(\n                        line.decode('utf-8', errors='replace'),\n                        elapsed(),\n                    )\n            \n            async for event in read_stream():\n                yield event\n            \n            await process.wait()\n            \n            if process.returncode == 0:\n                yield StreamEvent.done(elapsed())\n            else:\n                stderr = await process.stderr.read()\n                error, message = self.categorize_error(\n                    process.returncode, '', stderr.decode()\n                )\n                yield StreamEvent.error(message, elapsed())\n                \n        except asyncio.TimeoutError:\n            yield StreamEvent.error(\n                f'Exceeded {timeout}s timeout',\n                elapsed(),\n            )\n        except FileNotFoundError:\n            yield StreamEvent.error(\n                f'{self.name} CLI not found',\n                elapsed(),\n            )\n```\n\n## Acceptance Criteria\n- [ ] ABC with all abstract methods defined\n- [ ] `run()` method handles success, failure, timeout\n- [ ] `stream()` method yields events in real-time\n- [ ] FileNotFoundError caught and categorized\n- [ ] Timeout handled gracefully\n- [ ] All methods have comprehensive docstrings\n- [ ] Type hints complete\n\n## Testing Strategy\n```python\n# Test with mock adapter\nclass MockAdapter(ProviderAdapter):\n    name = 'mock'\n    \n    def build_command(self, prompt):\n        return ['echo', prompt]\n    \n    def parse_output(self, stdout, stderr):\n        return stdout.strip()\n    \n    def categorize_error(self, returncode, stdout, stderr):\n        return AdvisorError.UNKNOWN, 'Mock error'\n    \n    async def check_available(self):\n        return AvailabilityResult('mock', True)\n\nasync def test_mock_adapter_run():\n    adapter = MockAdapter()\n    result = await adapter.run('hello')\n    assert result.success\n    assert result.content == 'hello'\n```","notes":"## ADDITIONAL REQUIREMENT: Subprocess Tracking for Signal Handler\n\nFor graceful shutdown (Task 8.2), the signal handler needs access to running subprocess PIDs.\n\n### Base Class Additions\n\n```python\nclass ProviderAdapter(ABC):\n    \"\"\"Abstract base class with subprocess tracking.\"\"\"\n    \n    def __init__(self):\n        self._current_process: asyncio.subprocess.Process | None = None\n    \n    @property\n    def current_process(self) -\u003e asyncio.subprocess.Process | None:\n        \"\"\"Get currently running subprocess, if any.\"\"\"\n        return self._current_process\n    \n    async def kill_current(self) -\u003e None:\n        \"\"\"Kill currently running subprocess.\"\"\"\n        if self._current_process and self._current_process.returncode is None:\n            try:\n                self._current_process.terminate()\n                # Give it 2 seconds to terminate gracefully\n                await asyncio.wait_for(\n                    self._current_process.wait(),\n                    timeout=2.0\n                )\n            except asyncio.TimeoutError:\n                # Force kill if it doesn't terminate\n                self._current_process.kill()\n            except ProcessLookupError:\n                pass  # Already dead\n    \n    async def run(self, prompt: str, timeout: float = 600.0, cwd=None) -\u003e AdvisorResult:\n        \"\"\"Run with subprocess tracking.\"\"\"\n        cmd = self.build_command(prompt)\n        \n        try:\n            self._current_process = await asyncio.create_subprocess_exec(...)\n            # ... existing logic ...\n        finally:\n            self._current_process = None\n```\n\n### Signal Handler Integration (Task 8.2)\n\n```python\n# In orchestrator, collect all adapters\nasync def handle_shutdown(adapters: list[ProviderAdapter]):\n    \"\"\"Kill all running advisor subprocesses.\"\"\"\n    await asyncio.gather(\n        *[adapter.kill_current() for adapter in adapters],\n        return_exceptions=True\n    )\n```\n\n### Acceptance Criteria Addition\n- [ ] _current_process tracked during run()\n- [ ] kill_current() method terminates gracefully then forces kill\n- [ ] Process reference cleared after run completes\n- [ ] Safe to call kill_current() when no process running","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:11:59.615436-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:04:23.213488-06:00","labels":["abstract","adapter","foundation"],"dependencies":[{"issue_id":"meld-eq0.3.2","depends_on_id":"meld-eq0.3","type":"parent-child","created_at":"2026-01-15T23:11:59.616279-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.3.2","depends_on_id":"meld-eq0.3.1","type":"blocks","created_at":"2026-01-15T23:43:39.790658-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.4","title":"Task 4: Provider Adapters Implementation","description":"# Task 4: Provider Adapters Implementation\n\n## Overview\nImplement concrete adapters for Claude CLI, Gemini CLI, and Codex CLI, each encapsulating their specific command-line interfaces.\n\n## Why Three Separate Adapters?\n\nEach CLI has significant differences:\n\n### Claude CLI\n- Uses `-p` for prompt mode\n- Has `--permission-mode plan` for read-only\n- Uses `--output-format text` for plain text\n- Returns clean text output\n\n### Gemini CLI\n- Uses `-p` for prompt\n- Uses `--sandbox` for read-only mode\n- Uses `-m` for model selection\n- May have different output formatting\n\n### Codex CLI\n- Uses `exec` subcommand for execution\n- Uses `--sandbox read-only` for protection\n- Uses `--model` for model selection\n- Different output structure\n\n## Design Principles\n\n### Flags Are Hardcoded in v1\nWe hardcode the \"best\" flags for each CLI because:\n1. Simplifies user experience (no config needed)\n2. Ensures consistent behavior\n3. Easy to update when CLIs change\n\n### Error Patterns Are CLI-Specific\nEach CLI reports errors differently:\n- Different exit codes for same conditions\n- Different stderr formats\n- Different authentication error messages\n\n## Technical Requirements\n\n### Claude Adapter\n```bash\nclaude -p \"{prompt}\" \\\n  --permission-mode plan \\\n  --model opus \\\n  --output-format text\n```\n\n### Gemini Adapter\n```bash\ngemini -p \"{prompt}\" \\\n  -m gemini-2.5-pro \\\n  --sandbox\n```\n\n### Codex Adapter\n```bash\ncodex exec \"{prompt}\" \\\n  --sandbox read-only \\\n  --model gpt-5.2\n```\n\n## Error Pattern Recognition\n\n### Common Patterns to Detect\n1. **Not authenticated**: \"not logged in\", \"authentication required\", \"unauthorized\"\n2. **Rate limited**: \"rate limit\", \"quota exceeded\", \"too many requests\"\n3. **Network error**: \"connection refused\", \"network unreachable\", \"DNS\"\n4. **Timeout**: (handled by base class)\n\n### Exit Code Interpretation\n- 0: Success\n- 1: General error (parse stderr)\n- 2+: CLI-specific\n\n## Acceptance Criteria\n- [ ] Claude adapter builds correct command\n- [ ] Gemini adapter builds correct command\n- [ ] Codex adapter builds correct command\n- [ ] All adapters parse output correctly\n- [ ] All adapters categorize errors correctly\n- [ ] All adapters implement check_available()\n- [ ] Integration test with real CLIs passes (if installed)\n\n## Notes for Implementation\n- Each adapter in its own file: `claude.py`, `gemini.py`, `openai.py`\n- Expose all adapters from `providers/__init__.py`\n- Include version detection where possible\n- Log raw output in verbose mode for debugging","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:13:36.768912-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:13:36.768912-06:00","labels":["adapters","core-logic","phase-2"],"dependencies":[{"issue_id":"meld-eq0.4","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:13:36.772749-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.4","depends_on_id":"meld-eq0.3","type":"blocks","created_at":"2026-01-15T23:43:39.969722-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.4.1","title":"4.1: Implement Claude CLI adapter","description":"# Subtask 4.1: Claude CLI Adapter\n\n## What to Implement\nConcrete ProviderAdapter for the Claude Code CLI.\n\n## Location\n`meld/providers/claude.py`\n\n## CLI Specification\n\n### Command Format\n```bash\nclaude -p \"{prompt}\" \\\n  --permission-mode plan \\\n  --model opus \\\n  --output-format text\n```\n\n### Flag Reference\n| Flag | Purpose |\n|------|---------|\n| `-p \"PROMPT\"` | Non-interactive mode, returns text |\n| `--permission-mode plan` | Read-only, proposes but doesn't execute |\n| `--model opus` | Use Claude Opus (highest capability) |\n| `--output-format text` | Plain text output for parsing |\n\n### Working Directory\nCLI runs in user's cwd to have access to codebase context.\n\n## Implementation\n\n```python\nimport shutil\nfrom pathlib import Path\n\nfrom meld.providers.base import ProviderAdapter\nfrom meld.models import AdvisorError, AvailabilityResult\n\n\nclass ClaudeAdapter(ProviderAdapter):\n    \"\"\"\n    Adapter for Claude Code CLI.\n    \n    Uses Claude as both Melder (plan generation/synthesis) and\n    as one of the three advisors providing feedback.\n    \n    CLI Documentation: https://code.claude.com/docs/en/cli-usage\n    \"\"\"\n    \n    name = 'claude'\n    \n    # CLI binary name\n    CLI_BINARY = 'claude'\n    \n    # Install instructions for preflight\n    INSTALL_HINT = 'npm install -g @anthropic-ai/claude-code'\n    \n    def build_command(self, prompt: str) -\u003e list[str]:\n        \"\"\"\n        Build Claude CLI command.\n        \n        Uses:\n        - plan mode for read-only operation\n        - opus model for best reasoning\n        - text output for easy parsing\n        \"\"\"\n        return [\n            self.CLI_BINARY,\n            '-p', prompt,\n            '--permission-mode', 'plan',\n            '--model', 'opus',\n            '--output-format', 'text',\n        ]\n    \n    def parse_output(self, stdout: str, stderr: str) -\u003e str:\n        \"\"\"\n        Parse Claude CLI output.\n        \n        Claude with --output-format text returns clean text.\n        We strip any leading/trailing whitespace.\n        \"\"\"\n        # Claude outputs clean text with this format\n        return stdout.strip()\n    \n    def categorize_error(\n        self,\n        returncode: int,\n        stdout: str,\n        stderr: str,\n    ) -\u003e tuple[AdvisorError, str]:\n        \"\"\"\n        Categorize Claude CLI errors.\n        \n        Common error patterns:\n        - \"not logged in\" / \"authentication\" → AUTH_FAILED\n        - \"rate limit\" / \"quota\" → RATE_LIMITED\n        - \"network\" / \"connection\" → NETWORK_ERROR\n        \"\"\"\n        combined = (stdout + stderr).lower()\n        \n        # Authentication errors\n        if any(term in combined for term in [\n            'not logged in', 'authentication', 'unauthorized', 'not authenticated'\n        ]):\n            return (\n                AdvisorError.AUTH_FAILED,\n                'Claude CLI not authenticated. Run: claude auth login'\n            )\n        \n        # Rate limiting\n        if any(term in combined for term in [\n            'rate limit', 'quota', 'too many requests', '429'\n        ]):\n            return (\n                AdvisorError.RATE_LIMITED,\n                'Rate limit exceeded. Wait and retry.'\n            )\n        \n        # Network errors\n        if any(term in combined for term in [\n            'network', 'connection refused', 'dns', 'timeout', 'unreachable'\n        ]):\n            return (\n                AdvisorError.NETWORK_ERROR,\n                'Network error connecting to Claude API'\n            )\n        \n        # Unknown error\n        error_msg = stderr.strip() or stdout.strip() or f'Exit code {returncode}'\n        return (\n            AdvisorError.INTERNAL_ERROR,\n            f'Claude CLI error: {error_msg[:200]}'\n        )\n    \n    async def check_available(self) -\u003e AvailabilityResult:\n        \"\"\"\n        Check if Claude CLI is installed and accessible.\n        \n        Checks:\n        1. CLI binary exists in PATH\n        2. Can run --version successfully\n        \n        Note: Full auth check requires API call, done in preflight.\n        \"\"\"\n        # Check if binary exists\n        if not shutil.which(self.CLI_BINARY):\n            return AvailabilityResult(\n                provider=self.name,\n                available=False,\n                error=f'{self.CLI_BINARY} not found in PATH',\n                install_hint=self.INSTALL_HINT,\n            )\n        \n        # Try to get version\n        try:\n            result = await self.run_simple(['--version'])\n            return AvailabilityResult(\n                provider=self.name,\n                available=True,\n                version=result.strip(),\n            )\n        except Exception as e:\n            return AvailabilityResult(\n                provider=self.name,\n                available=False,\n                error=str(e),\n            )\n    \n    async def run_simple(self, args: list[str]) -\u003e str:\n        \"\"\"Run a simple command (for version checks etc).\"\"\"\n        import asyncio\n        \n        process = await asyncio.create_subprocess_exec(\n            self.CLI_BINARY, *args,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n        )\n        stdout, _ = await process.communicate()\n        return stdout.decode()\n```\n\n## Acceptance Criteria\n- [ ] build_command() returns correct flags\n- [ ] parse_output() returns clean text\n- [ ] categorize_error() detects auth failures\n- [ ] categorize_error() detects rate limits\n- [ ] categorize_error() detects network errors\n- [ ] check_available() finds CLI in PATH\n- [ ] check_available() gets version\n- [ ] Integration test passes with real CLI\n\n## Testing Strategy\n```python\ndef test_claude_build_command():\n    adapter = ClaudeAdapter()\n    cmd = adapter.build_command('Hello world')\n    assert cmd[0] == 'claude'\n    assert '-p' in cmd\n    assert '--permission-mode' in cmd\n    assert 'plan' in cmd\n\ndef test_claude_categorize_auth_error():\n    adapter = ClaudeAdapter()\n    error, msg = adapter.categorize_error(\n        1, '', 'Error: not logged in'\n    )\n    assert error == AdvisorError.AUTH_FAILED\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:14:26.236538-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:14:26.236538-06:00","labels":["adapter","claude","core-logic"],"dependencies":[{"issue_id":"meld-eq0.4.1","depends_on_id":"meld-eq0.4","type":"parent-child","created_at":"2026-01-15T23:14:26.241516-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.4.1","depends_on_id":"meld-eq0.3.2","type":"blocks","created_at":"2026-01-15T23:43:40.143444-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.4.2","title":"4.2: Implement Gemini CLI adapter","description":"# Subtask 4.2: Gemini CLI Adapter\n\n## What to Implement\nConcrete ProviderAdapter for the Gemini CLI.\n\n## Location\n`meld/providers/gemini.py`\n\n## CLI Specification\n\n### Command Format\n```bash\ngemini -p \"{prompt}\" \\\n  -m gemini-2.5-pro \\\n  --sandbox\n```\n\n### Flag Reference\n| Flag | Purpose |\n|------|---------|\n| `-p \"PROMPT\"` | Non-interactive mode |\n| `-m gemini-2.5-pro` | Use Gemini 2.5 Pro (1M context, best reasoning) |\n| `--sandbox` | Sandboxed execution, prevents file modifications |\n\n### Future Flags\nWhen available (see GitHub issue #6693):\n- `--thinking-budget 32768` for maximum reasoning\n\n### Working Directory\nCLI runs in user's cwd for codebase context access.\n\n## Implementation\n\n```python\nimport shutil\nfrom pathlib import Path\n\nfrom meld.providers.base import ProviderAdapter\nfrom meld.models import AdvisorError, AvailabilityResult\n\n\nclass GeminiAdapter(ProviderAdapter):\n    \"\"\"\n    Adapter for Google Gemini CLI.\n    \n    Uses Gemini 2.5 Pro for maximum context window (1M tokens)\n    and best reasoning capabilities.\n    \n    CLI Documentation: https://github.com/google-gemini/gemini-cli\n    Sandbox Docs: https://geminicli.com/docs/cli/sandbox/\n    \"\"\"\n    \n    name = 'gemini'\n    \n    CLI_BINARY = 'gemini'\n    INSTALL_HINT = 'npm install -g @google/gemini-cli'\n    \n    # Model to use\n    MODEL = 'gemini-2.5-pro'\n    \n    def build_command(self, prompt: str) -\u003e list[str]:\n        \"\"\"\n        Build Gemini CLI command.\n        \n        Uses:\n        - 2.5-pro model for best reasoning\n        - sandbox mode for read-only safety\n        \"\"\"\n        return [\n            self.CLI_BINARY,\n            '-p', prompt,\n            '-m', self.MODEL,\n            '--sandbox',\n        ]\n    \n    def parse_output(self, stdout: str, stderr: str) -\u003e str:\n        \"\"\"\n        Parse Gemini CLI output.\n        \n        Gemini may have some formatting in output.\n        We extract the main response text.\n        \"\"\"\n        # Gemini outputs response to stdout\n        # May have some header/footer - strip them\n        output = stdout.strip()\n        \n        # Remove any status lines at start\n        lines = output.split('\\n')\n        # Skip lines that look like status (e.g., \"Thinking...\")\n        content_lines = []\n        started = False\n        for line in lines:\n            if started:\n                content_lines.append(line)\n            elif not line.startswith('[') and line.strip():\n                started = True\n                content_lines.append(line)\n        \n        return '\\n'.join(content_lines).strip() if content_lines else output\n    \n    def categorize_error(\n        self,\n        returncode: int,\n        stdout: str,\n        stderr: str,\n    ) -\u003e tuple[AdvisorError, str]:\n        \"\"\"\n        Categorize Gemini CLI errors.\n        \n        Gemini error patterns differ from Claude.\n        \"\"\"\n        combined = (stdout + stderr).lower()\n        \n        # Authentication errors\n        if any(term in combined for term in [\n            'not authenticated', 'login required', 'unauthorized',\n            'authentication failed', 'api key'\n        ]):\n            return (\n                AdvisorError.AUTH_FAILED,\n                'Gemini CLI not authenticated. Run: gemini auth login'\n            )\n        \n        # Rate limiting\n        if any(term in combined for term in [\n            'rate limit', 'quota exceeded', 'resource exhausted', '429'\n        ]):\n            return (\n                AdvisorError.RATE_LIMITED,\n                'Gemini rate limit exceeded. Wait and retry.'\n            )\n        \n        # Network errors\n        if any(term in combined for term in [\n            'network', 'connection', 'dns', 'timeout', 'unreachable',\n            'failed to connect'\n        ]):\n            return (\n                AdvisorError.NETWORK_ERROR,\n                'Network error connecting to Gemini API'\n            )\n        \n        # Model not available\n        if 'model' in combined and ('not found' in combined or 'unavailable' in combined):\n            return (\n                AdvisorError.INTERNAL_ERROR,\n                f'Model {self.MODEL} not available. Check Gemini CLI version.'\n            )\n        \n        # Unknown error\n        error_msg = stderr.strip() or stdout.strip() or f'Exit code {returncode}'\n        return (\n            AdvisorError.INTERNAL_ERROR,\n            f'Gemini CLI error: {error_msg[:200]}'\n        )\n    \n    async def check_available(self) -\u003e AvailabilityResult:\n        \"\"\"Check if Gemini CLI is installed and accessible.\"\"\"\n        if not shutil.which(self.CLI_BINARY):\n            return AvailabilityResult(\n                provider=self.name,\n                available=False,\n                error=f'{self.CLI_BINARY} not found in PATH',\n                install_hint=self.INSTALL_HINT,\n            )\n        \n        try:\n            result = await self.run_simple(['--version'])\n            return AvailabilityResult(\n                provider=self.name,\n                available=True,\n                version=result.strip(),\n            )\n        except Exception as e:\n            return AvailabilityResult(\n                provider=self.name,\n                available=False,\n                error=str(e),\n            )\n    \n    async def run_simple(self, args: list[str]) -\u003e str:\n        \"\"\"Run a simple command.\"\"\"\n        import asyncio\n        process = await asyncio.create_subprocess_exec(\n            self.CLI_BINARY, *args,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n        )\n        stdout, _ = await process.communicate()\n        return stdout.decode()\n```\n\n## Acceptance Criteria\n- [ ] build_command() includes correct model flag\n- [ ] build_command() includes --sandbox\n- [ ] parse_output() handles status lines\n- [ ] categorize_error() detects Gemini-specific patterns\n- [ ] check_available() works correctly\n- [ ] Integration test passes with real CLI\n\n## Notes\n- Gemini CLI output format may vary between versions\n- Keep parse_output() flexible to handle variations\n- Monitor GitHub for thinkingBudget flag availability","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:15:14.15148-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:15:14.15148-06:00","labels":["adapter","core-logic","gemini"],"dependencies":[{"issue_id":"meld-eq0.4.2","depends_on_id":"meld-eq0.4","type":"parent-child","created_at":"2026-01-15T23:15:14.155699-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.4.2","depends_on_id":"meld-eq0.3.2","type":"blocks","created_at":"2026-01-15T23:43:40.313013-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.4.3","title":"4.3: Implement Codex/OpenAI CLI adapter","description":"# Subtask 4.3: Codex/OpenAI CLI Adapter\n\n## What to Implement\nConcrete ProviderAdapter for the OpenAI Codex CLI.\n\n## Location\n`meld/providers/openai.py`\n\nNote: File is named `openai.py` since Codex is OpenAI's CLI tool.\n\n## CLI Specification\n\n### Command Format\n```bash\ncodex exec \"{prompt}\" \\\n  --sandbox read-only \\\n  --model gpt-5.2\n```\n\n### Flag Reference\n| Flag | Purpose |\n|------|---------|\n| `exec \"PROMPT\"` | Non-interactive execution mode |\n| `--sandbox read-only` | Prevents file modifications |\n| `--model gpt-5.2` | Use GPT-5.2 model |\n\n### Reasoning Configuration\nMaximum reasoning requires config file setting:\n```toml\n# ~/.codex/config.toml\nmodel_reasoning_effort = \"xhigh\"\n```\n\nThis can't be passed via CLI flag currently.\n\n### Working Directory\nCLI runs in user's cwd for codebase context.\n\n## Implementation\n\n```python\nimport shutil\nfrom pathlib import Path\n\nfrom meld.providers.base import ProviderAdapter\nfrom meld.models import AdvisorError, AvailabilityResult\n\n\nclass CodexAdapter(ProviderAdapter):\n    \"\"\"\n    Adapter for OpenAI Codex CLI.\n    \n    Uses Codex in read-only sandbox mode with GPT-5.2.\n    \n    CLI Reference: https://developers.openai.com/codex/cli/reference/\n    Config Reference: https://developers.openai.com/codex/config-reference/\n    \n    Note: For maximum reasoning, user should set model_reasoning_effort\n    in ~/.codex/config.toml as this can't be passed via CLI flag.\n    \"\"\"\n    \n    name = 'codex'\n    \n    CLI_BINARY = 'codex'\n    INSTALL_HINT = 'npm install -g @openai/codex'\n    \n    # Model to use\n    MODEL = 'gpt-5.2'\n    \n    def build_command(self, prompt: str) -\u003e list[str]:\n        \"\"\"\n        Build Codex CLI command.\n        \n        Uses exec subcommand for non-interactive mode\n        with read-only sandbox for safety.\n        \"\"\"\n        return [\n            self.CLI_BINARY,\n            'exec', prompt,\n            '--sandbox', 'read-only',\n            '--model', self.MODEL,\n        ]\n    \n    def parse_output(self, stdout: str, stderr: str) -\u003e str:\n        \"\"\"\n        Parse Codex CLI output.\n        \n        Codex exec mode outputs response text.\n        May include some formatting to strip.\n        \"\"\"\n        output = stdout.strip()\n        \n        # Codex may wrap output in markers\n        # Handle known patterns\n        lines = output.split('\\n')\n        \n        # Skip any header/footer lines from the CLI\n        content_lines = []\n        for line in lines:\n            # Skip known status/wrapper lines\n            if line.startswith('---') or line.startswith('==='):\n                continue\n            if line.lower().startswith('codex:'):\n                continue\n            content_lines.append(line)\n        \n        return '\\n'.join(content_lines).strip() if content_lines else output\n    \n    def categorize_error(\n        self,\n        returncode: int,\n        stdout: str,\n        stderr: str,\n    ) -\u003e tuple[AdvisorError, str]:\n        \"\"\"\n        Categorize Codex CLI errors.\n        \n        OpenAI error patterns.\n        \"\"\"\n        combined = (stdout + stderr).lower()\n        \n        # Authentication errors\n        if any(term in combined for term in [\n            'not authenticated', 'api key', 'unauthorized',\n            'authentication failed', 'invalid api key', 'no api key'\n        ]):\n            return (\n                AdvisorError.AUTH_FAILED,\n                'Codex CLI not authenticated. Run: codex auth login'\n            )\n        \n        # Rate limiting\n        if any(term in combined for term in [\n            'rate limit', 'quota', 'too many requests', '429',\n            'capacity', 'overloaded'\n        ]):\n            return (\n                AdvisorError.RATE_LIMITED,\n                'OpenAI rate limit exceeded. Wait and retry.'\n            )\n        \n        # Network errors\n        if any(term in combined for term in [\n            'network', 'connection', 'dns', 'timeout', 'unreachable',\n            'failed to connect', 'econnrefused'\n        ]):\n            return (\n                AdvisorError.NETWORK_ERROR,\n                'Network error connecting to OpenAI API'\n            )\n        \n        # Model errors\n        if 'model' in combined and any(term in combined for term in [\n            'not found', 'unavailable', 'does not exist', 'invalid model'\n        ]):\n            return (\n                AdvisorError.INTERNAL_ERROR,\n                f'Model {self.MODEL} not available. Check Codex CLI version.'\n            )\n        \n        # Sandbox violations (shouldn't happen but handle)\n        if 'sandbox' in combined and 'violation' in combined:\n            return (\n                AdvisorError.INTERNAL_ERROR,\n                'Sandbox violation - CLI attempted blocked operation'\n            )\n        \n        # Unknown error\n        error_msg = stderr.strip() or stdout.strip() or f'Exit code {returncode}'\n        return (\n            AdvisorError.INTERNAL_ERROR,\n            f'Codex CLI error: {error_msg[:200]}'\n        )\n    \n    async def check_available(self) -\u003e AvailabilityResult:\n        \"\"\"Check if Codex CLI is installed and accessible.\"\"\"\n        if not shutil.which(self.CLI_BINARY):\n            return AvailabilityResult(\n                provider=self.name,\n                available=False,\n                error=f'{self.CLI_BINARY} not found in PATH',\n                install_hint=self.INSTALL_HINT,\n            )\n        \n        try:\n            result = await self.run_simple(['--version'])\n            return AvailabilityResult(\n                provider=self.name,\n                available=True,\n                version=result.strip(),\n            )\n        except Exception as e:\n            return AvailabilityResult(\n                provider=self.name,\n                available=False,\n                error=str(e),\n            )\n    \n    async def run_simple(self, args: list[str]) -\u003e str:\n        \"\"\"Run a simple command.\"\"\"\n        import asyncio\n        process = await asyncio.create_subprocess_exec(\n            self.CLI_BINARY, *args,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n        )\n        stdout, _ = await process.communicate()\n        return stdout.decode()\n```\n\n## Acceptance Criteria\n- [ ] build_command() uses 'exec' subcommand\n- [ ] build_command() includes --sandbox read-only\n- [ ] build_command() includes --model flag\n- [ ] parse_output() handles wrapper text\n- [ ] categorize_error() detects OpenAI patterns\n- [ ] check_available() works correctly\n- [ ] Integration test passes with real CLI\n\n## Notes on Reasoning Configuration\nDocument for users that maximum reasoning requires:\n```toml\n# ~/.codex/config.toml\nmodel_reasoning_effort = \"xhigh\"\n```\n\nThis should be mentioned in `meld doctor` output.","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:16:09.93073-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:16:09.93073-06:00","labels":["adapter","codex","core-logic","openai"],"dependencies":[{"issue_id":"meld-eq0.4.3","depends_on_id":"meld-eq0.4","type":"parent-child","created_at":"2026-01-15T23:16:09.935146-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.4.3","depends_on_id":"meld-eq0.3.2","type":"blocks","created_at":"2026-01-15T23:43:40.58423-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.5","title":"Task 5: Preflight Validation \u0026 Doctor Command","description":"# Task 5: Preflight Validation \u0026 Doctor Command\n\n## Overview\nImplement preflight checks that run before every meld session and a dedicated `meld doctor` command for comprehensive diagnostics.\n\n## Why Preflight Matters\nWithout preflight:\n1. Session starts and fails mid-way when a CLI is missing\n2. User wastes time waiting for other advisors before error\n3. Error messages are cryptic subprocess failures\n\nWith preflight:\n1. All problems detected immediately\n2. Clear, actionable error messages\n3. No wasted work\n\n## Two Modes of Preflight\n\n### Inline Preflight (before each `meld run`)\n- Quick check: CLIs exist in PATH\n- Skip with `--skip-preflight` if user knows what they're doing\n- Errors immediately terminate with clear message\n\n### Doctor Command (`meld doctor`)\n- Comprehensive check: CLIs exist + version + auth status\n- Shows all issues, not just first\n- Suggests fixes for each problem\n- Non-blocking (always runs to completion)\n\n## What to Check\n\n### CLI Existence\n```bash\nwhich claude gemini codex\n```\n\n### Version Detection\n```bash\nclaude --version\ngemini --version\ncodex --version\n```\n\n### Authentication (Doctor only)\nFor each CLI, attempt minimal API call to verify auth.\nThis is slower, so only in doctor mode.\n\n## Error Messages\n\n### Missing CLI\n```\n❌ CLI not found: gemini\n   Install with: npm install -g @google/gemini-cli\n```\n\n### Not Authenticated\n```\n❌ claude not authenticated\n   Run: claude auth login\n```\n\n### Version Too Old\n```\n⚠️  gemini version 0.5.0 may be outdated\n   Latest: 0.7.0\n   Update: npm update -g @google/gemini-cli\n```\n\n## Graceful Degradation Decision\nMeld can work with 2 out of 3 advisors, so:\n- Missing 1 CLI: Warn but continue\n- Missing 2+ CLIs: Error and stop\n\n## Technical Requirements\n\n### PreflightResult\n```python\n@dataclass\nclass PreflightResult:\n    passed: bool\n    errors: list[str]\n    warnings: list[str]\n    available_advisors: list[str]\n```\n\n### Preflight Function\n```python\nasync def run_preflight(\n    skip: bool = False,\n) -\u003e PreflightResult:\n    \"\"\"\n    Run preflight checks.\n    \n    Args:\n        skip: If True, skip all checks\n        \n    Returns:\n        PreflightResult with status and issues\n    \"\"\"\n```\n\n### Doctor Function\n```python\nasync def run_doctor() -\u003e None:\n    \"\"\"\n    Run comprehensive diagnostics and print results.\n    \n    Always runs to completion, showing all issues.\n    \"\"\"\n```\n\n## Acceptance Criteria\n- [ ] Preflight detects missing CLIs\n- [ ] Preflight shows install instructions\n- [ ] --skip-preflight bypasses checks\n- [ ] Doctor shows version for each CLI\n- [ ] Doctor shows auth status\n- [ ] Doctor suggests fixes for issues\n- [ ] 1 missing CLI: warn + continue\n- [ ] 2+ missing CLIs: error + stop\n- [ ] Exit codes are correct (0 pass, 2 fail)\n\n## Doctor Output Example\n```\nMeld Doctor - System Check\n==========================\n\nclaude\n  ✓ Installed: /usr/local/bin/claude\n  ✓ Version: 1.5.0\n  ✓ Authenticated\n\ngemini\n  ✓ Installed: /usr/local/bin/gemini\n  ✓ Version: 0.7.0\n  ⚠ Authentication: unknown (unable to verify)\n\ncodex\n  ✗ Not installed\n  → Install: npm install -g @openai/codex\n\nSummary: 2/3 advisors available\nMeld can run with reduced advisor pool.\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:16:44.797621-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:16:44.797621-06:00","labels":["core-logic","doctor","phase-2","preflight"],"dependencies":[{"issue_id":"meld-eq0.5","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:16:44.807783-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.5","depends_on_id":"meld-eq0.4","type":"blocks","created_at":"2026-01-15T23:44:07.328545-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.5.1","title":"5.1: Implement quick preflight checks","description":"# Subtask 5.1: Quick Preflight Checks\n\n## What to Implement\nFast CLI existence checks that run before each meld session.\n\n## Location\n`meld/preflight.py`\n\n## Implementation\n\n```python\nimport shutil\nfrom dataclasses import dataclass, field\n\n# Install instructions for each CLI\nCLI_INSTALL = {\n    'claude': 'npm install -g @anthropic-ai/claude-code',\n    'gemini': 'npm install -g @google/gemini-cli',\n    'codex': 'npm install -g @openai/codex',\n}\n\n# Minimum required advisors for meld to run\nMIN_ADVISORS = 2\n\n\n@dataclass\nclass PreflightResult:\n    \"\"\"Result of preflight checks.\n    \n    Attributes:\n        passed: Whether meld can proceed\n        errors: Critical issues that block execution\n        warnings: Non-critical issues to inform user\n        available_advisors: List of available CLI names\n    \"\"\"\n    passed: bool\n    errors: list[str] = field(default_factory=list)\n    warnings: list[str] = field(default_factory=list)\n    available_advisors: list[str] = field(default_factory=list)\n\n\ndef check_cli_exists(cli_name: str) -\u003e tuple[bool, str | None]:\n    \"\"\"\n    Check if CLI exists in PATH.\n    \n    Args:\n        cli_name: Name of CLI binary\n        \n    Returns:\n        (exists, path_or_error)\n    \"\"\"\n    path = shutil.which(cli_name)\n    if path:\n        return True, path\n    return False, CLI_INSTALL.get(cli_name, f'Install {cli_name}')\n\n\ndef run_preflight_sync() -\u003e PreflightResult:\n    \"\"\"\n    Run synchronous preflight checks.\n    \n    Checks only CLI existence (fast).\n    For auth checks, use run_doctor().\n    \n    Returns:\n        PreflightResult with status and available advisors\n    \"\"\"\n    errors = []\n    warnings = []\n    available = []\n    \n    for cli in ['claude', 'gemini', 'codex']:\n        exists, info = check_cli_exists(cli)\n        if exists:\n            available.append(cli)\n        else:\n            # Error message with install hint\n            errors.append(f\"CLI not found: {cli}\\n   Install with: {info}\")\n    \n    # Determine if we can proceed\n    if len(available) \u003e= MIN_ADVISORS:\n        passed = True\n        # Convert some errors to warnings if we can continue\n        if len(available) \u003c 3:\n            missing = set(['claude', 'gemini', 'codex']) - set(available)\n            warnings = [e for e in errors if any(m in e for m in missing)]\n            errors = []\n    else:\n        passed = False\n    \n    return PreflightResult(\n        passed=passed,\n        errors=errors,\n        warnings=warnings,\n        available_advisors=available,\n    )\n\n\ndef format_preflight_result(result: PreflightResult) -\u003e str:\n    \"\"\"Format preflight result for display.\"\"\"\n    lines = []\n    \n    for error in result.errors:\n        lines.append(f\"❌ {error}\")\n    \n    for warning in result.warnings:\n        lines.append(f\"⚠️  {warning}\")\n    \n    if result.passed:\n        if len(result.available_advisors) == 3:\n            lines.append(f\"✓ All advisors available: {', '.join(result.available_advisors)}\")\n        else:\n            lines.append(f\"⚠️  Proceeding with {len(result.available_advisors)}/3 advisors: {', '.join(result.available_advisors)}\")\n    else:\n        lines.append(f\"\\n❌ Cannot proceed: need at least {MIN_ADVISORS} advisors\")\n    \n    return '\\n'.join(lines)\n```\n\n## Integration with CLI\n\n```python\n# In cli.py run_command()\ndef run_command(args) -\u003e int:\n    if not args.skip_preflight:\n        result = run_preflight_sync()\n        \n        if result.warnings:\n            print(format_preflight_result(result), file=sys.stderr)\n        \n        if not result.passed:\n            print(format_preflight_result(result), file=sys.stderr)\n            return 2  # Exit code for preflight failure\n    \n    # Continue with meld session...\n```\n\n## Acceptance Criteria\n- [ ] Detects missing CLIs\n- [ ] Shows install instructions\n- [ ] Passes with 2/3 advisors (with warning)\n- [ ] Fails with 1/3 or 0/3 advisors\n- [ ] Returns exit code 2 on failure\n- [ ] --skip-preflight bypasses checks\n- [ ] Output is clear and actionable","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:17:17.18966-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:17:17.18966-06:00","labels":["core-logic","preflight"],"dependencies":[{"issue_id":"meld-eq0.5.1","depends_on_id":"meld-eq0.5","type":"parent-child","created_at":"2026-01-15T23:17:17.192932-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.5.2","title":"5.2: Implement meld doctor command","description":"# Subtask 5.2: Doctor Command\n\n## What to Implement\nComprehensive diagnostic command that checks all CLIs and their status.\n\n## Location\n`meld/preflight.py` (alongside preflight) or `meld/doctor.py`\n\n## Implementation\n\n```python\nimport asyncio\nfrom meld.providers import ClaudeAdapter, GeminiAdapter, CodexAdapter\n\n\nasync def run_doctor() -\u003e int:\n    \"\"\"\n    Run comprehensive diagnostics.\n    \n    Checks:\n    1. CLI installation status\n    2. CLI version\n    3. Authentication status (if possible)\n    \n    Always runs to completion, showing all issues.\n    \n    Returns:\n        Exit code (0 = all good, 1 = issues found)\n    \"\"\"\n    print(\"Meld Doctor - System Check\")\n    print(\"==\" * 13)\n    print()\n    \n    adapters = [\n        ClaudeAdapter(),\n        GeminiAdapter(),\n        CodexAdapter(),\n    ]\n    \n    issues_found = False\n    available_count = 0\n    \n    for adapter in adapters:\n        print(f\"{adapter.name}\")\n        \n        result = await adapter.check_available()\n        \n        if not result.available:\n            print(f\"  ✗ Not installed\")\n            print(f\"  → Install: {result.install_hint}\")\n            issues_found = True\n        else:\n            available_count += 1\n            path = shutil.which(adapter.CLI_BINARY)\n            print(f\"  ✓ Installed: {path}\")\n            \n            if result.version:\n                print(f\"  ✓ Version: {result.version}\")\n            \n            if result.authenticated is True:\n                print(f\"  ✓ Authenticated\")\n            elif result.authenticated is False:\n                print(f\"  ✗ Not authenticated\")\n                print(f\"  → Run: {adapter.CLI_BINARY} auth login\")\n                issues_found = True\n            else:\n                print(f\"  ⚠ Authentication: unable to verify\")\n        \n        print()\n    \n    # Summary\n    print(\"Summary\")\n    print(\"-\" * 40)\n    \n    if available_count == 3:\n        print(\"✓ All 3 advisors available\")\n        print(\"Meld is ready to use.\")\n    elif available_count \u003e= 2:\n        print(f\"⚠ {available_count}/3 advisors available\")\n        print(\"Meld can run with reduced advisor pool.\")\n    else:\n        print(f\"✗ Only {available_count}/3 advisors available\")\n        print(\"Meld requires at least 2 advisors.\")\n        issues_found = True\n    \n    # Reasoning config reminder\n    print()\n    print(\"Tip: For maximum reasoning with Codex, add to ~/.codex/config.toml:\")\n    print('  model_reasoning_effort = \"xhigh\"')\n    \n    return 1 if issues_found else 0\n\n\ndef doctor_command() -\u003e int:\n    \"\"\"CLI entry point for doctor command.\"\"\"\n    return asyncio.run(run_doctor())\n```\n\n## Enhanced Checks\n\n### Version Comparison (Optional for v1.1)\n```python\n# Could check against known minimum versions\nMIN_VERSIONS = {\n    'claude': '1.0.0',\n    'gemini': '0.5.0',\n    'codex': '1.0.0',\n}\n```\n\n### Auth Check Implementation\n```python\nasync def check_auth(adapter: ProviderAdapter) -\u003e bool | None:\n    \"\"\"\n    Check if CLI is authenticated.\n    \n    Strategy: Run minimal prompt and check for auth errors.\n    Returns None if unable to determine.\n    \"\"\"\n    try:\n        # Minimal prompt to check auth\n        result = await asyncio.wait_for(\n            adapter.run('Say \"ok\" and nothing else.', timeout=30),\n            timeout=35,\n        )\n        \n        if result.success:\n            return True\n        elif result.error == AdvisorError.AUTH_FAILED:\n            return False\n        else:\n            return None  # Other error, can't determine\n            \n    except Exception:\n        return None\n```\n\n## CLI Integration\n\n```python\n# In cli.py\ndef main() -\u003e int:\n    parser = create_parser()\n    args = parser.parse_args()\n    \n    if args.command == 'doctor':\n        return doctor_command()\n    # ...\n```\n\n## Acceptance Criteria\n- [ ] Shows status for all 3 CLIs\n- [ ] Shows version when available\n- [ ] Shows auth status when checkable\n- [ ] Shows install commands for missing\n- [ ] Summary indicates overall status\n- [ ] Exit code 0 when all good\n- [ ] Exit code 1 when issues found\n- [ ] Runs to completion (doesn't stop on first error)\n\n## Output Example\n```\nMeld Doctor - System Check\n==========================\n\nclaude\n  ✓ Installed: /usr/local/bin/claude\n  ✓ Version: 1.5.0\n  ✓ Authenticated\n\ngemini\n  ✓ Installed: /usr/local/bin/gemini\n  ✓ Version: 0.7.0\n  ⚠ Authentication: unable to verify\n\ncodex\n  ✗ Not installed\n  → Install: npm install -g @openai/codex\n\nSummary\n----------------------------------------\n⚠ 2/3 advisors available\nMeld can run with reduced advisor pool.\n\nTip: For maximum reasoning with Codex, add to ~/.codex/config.toml:\n  model_reasoning_effort = \"xhigh\"\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:17:55.280717-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:17:55.280717-06:00","labels":["cli","core-logic","doctor"],"dependencies":[{"issue_id":"meld-eq0.5.2","depends_on_id":"meld-eq0.5","type":"parent-child","created_at":"2026-01-15T23:17:55.284513-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.5.2","depends_on_id":"meld-eq0.5.1","type":"blocks","created_at":"2026-01-15T23:44:07.565269-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.6","title":"Task 6: Melder Implementation","description":"# Task 6: Melder Implementation\n\n## Overview\nThe Melder is the core intelligence of the system - it generates initial plans, synthesizes advisor feedback, and determines when convergence is reached.\n\n## Role of the Melder\nThe Melder uses Claude as its underlying model and has two operating modes:\n\n### Mode 1: Initial Plan Generation\nGiven a task (and optional PRD), generate a structured implementation plan.\n\n### Mode 2: Feedback Synthesis\nGiven the current plan and feedback from all advisors, synthesize an updated plan with:\n- Decision log (what was accepted/rejected/deferred)\n- Convergence assessment\n\n## Why Claude for Melder?\n- Best at synthesis and judgment\n- Consistent with advisor Claude for coherent conversation\n- Strong structured output capabilities\n\n## Design Decisions\n\n### Streaming Output\nMelder streams output to TUI for real-time visibility.\nUsers see the plan being generated, not just final result.\n\n### Structured Convergence Block\nMelder ends synthesis with structured JSON for machine parsing:\n```json\n{\"status\": \"CONVERGED\", \"changes_made\": 3, \"open_items\": 0, \"rationale\": \"...\"}\n```\n\n### Decision Log\nHuman-readable log of how feedback was handled:\n```\nACCEPTED:\n- [Claude] Use JWT instead of sessions\n\nREJECTED:\n- [Gemini] Remove all validation (security risk)\n\nDEFERRED:\n- Database choice needs human input\n```\n\n## Technical Requirements\n\n### Melder Class\n```python\nclass Melder:\n    def __init__(self, adapter: ClaudeAdapter, session: SessionManager):\n        self.adapter = adapter\n        self.session = session\n    \n    async def generate_initial_plan(\n        self,\n        task: str,\n        prd: str | None,\n    ) -\u003e AsyncIterator[StreamEvent]:\n        \"\"\"Generate initial plan with streaming.\"\"\"\n    \n    async def synthesize_feedback(\n        self,\n        task: str,\n        prd: str | None,\n        current_plan: str,\n        feedback: dict[str, str],  # advisor -\u003e feedback\n        round_num: int,\n    ) -\u003e AsyncIterator[StreamEvent]:\n        \"\"\"Synthesize feedback into updated plan.\"\"\"\n```\n\n### MelderResult\n```python\n@dataclass\nclass MelderResult:\n    plan: str\n    decision_log: str\n    convergence: ConvergenceAssessment\n    duration_ms: int\n```\n\n### ConvergenceAssessment\n```python\n@dataclass\nclass ConvergenceAssessment:\n    status: str  # 'CONVERGED' or 'CONTINUING'\n    changes_made: int\n    open_items: int\n    deferred_items: list[str]\n    rationale: str\n```\n\n## Prompts\n\n### Initial Plan Prompt\nSee `meld/prompts.py:MELDER_INITIAL_PLAN`\n\n### Synthesis Prompt\nSee `meld/prompts.py:MELDER_SYNTHESIZE`\n\n## Convergence Detection\n\n### Primary: Parse JSON Block\n```python\ndef parse_convergence(response: str) -\u003e ConvergenceAssessment:\n    # Find JSON block at end of response\n    match = re.search(r'```json\\s*(\\{[^\\x60]+\\})\\s*```', response)\n    if match:\n        data = json.loads(match.group(1))\n        return ConvergenceAssessment(**data)\n```\n\n### Secondary: Validate with Diff\n```python\ndef validate_convergence(\n    assessment: ConvergenceAssessment,\n    prev_plan: str,\n    curr_plan: str,\n) -\u003e tuple[bool, str]:\n    # Trust OPEN_ITEMS \u003e 0 as non-convergence\n    if assessment.open_items \u003e 0:\n        return False, f\"{assessment.open_items} open items remain\"\n    \n    # Validate CONVERGED claim with diff\n    diff_ratio = compute_diff_ratio(prev_plan, curr_plan)\n    if assessment.status == 'CONVERGED' and diff_ratio \u003e 0.05:\n        return False, \"Claimed converged but significant changes detected\"\n    \n    return assessment.status == 'CONVERGED', assessment.rationale\n```\n\n## Acceptance Criteria\n- [ ] Initial plan generation works\n- [ ] Synthesis incorporates feedback\n- [ ] Decision log is generated\n- [ ] Convergence JSON is parseable\n- [ ] Streaming works correctly\n- [ ] Results saved to session\n- [ ] Diff validation prevents false convergence\n\n## Notes\n- Never converge on round 1\n- OPEN_ITEMS \u003e 0 always blocks convergence\n- Deferred items count as open items","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:18:52.776561-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:18:52.776561-06:00","labels":["convergence","core-logic","melder","phase-2"],"dependencies":[{"issue_id":"meld-eq0.6","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:18:52.779608-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.6","depends_on_id":"meld-eq0.3","type":"blocks","created_at":"2026-01-15T23:44:07.799875-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.6","depends_on_id":"meld-eq0.2","type":"blocks","created_at":"2026-01-15T23:44:08.018125-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.6.1","title":"6.1: Create prompt templates for Melder","description":"# Subtask 6.1: Melder Prompt Templates\n\n## What to Implement\nExternalized prompt templates for Melder operations.\n\n## Location\n`meld/prompts.py`\n\n## Initial Plan Prompt\n\n```python\nMELDER_INITIAL_PLAN = '''\nYou are designing an implementation plan for a software task.\n\n## Task Description\n\n{task}\n\n{prd_section}\n\n---\n\nCreate a comprehensive implementation plan. Structure your response as follows:\n\n## Overview\nBrief summary of what will be built (2-3 sentences)\n\n## Architecture Decisions\nKey technical choices and their rationale\n\n## Implementation Steps\nOrdered list of implementation tasks:\n1. Step name\n   - Details and considerations\n   - Files to create/modify\n   \n2. Next step...\n\n## Technical Requirements\n- Specific requirements and constraints\n- Dependencies needed\n- Configuration needed\n\n## Edge Cases \u0026 Error Handling\n- Edge cases to handle\n- Error scenarios\n\n## Testing Strategy\n- What to test\n- Test approach\n\n## Open Questions\nList any ambiguities or decisions that need human input.\nMark these clearly as they affect convergence.\n\n---\n\nFocus on clarity and actionability. Each step should be concrete enough\nfor a developer to start working immediately.\n'''\n\n# PRD section template (inserted if PRD provided)\nPRD_SECTION = '''\n## Product Requirements Document\n\n{prd_content}\n'''\n```\n\n## Synthesis Prompt\n\n```python\nMELDER_SYNTHESIZE = '''\nYou are refining an implementation plan based on multi-model feedback.\n\n## Original Task\n\n{task}\n\n{prd_section}\n\n## Current Plan (Round {round_num})\n\n{current_plan}\n\n## Advisor Feedback\n\n### Claude Advisor\n{claude_feedback}\n\n### Gemini Advisor\n{gemini_feedback}\n\n### Codex Advisor\n{codex_feedback}\n\n---\n\nReview all feedback carefully and update the plan.\n\n1. Consider each suggestion on its merits\n2. Accept improvements that enhance the plan\n3. Reject suggestions that conflict with requirements or introduce problems\n4. Defer decisions that need human input\n\nAfter updating the plan, end your response with two sections:\n\n## Decision Log\n\nACCEPTED:\n- [Advisor] Brief description of incorporated feedback (1 line each)\n\nREJECTED:\n- [Advisor] Brief description — reason for rejection (1 line each)\n\nDEFERRED / NEEDS HUMAN DECISION:\n- Brief description + what input is needed (1 line each)\n\n## Convergence Assessment\n\nSTATUS: [CONVERGED | CONTINUING]\nCHANGES_MADE: [count of substantive changes incorporated]\nOPEN_ITEMS: [count of unresolved concerns, including DEFERRED items]\nRATIONALE: [one sentence explanation]\n\nThen output a JSON block for machine parsing:\n\n```json\n{{\n  \"status\": \"CONVERGED\" | \"CONTINUING\",\n  \"changes_made\": \u003cnumber\u003e,\n  \"open_items\": \u003cnumber\u003e,\n  \"deferred_items\": [\"item1\", \"item2\"],\n  \"rationale\": \"one sentence\"\n}}\n```\n\nChoose CONVERGED only if:\n- No substantive changes to plan structure or approach\n- All advisor concerns addressed or reasonably dismissed\n- DEFERRED list is empty (no items needing human decision)\n- Plan is ready for implementation\n\nChoose CONTINUING if:\n- You made meaningful changes\n- Unresolved concerns remain\n- Any DEFERRED items exist\n- Any uncertainty about the plan\n'''\n```\n\n## Advisor Feedback Prompt\n\n```python\nADVISOR_FEEDBACK = '''\nYou are reviewing an implementation plan for a software task.\n\n## Original Task\n\n{task}\n\n{prd_section}\n\n## Current Plan (Round {round_num})\n\n{current_plan}\n\n---\n\nReview this plan honestly. Your job is to help improve it.\n\nReturn feedback using the following headings (keep it concise):\n\n## Summary (1-3 bullets)\n- Your overall assessment\n\n## Must-Fix Risks (0-5 bullets)\n- [Severity: Low/Med/High] Issue description + suggested fix\n\n## Improvements (0-10 bullets)\n- Suggestions that would make the plan better\n\n## Missing Requirements / Edge Cases (0-10 bullets)\n- PRD requirements not addressed, or edge cases not considered\n\n## Questions / Assumptions to Validate (0-10 bullets)\n- Clarifications needed before implementation\n\nIf the plan is solid and you have no substantive feedback, say so\nbriefly under \"Summary\" and leave the other sections empty.\n'''\n```\n\n## Helper Functions\n\n```python\ndef format_initial_plan_prompt(task: str, prd: str | None) -\u003e str:\n    \"\"\"Format the initial plan prompt.\"\"\"\n    prd_section = PRD_SECTION.format(prd_content=prd) if prd else ''\n    return MELDER_INITIAL_PLAN.format(task=task, prd_section=prd_section)\n\n\ndef format_synthesis_prompt(\n    task: str,\n    prd: str | None,\n    current_plan: str,\n    feedback: dict[str, str],\n    round_num: int,\n) -\u003e str:\n    \"\"\"Format the synthesis prompt.\"\"\"\n    prd_section = PRD_SECTION.format(prd_content=prd) if prd else ''\n    return MELDER_SYNTHESIZE.format(\n        task=task,\n        prd_section=prd_section,\n        current_plan=current_plan,\n        round_num=round_num,\n        claude_feedback=feedback.get('claude', 'No feedback received'),\n        gemini_feedback=feedback.get('gemini', 'No feedback received'),\n        codex_feedback=feedback.get('codex', 'No feedback received'),\n    )\n\n\ndef format_advisor_prompt(\n    task: str,\n    prd: str | None,\n    current_plan: str,\n    round_num: int,\n) -\u003e str:\n    \"\"\"Format the advisor feedback prompt.\"\"\"\n    prd_section = PRD_SECTION.format(prd_content=prd) if prd else ''\n    return ADVISOR_FEEDBACK.format(\n        task=task,\n        prd_section=prd_section,\n        current_plan=current_plan,\n        round_num=round_num,\n    )\n```\n\n## Acceptance Criteria\n- [ ] All prompts externalized in prompts.py\n- [ ] Helper functions for formatting work\n- [ ] JSON block format documented\n- [ ] Decision log format documented\n- [ ] PRD section conditional\n- [ ] No hardcoded prompts elsewhere","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:19:58.824935-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:19:58.824935-06:00","labels":["core-logic","melder","prompts"],"dependencies":[{"issue_id":"meld-eq0.6.1","depends_on_id":"meld-eq0.6","type":"parent-child","created_at":"2026-01-15T23:19:58.825817-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.6.2","title":"6.2: Implement Melder class with streaming","description":"# Subtask 6.2: Melder Class Implementation\n\n## What to Implement\nThe Melder class that handles plan generation and synthesis with streaming support.\n\n## Location\n`meld/melder.py`\n\n## Implementation\n\n```python\nfrom typing import AsyncIterator\nfrom pathlib import Path\nimport time\n\nfrom meld.providers.claude import ClaudeAdapter\nfrom meld.session import SessionManager\nfrom meld.models import StreamEvent, StreamEventType\nfrom meld.prompts import format_initial_plan_prompt, format_synthesis_prompt\n\n\nclass Melder:\n    \"\"\"\n    The Melder generates and refines plans using Claude.\n    \n    Two operating modes:\n    1. Initial plan generation from task/PRD\n    2. Synthesis of advisor feedback into refined plan\n    \n    All operations stream output for real-time TUI display.\n    \"\"\"\n    \n    def __init__(\n        self,\n        adapter: ClaudeAdapter,\n        session: SessionManager,\n    ):\n        \"\"\"\n        Initialize Melder.\n        \n        Args:\n            adapter: Claude adapter for API calls\n            session: Session manager for artifact storage\n        \"\"\"\n        self.adapter = adapter\n        self.session = session\n        self._accumulated_content = ''\n    \n    async def generate_initial_plan(\n        self,\n        task: str,\n        prd: str | None = None,\n    ) -\u003e AsyncIterator[StreamEvent]:\n        \"\"\"\n        Generate initial implementation plan.\n        \n        Streams the plan as it's generated.\n        Final plan saved to session as plan.round0.md.\n        \n        Args:\n            task: Task description\n            prd: Optional PRD content\n            \n        Yields:\n            StreamEvent objects with plan content\n        \"\"\"\n        prompt = format_initial_plan_prompt(task, prd)\n        self._accumulated_content = ''\n        start_time = time.monotonic()\n        \n        async for event in self.adapter.stream(prompt):\n            if event.type == StreamEventType.TEXT:\n                self._accumulated_content += event.content\n            yield event\n        \n        # Save plan to session\n        if self._accumulated_content:\n            self.session.write_artifact(\n                'plan.round0.md',\n                self._accumulated_content,\n            )\n        \n        duration_ms = int((time.monotonic() - start_time) * 1000)\n        self.session.append_event({\n            'event': 'plan_generated',\n            'round': 0,\n            'duration_ms': duration_ms,\n        })\n    \n    async def synthesize_feedback(\n        self,\n        task: str,\n        prd: str | None,\n        current_plan: str,\n        feedback: dict[str, str],\n        round_num: int,\n    ) -\u003e AsyncIterator[StreamEvent]:\n        \"\"\"\n        Synthesize advisor feedback into updated plan.\n        \n        Incorporates feedback, generates decision log,\n        and assesses convergence.\n        \n        Args:\n            task: Original task description\n            prd: Optional PRD content\n            current_plan: Plan from previous round\n            feedback: Dict of advisor name -\u003e feedback content\n            round_num: Current round number\n            \n        Yields:\n            StreamEvent objects with synthesis content\n        \"\"\"\n        prompt = format_synthesis_prompt(\n            task=task,\n            prd=prd,\n            current_plan=current_plan,\n            feedback=feedback,\n            round_num=round_num,\n        )\n        self._accumulated_content = ''\n        start_time = time.monotonic()\n        \n        async for event in self.adapter.stream(prompt):\n            if event.type == StreamEventType.TEXT:\n                self._accumulated_content += event.content\n            yield event\n        \n        # Save synthesized plan\n        if self._accumulated_content:\n            self.session.write_artifact(\n                f'plan.round{round_num}.md',\n                self._accumulated_content,\n            )\n        \n        duration_ms = int((time.monotonic() - start_time) * 1000)\n        self.session.append_event({\n            'event': 'synthesis_completed',\n            'round': round_num,\n            'duration_ms': duration_ms,\n        })\n    \n    def get_accumulated_content(self) -\u003e str:\n        \"\"\"Get the full accumulated content from last operation.\"\"\"\n        return self._accumulated_content\n    \n    def extract_plan_content(self) -\u003e str:\n        \"\"\"\n        Extract just the plan content (without Decision Log/Convergence).\n        \n        Used for passing to next round's advisors.\n        \"\"\"\n        content = self._accumulated_content\n        \n        # Find where Decision Log starts\n        decision_log_start = content.find('## Decision Log')\n        if decision_log_start \u003e 0:\n            return content[:decision_log_start].strip()\n        \n        return content.strip()\n    \n    def extract_decision_log(self) -\u003e str:\n        \"\"\"Extract the Decision Log section.\"\"\"\n        content = self._accumulated_content\n        \n        start = content.find('## Decision Log')\n        if start \u003c 0:\n            return ''\n        \n        # Find end (Convergence Assessment)\n        end = content.find('## Convergence Assessment', start)\n        if end \u003e start:\n            return content[start:end].strip()\n        \n        return content[start:].strip()\n```\n\n## Acceptance Criteria\n- [ ] Initial plan generation streams correctly\n- [ ] Synthesis streams correctly\n- [ ] Plan content saved to session\n- [ ] Events logged correctly\n- [ ] Can extract plan without metadata\n- [ ] Can extract decision log\n- [ ] Content accumulation works","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:20:55.604949-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:20:55.604949-06:00","labels":["core-logic","melder","streaming"],"dependencies":[{"issue_id":"meld-eq0.6.2","depends_on_id":"meld-eq0.6","type":"parent-child","created_at":"2026-01-15T23:20:55.60821-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.6.2","depends_on_id":"meld-eq0.6.1","type":"blocks","created_at":"2026-01-15T23:44:08.215868-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.6.2","depends_on_id":"meld-eq0.4.1","type":"blocks","created_at":"2026-01-15T23:44:08.504771-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.6.3","title":"6.3: Implement convergence detection logic","description":"# Subtask 6.3: Convergence Detection\n\n## What to Implement\nLogic to parse Melder's convergence assessment and validate it.\n\n## Location\n`meld/convergence.py`\n\n## Implementation\n\n```python\nimport re\nimport json\nfrom dataclasses import dataclass\nfrom difflib import SequenceMatcher\n\n\n@dataclass\nclass ConvergenceAssessment:\n    \"\"\"Parsed convergence assessment from Melder output.\"\"\"\n    status: str  # 'CONVERGED' or 'CONTINUING'\n    changes_made: int\n    open_items: int\n    deferred_items: list[str]\n    rationale: str\n    \n    @property\n    def is_converged(self) -\u003e bool:\n        return self.status == 'CONVERGED'\n\n\ndef parse_convergence_json(response: str) -\u003e ConvergenceAssessment | None:\n    \"\"\"\n    Extract JSON convergence block from Melder response.\n    \n    Looks for:\n    ```json\n    {\"status\": \"CONVERGED\", ...}\n    ```\n    \n    Returns:\n        ConvergenceAssessment if found and valid, else None\n    \"\"\"\n    # Find JSON code block\n    match = re.search(\n        r'```json\\s*(\\{[^\\x60]+\\})\\s*```',\n        response,\n        re.DOTALL\n    )\n    \n    if not match:\n        return None\n    \n    try:\n        data = json.loads(match.group(1))\n        return ConvergenceAssessment(\n            status=data.get('status', 'CONTINUING'),\n            changes_made=data.get('changes_made', 0),\n            open_items=data.get('open_items', 0),\n            deferred_items=data.get('deferred_items', []),\n            rationale=data.get('rationale', ''),\n        )\n    except (json.JSONDecodeError, KeyError):\n        return None\n\n\ndef parse_convergence_fallback(response: str) -\u003e ConvergenceAssessment:\n    \"\"\"\n    Fallback parser using regex on text format.\n    \n    Parses:\n    STATUS: CONVERGED\n    CHANGES_MADE: 3\n    OPEN_ITEMS: 0\n    \"\"\"\n    def extract_int(pattern: str) -\u003e int:\n        match = re.search(pattern, response)\n        return int(match.group(1)) if match else 0\n    \n    status_match = re.search(r'STATUS:\\s*(CONVERGED|CONTINUING)', response)\n    status = status_match.group(1) if status_match else 'CONTINUING'\n    \n    return ConvergenceAssessment(\n        status=status,\n        changes_made=extract_int(r'CHANGES_MADE:\\s*(\\d+)'),\n        open_items=extract_int(r'OPEN_ITEMS:\\s*(\\d+)'),\n        deferred_items=[],  # Can't parse from text easily\n        rationale='Parsed from text fallback',\n    )\n\n\ndef compute_diff_ratio(old: str, new: str) -\u003e float:\n    \"\"\"\n    Compute how different two texts are.\n    \n    Returns:\n        0.0 = identical, 1.0 = completely different\n    \"\"\"\n    # Normalize whitespace for comparison\n    old_norm = ' '.join(old.split())\n    new_norm = ' '.join(new.split())\n    \n    # Use SequenceMatcher ratio\n    similarity = SequenceMatcher(None, old_norm, new_norm).ratio()\n    return 1.0 - similarity\n\n\ndef detect_convergence(\n    melder_response: str,\n    prev_plan: str,\n    curr_plan: str,\n    round_num: int,\n) -\u003e tuple[bool, str]:\n    \"\"\"\n    Determine if convergence has been reached.\n    \n    Uses hybrid approach:\n    1. Never converge on round 1\n    2. Parse Melder's convergence assessment\n    3. OPEN_ITEMS \u003e 0 always blocks convergence\n    4. Validate CONVERGED claim with diff check\n    \n    Args:\n        melder_response: Full Melder synthesis response\n        prev_plan: Plan from previous round\n        curr_plan: Plan from current round\n        round_num: Current round number\n        \n    Returns:\n        (converged, rationale) tuple\n    \"\"\"\n    # Rule 1: Never converge on round 1\n    if round_num == 1:\n        return False, 'First round - collecting initial feedback'\n    \n    # Parse convergence assessment\n    assessment = parse_convergence_json(melder_response)\n    if not assessment:\n        assessment = parse_convergence_fallback(melder_response)\n    \n    # Rule 2: OPEN_ITEMS \u003e 0 blocks convergence\n    if assessment.open_items \u003e 0:\n        return False, f'Continuing: {assessment.open_items} open items remain'\n    \n    # Rule 3: Deferred items count as open\n    if assessment.deferred_items:\n        count = len(assessment.deferred_items)\n        return False, f'Continuing: {count} items deferred for human decision'\n    \n    # Compute diff ratio\n    diff_ratio = compute_diff_ratio(prev_plan, curr_plan)\n    small_diff = diff_ratio \u003c 0.05  # Less than 5% changed\n    \n    # Decision matrix\n    if assessment.is_converged and small_diff:\n        return True, 'Converged: Melder confirmed, minimal changes, no open items'\n    \n    if assessment.is_converged and not small_diff:\n        return False, 'Continuing: Melder said converged but significant changes detected'\n    \n    if not assessment.is_converged and small_diff:\n        return False, 'Continuing: Melder indicates work remains'\n    \n    return False, 'Continuing: substantive changes made'\n\n\n# Decision matrix for reference:\n# | OPEN_ITEMS | Melder Says | Diff Size | Round | Action |\n# |------------|-------------|-----------|-------|--------|\n# | \u003e 0        | any         | any       | any   | Never converge |\n# | 0          | CONVERGED   | \u003c5%       | 2+    | Converge |\n# | 0          | CONVERGED   | \u003e5%       | any   | Continue |\n# | 0          | CONTINUING  | any       | any   | Continue |\n# | unknown    | any         | any       | any   | Fall back |\n# | any        | any         | any       | 1     | Continue |\n```\n\n## Acceptance Criteria\n- [ ] JSON parsing extracts all fields\n- [ ] Fallback regex works when JSON missing\n- [ ] Diff ratio computed correctly\n- [ ] Round 1 never converges\n- [ ] OPEN_ITEMS \u003e 0 blocks convergence\n- [ ] Deferred items block convergence\n- [ ] CONVERGED + large diff = continue\n- [ ] Decision matrix implemented correctly\n\n## Testing Strategy\n```python\ndef test_json_parsing():\n    response = '''\n## Convergence Assessment\n```json\n{\"status\": \"CONVERGED\", \"changes_made\": 2, \"open_items\": 0, \"rationale\": \"Plan stable\"}\n```\n'''\n    assessment = parse_convergence_json(response)\n    assert assessment.status == 'CONVERGED'\n    assert assessment.open_items == 0\n\ndef test_open_items_blocks():\n    response = '{\"status\": \"CONVERGED\", \"open_items\": 1}'\n    converged, _ = detect_convergence(response, 'old', 'new', 2)\n    assert not converged\n\ndef test_round_1_never_converges():\n    response = '{\"status\": \"CONVERGED\", \"open_items\": 0}'\n    converged, _ = detect_convergence(response, 'plan', 'plan', 1)\n    assert not converged\n```","notes":"## ADDITIONAL REQUIREMENT: Oscillation Detection\n\nThe epic specifies: 'Oscillation guard: Track last 3 plans; if cycling, produce stable plan with explicit Needs Human Decision section'\n\n### Implementation Addition\n\n```python\nimport hashlib\nfrom collections import deque\n\nclass OscillationDetector:\n    \"\"\"\n    Detect when plans are cycling between states.\n    \n    Tracks hashes of recent plans to detect repetition.\n    \"\"\"\n    \n    def __init__(self, history_size: int = 3):\n        self._history: deque[str] = deque(maxlen=history_size)\n    \n    def add_plan(self, plan: str) -\u003e bool:\n        \"\"\"\n        Add plan to history and check for oscillation.\n        \n        Returns:\n            True if oscillation detected (plan matches one in history)\n        \"\"\"\n        plan_hash = hashlib.sha256(plan.encode()).hexdigest()[:16]\n        \n        is_oscillating = plan_hash in self._history\n        self._history.append(plan_hash)\n        \n        return is_oscillating\n    \n    def get_oscillation_response(self, competing_options: list[str]) -\u003e str:\n        \"\"\"Generate response for detected oscillation.\"\"\"\n        return f'''\n## ⚠️ Needs Human Decision\n\nThe advisors are providing conflicting feedback that creates a cycle.\nManual intervention is required to break the tie.\n\n### Competing Options\n{chr(10).join(f'- {opt}' for opt in competing_options)}\n\nPlease make a decision and re-run with the clarified requirement.\n'''\n```\n\n### Updated detect_convergence Function\n\n```python\ndef detect_convergence(\n    response: str,\n    prev_plan: str,\n    curr_plan: str,\n    round_num: int,\n    oscillation_detector: OscillationDetector | None = None,\n) -\u003e tuple[bool, str]:\n    \"\"\"\n    Detect if convergence is reached.\n    \n    Updated to include oscillation detection.\n    \"\"\"\n    # Check for oscillation first\n    if oscillation_detector and oscillation_detector.add_plan(curr_plan):\n        return False, 'Oscillation detected - plans are cycling'\n    \n    # ... rest of existing logic ...\n```\n\n### Acceptance Criteria Addition\n- [ ] Oscillation detector tracks last 3 plans\n- [ ] Returns True when current plan matches any in history\n- [ ] Generates 'Needs Human Decision' response with competing options\n- [ ] Oscillation blocks convergence even if Melder claims converged","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:21:53.978212-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:03:52.422324-06:00","labels":["convergence","core-logic","detection"],"dependencies":[{"issue_id":"meld-eq0.6.3","depends_on_id":"meld-eq0.6","type":"parent-child","created_at":"2026-01-15T23:21:53.98148-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.6.3","depends_on_id":"meld-eq0.6.2","type":"blocks","created_at":"2026-01-15T23:44:08.793233-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.6.4","title":"6.4: Implement plan content extraction","description":"# Subtask 6.4: Plan Content Extraction\n\n## Overview\nExtract the clean plan content from Melder output, separating it from the Decision Log and Convergence Assessment JSON block.\n\n## Why This Is Needed\nThe Melder synthesize response contains three parts:\n1. The implementation plan (what we want for final output)\n2. Decision Log section (internal tracking)\n3. Convergence Assessment JSON block (for convergence detection)\n\nThe final output should contain ONLY the clean plan.\n\n## Input Structure (Melder Response)\n```markdown\n# Implementation Plan\n\n## Overview\n[Plan content here...]\n\n## Architecture Decisions\n[More plan content...]\n\n## Implementation Steps\n[Steps here...]\n\n---\n\n## Decision Log\n\nACCEPTED:\n- [Claude] Use JWT instead of sessions\n\nREJECTED:\n- [Gemini] Remove validation\n\nDEFERRED:\n- Database choice needs input\n\n## Convergence Assessment\n\n\\`\\`\\`json\n{\"status\": \"CONVERGED\", \"changes_made\": 2, \"open_items\": 0}\n\\`\\`\\`\n```\n\n## Expected Output (Clean Plan)\n```markdown\n# Implementation Plan\n\n## Overview\n[Plan content here...]\n\n## Architecture Decisions\n[More plan content...]\n\n## Implementation Steps\n[Steps here...]\n```\n\n## Implementation\n\n### Extraction Function\n```python\nimport re\n\ndef extract_plan_content(melder_response: str) -\u003e str:\n    \"\"\"\n    Extract clean plan content from Melder response.\n    \n    Removes:\n    - Decision Log section\n    - Convergence Assessment section\n    - Any trailing separators\n    \n    Args:\n        melder_response: Full Melder output\n        \n    Returns:\n        Clean plan content only\n    \"\"\"\n    # Find the Decision Log marker\n    decision_log_pattern = r'\\n---\\s*\\n##\\s*Decision\\s*Log'\n    \n    match = re.search(decision_log_pattern, melder_response, re.IGNORECASE)\n    \n    if match:\n        # Return everything before the Decision Log\n        plan = melder_response[:match.start()].strip()\n    else:\n        # No Decision Log found, try to find Convergence Assessment\n        convergence_pattern = r'\\n##\\s*Convergence\\s*Assessment'\n        match = re.search(convergence_pattern, melder_response, re.IGNORECASE)\n        \n        if match:\n            plan = melder_response[:match.start()].strip()\n        else:\n            # No markers found, return as-is (initial plan case)\n            plan = melder_response.strip()\n    \n    return plan\n```\n\n### Integration with Melder Class\n```python\nclass Melder:\n    def extract_plan_content(self) -\u003e str:\n        \"\"\"Get just the plan portion of the last response.\"\"\"\n        return extract_plan_content(self._accumulated_content)\n```\n\n## Edge Cases\n1. Initial plan (no Decision Log): Return entire response\n2. Malformed response (markers in wrong place): Best effort extraction\n3. Empty response: Return empty string\n4. Plan contains code blocks with similar markers: Handle carefully\n\n## Acceptance Criteria\n- [ ] Correctly extracts plan from synthesis response\n- [ ] Works for initial plan (no Decision Log)\n- [ ] Handles missing or malformed markers gracefully\n- [ ] Does not accidentally remove plan content\n- [ ] Strips trailing whitespace and separators\n- [ ] Preserves all markdown formatting in plan\n\n## Testing Strategy\n```python\ndef test_extract_with_decision_log():\n    response = '''# Plan\nContent here\n\n---\n\n## Decision Log\nACCEPTED: ...'''\n    \n    result = extract_plan_content(response)\n    assert result == '# Plan\\nContent here'\n    assert 'Decision Log' not in result\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-16T00:03:24.427621-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:03:24.427621-06:00","labels":["core-logic","extraction","melder"],"dependencies":[{"issue_id":"meld-eq0.6.4","depends_on_id":"meld-eq0.6","type":"parent-child","created_at":"2026-01-16T00:03:24.455298-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.6.4","depends_on_id":"meld-eq0.6.2","type":"blocks","created_at":"2026-01-16T00:03:30.568272-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.7","title":"Task 7: Advisor Pool Implementation","description":"# Task 7: Advisor Pool Implementation\n\n## Overview\nThe Advisor Pool manages parallel invocation of all three advisors (Claude, Gemini, Codex), collecting their feedback, handling failures, and implementing retry logic.\n\n## Design Principles\n\n### Parallel Execution\nAll advisors run simultaneously to minimize latency.\nTotal time ≈ slowest advisor, not sum of all.\n\n### Graceful Degradation\n- If 1 advisor fails: Continue with 2\n- If 2 advisors fail: Continue with 1 (with warning)\n- If all fail: Use best plan so far\n\n### Retry Strategy\n| Error | Max Retries | Backoff |\n|-------|-------------|---------|\n| TIMEOUT | 1 | None |\n| RATE_LIMITED | 3 | Exponential (1s, 2s, 4s) |\n| NETWORK_ERROR | 3 | Linear (1s, 1s, 1s) |\n| AUTH_FAILED | 0 | N/A |\n| CLI_NOT_FOUND | 0 | N/A |\n\n## Technical Requirements\n\n### AdvisorPool Class\n```python\nclass AdvisorPool:\n    def __init__(\n        self,\n        adapters: list[ProviderAdapter],\n        session: SessionManager,\n    ):\n        self.adapters = adapters\n        self.session = session\n    \n    async def collect_feedback(\n        self,\n        task: str,\n        prd: str | None,\n        current_plan: str,\n        round_num: int,\n        timeout: float = 600.0,\n    ) -\u003e dict[str, AdvisorResult]:\n        \"\"\"Collect feedback from all advisors in parallel.\"\"\"\n    \n    async def stream_feedback(\n        self,\n        task: str,\n        prd: str | None,\n        current_plan: str,\n        round_num: int,\n    ) -\u003e AsyncIterator[tuple[str, StreamEvent]]:\n        \"\"\"Stream feedback from all advisors with provider tags.\"\"\"\n```\n\n### Streaming Interface\nFor TUI display, need to know which advisor each event came from:\n```python\nasync for provider, event in pool.stream_feedback(...):\n    tui.update_panel(provider, event)\n```\n\n### Result Aggregation\n```python\n@dataclass\nclass PoolResult:\n    feedback: dict[str, str]  # advisor -\u003e content\n    failed: list[str]         # advisors that failed\n    warnings: list[str]       # retry notices etc.\n    all_succeeded: bool\n```\n\n## Error Handling\n\n### Per-Advisor Retry\n```python\nasync def _run_with_retry(\n    self,\n    adapter: ProviderAdapter,\n    prompt: str,\n    timeout: float,\n) -\u003e AdvisorResult:\n    \"\"\"Run adapter with retry logic.\"\"\"\n    retry_config = RETRY_CONFIG.get(None)  # Default\n    \n    for attempt in range(retry_config['max_retries'] + 1):\n        result = await adapter.run(prompt, timeout)\n        \n        if result.success:\n            return result\n        \n        # Check if error is retryable\n        config = RETRY_CONFIG.get(result.error)\n        if not config or attempt \u003e= config['max_retries']:\n            return result  # Give up\n        \n        # Calculate backoff\n        if config['backoff'] == 'exponential':\n            delay = 2 ** attempt\n        else:\n            delay = 1\n        \n        await asyncio.sleep(delay)\n    \n    return result\n```\n\n### Parallel Execution\n```python\nasync def collect_feedback(...) -\u003e dict[str, AdvisorResult]:\n    prompt = format_advisor_prompt(task, prd, current_plan, round_num)\n    \n    # Run all advisors in parallel\n    tasks = {\n        adapter.name: self._run_with_retry(adapter, prompt, timeout)\n        for adapter in self.adapters\n    }\n    \n    results = await asyncio.gather(*tasks.values(), return_exceptions=True)\n    \n    return dict(zip(tasks.keys(), results))\n```\n\n## Session Integration\n\n### Save Feedback Artifacts\n```python\nfor name, result in results.items():\n    if result.success:\n        self.session.write_artifact(\n            f'advisor.{name}.round{round_num}.md',\n            result.content,\n        )\n```\n\n### Log Events\n```python\nself.session.append_event({\n    'event': 'advisor_completed',\n    'advisor': name,\n    'round': round_num,\n    'success': result.success,\n    'duration_ms': result.duration_ms,\n})\n```\n\n## Acceptance Criteria\n- [ ] Parallel execution works\n- [ ] Streaming with provider tags works\n- [ ] Retry logic for retryable errors\n- [ ] No retry for auth/CLI errors\n- [ ] Feedback saved to session\n- [ ] Events logged correctly\n- [ ] Graceful degradation works\n- [ ] Timeout per advisor honored\n\n## Notes\n- Each advisor gets same prompt\n- Keep feedback independent (no cross-contamination)\n- Resume should skip already-completed advisors","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:22:27.621204-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:22:27.621204-06:00","labels":["advisors","core-logic","parallel","phase-2"],"dependencies":[{"issue_id":"meld-eq0.7","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:22:27.624904-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.7","depends_on_id":"meld-eq0.4","type":"blocks","created_at":"2026-01-15T23:44:27.33385-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.7","depends_on_id":"meld-eq0.2","type":"blocks","created_at":"2026-01-15T23:44:27.586073-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.7.1","title":"7.1: Implement parallel advisor invocation","description":"# Subtask 7.1: Parallel Advisor Invocation\n\n## What to Implement\nCore parallel execution logic for running all advisors simultaneously.\n\n## Location\n`meld/advisors.py`\n\n## Implementation\n\n```python\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom typing import AsyncIterator\n\nfrom meld.providers.base import ProviderAdapter\nfrom meld.models import AdvisorResult, AdvisorError, StreamEvent\nfrom meld.session import SessionManager\nfrom meld.prompts import format_advisor_prompt\n\n\n@dataclass\nclass PoolResult:\n    \"\"\"Result from collecting feedback from advisor pool.\n    \n    Attributes:\n        feedback: Dict of advisor name -\u003e feedback content\n        results: Dict of advisor name -\u003e full AdvisorResult\n        failed: List of advisor names that failed\n        warnings: List of warning messages\n    \"\"\"\n    feedback: dict[str, str] = field(default_factory=dict)\n    results: dict[str, AdvisorResult] = field(default_factory=dict)\n    failed: list[str] = field(default_factory=list)\n    warnings: list[str] = field(default_factory=list)\n    \n    @property\n    def all_succeeded(self) -\u003e bool:\n        return len(self.failed) == 0\n    \n    @property\n    def sufficient(self) -\u003e bool:\n        \"\"\"At least 2 advisors succeeded.\"\"\"\n        return len(self.feedback) \u003e= 2\n\n\nclass AdvisorPool:\n    \"\"\"\n    Manages parallel invocation of advisor CLIs.\n    \n    Runs all advisors simultaneously, handles failures gracefully,\n    and provides streaming output for TUI display.\n    \"\"\"\n    \n    def __init__(\n        self,\n        adapters: list[ProviderAdapter],\n        session: SessionManager,\n    ):\n        \"\"\"\n        Initialize advisor pool.\n        \n        Args:\n            adapters: List of provider adapters to use\n            session: Session manager for artifact storage\n        \"\"\"\n        self.adapters = {a.name: a for a in adapters}\n        self.session = session\n    \n    async def collect_feedback(\n        self,\n        task: str,\n        prd: str | None,\n        current_plan: str,\n        round_num: int,\n        timeout: float = 600.0,\n        skip_advisors: list[str] | None = None,\n    ) -\u003e PoolResult:\n        \"\"\"\n        Collect feedback from all advisors in parallel.\n        \n        Args:\n            task: Original task description\n            prd: Optional PRD content\n            current_plan: Current plan to review\n            round_num: Current round number\n            timeout: Per-advisor timeout in seconds\n            skip_advisors: List of advisors to skip (e.g., already completed)\n            \n        Returns:\n            PoolResult with feedback and status\n        \"\"\"\n        skip = set(skip_advisors or [])\n        prompt = format_advisor_prompt(task, prd, current_plan, round_num)\n        \n        # Create tasks for each advisor\n        async def run_advisor(name: str, adapter: ProviderAdapter) -\u003e tuple[str, AdvisorResult]:\n            result = await adapter.run(prompt, timeout)\n            return name, result\n        \n        # Filter to active adapters\n        active = {\n            name: adapter \n            for name, adapter in self.adapters.items() \n            if name not in skip\n        }\n        \n        # Run in parallel\n        tasks = [\n            run_advisor(name, adapter)\n            for name, adapter in active.items()\n        ]\n        \n        results_list = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Process results\n        pool_result = PoolResult()\n        \n        for item in results_list:\n            if isinstance(item, Exception):\n                # Shouldn't happen but handle gracefully\n                pool_result.warnings.append(f'Unexpected error: {item}')\n                continue\n            \n            name, result = item\n            pool_result.results[name] = result\n            \n            if result.success:\n                pool_result.feedback[name] = result.content\n                \n                # Save to session\n                self.session.write_artifact(\n                    f'advisor.{name}.round{round_num}.md',\n                    result.content,\n                )\n            else:\n                pool_result.failed.append(name)\n                pool_result.warnings.append(\n                    f'{name} failed: {result.error_message}'\n                )\n            \n            # Log event\n            self.session.append_event({\n                'event': 'advisor_completed' if result.success else 'advisor_failed',\n                'advisor': name,\n                'round': round_num,\n                'success': result.success,\n                'duration_ms': result.duration_ms,\n                'error': result.error.name if result.error else None,\n            })\n        \n        return pool_result\n```\n\n## Acceptance Criteria\n- [ ] All advisors run in parallel\n- [ ] Results collected correctly\n- [ ] Failed advisors tracked\n- [ ] Feedback saved to session\n- [ ] Events logged\n- [ ] Skip list honored\n- [ ] Exceptions handled gracefully","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:23:04.136798-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:23:04.136798-06:00","labels":["advisors","core-logic","parallel"],"dependencies":[{"issue_id":"meld-eq0.7.1","depends_on_id":"meld-eq0.7","type":"parent-child","created_at":"2026-01-15T23:23:04.141718-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.7.1","depends_on_id":"meld-eq0.4.1","type":"blocks","created_at":"2026-01-15T23:44:27.790838-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.7.1","depends_on_id":"meld-eq0.4.2","type":"blocks","created_at":"2026-01-15T23:44:28.060122-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.7.1","depends_on_id":"meld-eq0.4.3","type":"blocks","created_at":"2026-01-15T23:44:28.351408-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.7.2","title":"7.2: Implement streaming feedback and retry logic","description":"# Subtask 7.2: Streaming \u0026 Retry Logic\n\n## What to Implement\nStreaming output for TUI display and retry logic for transient failures.\n\n## Location\n`meld/advisors.py` (continued)\n\n## Retry Configuration\n\n```python\nfrom meld.models import AdvisorError\n\n# Retry configuration per error type\nRETRY_CONFIG = {\n    AdvisorError.TIMEOUT: {'max_retries': 1, 'backoff': None},\n    AdvisorError.RATE_LIMITED: {'max_retries': 3, 'backoff': 'exponential'},\n    AdvisorError.NETWORK_ERROR: {'max_retries': 3, 'backoff': 'linear'},\n    # These should not be retried\n    AdvisorError.CLI_NOT_FOUND: {'max_retries': 0, 'backoff': None},\n    AdvisorError.AUTH_FAILED: {'max_retries': 0, 'backoff': None},\n    AdvisorError.PARSE_ERROR: {'max_retries': 0, 'backoff': None},\n    AdvisorError.INTERNAL_ERROR: {'max_retries': 1, 'backoff': None},\n}\n\ndef get_retry_config(error: AdvisorError) -\u003e dict:\n    \"\"\"Get retry configuration for error type.\"\"\"\n    return RETRY_CONFIG.get(error, {'max_retries': 0, 'backoff': None})\n```\n\n## Retry Implementation\n\n```python\nclass AdvisorPool:\n    # ... existing code ...\n    \n    async def _run_with_retry(\n        self,\n        adapter: ProviderAdapter,\n        prompt: str,\n        timeout: float,\n    ) -\u003e AdvisorResult:\n        \"\"\"\n        Run adapter with retry logic for transient failures.\n        \n        Args:\n            adapter: The provider adapter\n            prompt: Prompt to send\n            timeout: Timeout per attempt\n            \n        Returns:\n            AdvisorResult from final attempt\n        \"\"\"\n        last_result = None\n        \n        for attempt in range(4):  # Max 4 attempts (initial + 3 retries)\n            result = await adapter.run(prompt, timeout)\n            last_result = result\n            \n            if result.success:\n                return result\n            \n            # Check if we should retry\n            config = get_retry_config(result.error)\n            if attempt \u003e= config['max_retries']:\n                break\n            \n            # Calculate delay\n            if config['backoff'] == 'exponential':\n                delay = 2 ** attempt  # 1, 2, 4 seconds\n            elif config['backoff'] == 'linear':\n                delay = 1  # Always 1 second\n            else:\n                delay = 0\n            \n            if delay \u003e 0:\n                # Log retry attempt\n                self.session.append_event({\n                    'event': 'advisor_retry',\n                    'advisor': adapter.name,\n                    'attempt': attempt + 1,\n                    'error': result.error.name,\n                    'delay_seconds': delay,\n                })\n                await asyncio.sleep(delay)\n        \n        return last_result\n```\n\n## Streaming Implementation\n\n```python\nclass AdvisorPool:\n    # ... existing code ...\n    \n    async def stream_feedback(\n        self,\n        task: str,\n        prd: str | None,\n        current_plan: str,\n        round_num: int,\n        timeout: float = 600.0,\n    ) -\u003e AsyncIterator[tuple[str, StreamEvent]]:\n        \"\"\"\n        Stream feedback from all advisors with provider identification.\n        \n        Runs advisors in parallel and yields events tagged with\n        the source provider name.\n        \n        Args:\n            task: Original task description\n            prd: Optional PRD content\n            current_plan: Current plan to review\n            round_num: Current round number\n            timeout: Per-advisor timeout\n            \n        Yields:\n            (provider_name, StreamEvent) tuples\n        \"\"\"\n        prompt = format_advisor_prompt(task, prd, current_plan, round_num)\n        \n        # Create streaming tasks\n        async def stream_advisor(name: str, adapter: ProviderAdapter):\n            accumulated = ''\n            async for event in adapter.stream(prompt, timeout):\n                if event.type == StreamEventType.TEXT:\n                    accumulated += event.content\n                yield name, event\n            \n            # Save result when done\n            if accumulated:\n                self.session.write_artifact(\n                    f'advisor.{name}.round{round_num}.md',\n                    accumulated,\n                )\n        \n        # Merge streams from all advisors\n        streams = [\n            stream_advisor(name, adapter)\n            for name, adapter in self.adapters.items()\n        ]\n        \n        # Use async merge to interleave streams\n        async for item in self._merge_streams(streams):\n            yield item\n    \n    async def _merge_streams(\n        self,\n        streams: list[AsyncIterator[tuple[str, StreamEvent]]],\n    ) -\u003e AsyncIterator[tuple[str, StreamEvent]]:\n        \"\"\"\n        Merge multiple async streams fairly.\n        \n        Uses asyncio.Queue to collect from all streams\n        and yield in arrival order.\n        \"\"\"\n        queue = asyncio.Queue()\n        done_count = 0\n        total = len(streams)\n        \n        async def feed_queue(stream):\n            nonlocal done_count\n            try:\n                async for item in stream:\n                    await queue.put(item)\n            except Exception as e:\n                await queue.put(('__error__', e))\n            finally:\n                done_count += 1\n                if done_count \u003e= total:\n                    await queue.put(None)  # Signal completion\n        \n        # Start all feeders\n        feeders = [asyncio.create_task(feed_queue(s)) for s in streams]\n        \n        try:\n            while True:\n                item = await queue.get()\n                if item is None:\n                    break\n                if item[0] == '__error__':\n                    # Handle error somehow\n                    continue\n                yield item\n        finally:\n            for task in feeders:\n                task.cancel()\n```\n\n## Acceptance Criteria\n- [ ] Retry logic applies correct delays\n- [ ] Exponential backoff: 1s, 2s, 4s\n- [ ] Linear backoff: 1s, 1s, 1s\n- [ ] No retry for auth/CLI errors\n- [ ] Streaming merges all sources\n- [ ] Events tagged with provider\n- [ ] Content saved after streaming completes\n\n## Testing\n```python\nasync def test_retry_rate_limited():\n    # Mock adapter that fails twice then succeeds\n    adapter = MockAdapter(fail_count=2, error=AdvisorError.RATE_LIMITED)\n    pool = AdvisorPool([adapter], mock_session)\n    \n    result = await pool._run_with_retry(adapter, 'test', 60)\n    assert result.success\n    assert adapter.call_count == 3\n```","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:23:55.059971-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:23:55.059971-06:00","labels":["advisors","core-logic","retry","streaming"],"dependencies":[{"issue_id":"meld-eq0.7.2","depends_on_id":"meld-eq0.7","type":"parent-child","created_at":"2026-01-15T23:23:55.062604-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.7.2","depends_on_id":"meld-eq0.7.1","type":"blocks","created_at":"2026-01-15T23:44:28.650236-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.8","title":"Task 8: Orchestrator \u0026 Main Loop","description":"# Task 8: Orchestrator \u0026 Main Loop\n\n## Overview\nThe Orchestrator coordinates the entire convergence loop: initial plan → feedback → synthesis → check convergence → repeat.\n\n## Data Flow\n```\nTask Input (+ optional PRD)\n          ↓\n    Preflight checks\n          ↓\n    Create session\n          ↓\n    Melder (initial plan)\n          ↓\n    [Checkpoint: round 0]\n          ↓\n    ┌──────┼──────┐\n    ↓      ↓      ↓\n  Claude Gemini Codex\n  (parallel feedback)\n    └──────┼──────┘\n          ↓\n    Melder (synthesize)\n          ↓\n    [Checkpoint: round N]\n          ↓\n    OPEN_ITEMS \u003e 0? → Yes → Continue\n          ↓ No\n    Converged? → Yes → Output\n          ↓ No\n    Max rounds? → Yes → Output\n          ↓ No\n    Loop back to feedback\n```\n\n## State Machine\n```\nPLANNING → FEEDBACK → SYNTHESIS → CHECK_CONVERGENCE\n                                        ↓\n                           [converged] → COMPLETE\n                           [not converged \u0026 rounds left] → FEEDBACK\n                           [max rounds] → COMPLETE\n```\n\n## Technical Requirements\n\n### Orchestrator Class\n```python\nclass Orchestrator:\n    def __init__(\n        self,\n        session: SessionManager,\n        melder: Melder,\n        advisor_pool: AdvisorPool,\n        max_rounds: int = 5,\n        timeout: float = 600.0,\n    ):\n        self.session = session\n        self.melder = melder\n        self.pool = advisor_pool\n        self.max_rounds = max_rounds\n        self.timeout = timeout\n        self.current_round = 0\n        self.converged = False\n    \n    async def run(\n        self,\n        task: str,\n        prd: str | None = None,\n        on_event: Callable[[OrchestratorEvent], None] | None = None,\n    ) -\u003e OrchestrationResult:\n        \"\"\"Run the full convergence loop.\"\"\"\n    \n    async def run_from_checkpoint(\n        self,\n        resume_context: dict,\n    ) -\u003e OrchestrationResult:\n        \"\"\"Resume from a checkpoint.\"\"\"\n```\n\n### OrchestratorEvent\nFor TUI integration:\n```python\n@dataclass\nclass OrchestratorEvent:\n    type: str  # 'phase_changed', 'round_started', 'convergence_checked'\n    data: dict\n```\n\n### OrchestrationResult\n```python\n@dataclass\nclass OrchestrationResult:\n    final_plan: str\n    converged: bool\n    rounds_completed: int\n    decision_log: str\n    advisor_participation: dict[str, bool]\n    warnings: list[str]\n```\n\n## Main Loop Implementation\n\n```python\nasync def run(self, task: str, prd: str | None = None) -\u003e OrchestrationResult:\n    # Save inputs to session\n    self.session.write_artifact('task.md', task)\n    if prd:\n        self.session.write_artifact('prd.md', prd)\n    \n    # Phase 1: Generate initial plan\n    self._emit_event('phase_changed', {'phase': 'planning'})\n    async for event in self.melder.generate_initial_plan(task, prd):\n        self._emit_event('melder_output', {'event': event})\n    \n    current_plan = self.melder.get_accumulated_content()\n    self.session.update_status(SessionStatus.IN_PROGRESS, current_round=0)\n    \n    # Phase 2+: Feedback loop\n    for round_num in range(1, self.max_rounds + 1):\n        self.current_round = round_num\n        self._emit_event('round_started', {'round': round_num})\n        \n        # Collect feedback\n        self._emit_event('phase_changed', {'phase': 'feedback', 'round': round_num})\n        pool_result = await self.pool.collect_feedback(\n            task, prd, current_plan, round_num, self.timeout\n        )\n        \n        if not pool_result.feedback:\n            # All advisors failed\n            break\n        \n        # Synthesize\n        self._emit_event('phase_changed', {'phase': 'synthesis', 'round': round_num})\n        prev_plan = current_plan\n        \n        async for event in self.melder.synthesize_feedback(\n            task, prd, current_plan, pool_result.feedback, round_num\n        ):\n            self._emit_event('melder_output', {'event': event})\n        \n        current_plan = self.melder.extract_plan_content()\n        full_response = self.melder.get_accumulated_content()\n        \n        # Check convergence\n        converged, rationale = detect_convergence(\n            full_response, prev_plan, current_plan, round_num\n        )\n        \n        self._emit_event('convergence_checked', {\n            'round': round_num,\n            'converged': converged,\n            'rationale': rationale,\n        })\n        \n        self.session.update_status(\n            SessionStatus.IN_PROGRESS,\n            current_round=round_num,\n        )\n        \n        if converged:\n            self.converged = True\n            break\n    \n    # Complete\n    self.session.update_status(\n        SessionStatus.COMPLETED,\n        convergence={'status': 'converged' if self.converged else 'max_rounds'}\n    )\n    \n    return OrchestrationResult(...)\n```\n\n## Acceptance Criteria\n- [ ] Initial plan generation works\n- [ ] Feedback collection loops correctly\n- [ ] Synthesis updates plan\n- [ ] Convergence check terminates loop\n- [ ] Max rounds terminates loop\n- [ ] Events emitted for TUI\n- [ ] Session updated at checkpoints\n- [ ] Resume from checkpoint works\n\n## Integration Points\n- **TUI**: Subscribe to events for display updates\n- **Session**: Checkpoints after each phase\n- **Preflight**: Called before orchestrator runs\n- **Output**: Final plan passed to formatter","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:24:32.570767-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:24:32.570767-06:00","labels":["core-logic","loop","orchestrator","phase-2"],"dependencies":[{"issue_id":"meld-eq0.8","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:24:32.57436-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.8","depends_on_id":"meld-eq0.6","type":"blocks","created_at":"2026-01-15T23:44:28.835023-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.8","depends_on_id":"meld-eq0.7","type":"blocks","created_at":"2026-01-15T23:44:29.053281-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.8.1","title":"8.1: Implement main convergence loop","description":"# Subtask 8.1: Main Convergence Loop\n\n## What to Implement\nThe core orchestration logic that runs the plan → feedback → synthesis cycle.\n\n## Location\n`meld/orchestrator.py`\n\n## Implementation\n\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Callable, AsyncIterator\nimport asyncio\n\nfrom meld.session import SessionManager, SessionStatus\nfrom meld.melder import Melder\nfrom meld.advisors import AdvisorPool, PoolResult\nfrom meld.convergence import detect_convergence\nfrom meld.models import StreamEvent\n\n\n@dataclass\nclass OrchestratorEvent:\n    \"\"\"Event emitted during orchestration for TUI updates.\"\"\"\n    type: str\n    data: dict = field(default_factory=dict)\n\n\n@dataclass\nclass OrchestrationResult:\n    \"\"\"Final result of orchestration.\"\"\"\n    final_plan: str\n    converged: bool\n    rounds_completed: int\n    decision_log: str\n    advisor_participation: dict[str, bool]\n    warnings: list[str] = field(default_factory=list)\n\n\nclass Orchestrator:\n    \"\"\"\n    Coordinates the multi-model plan convergence loop.\n    \n    The orchestrator manages the flow:\n    1. Generate initial plan (Melder)\n    2. Collect advisor feedback (parallel)\n    3. Synthesize feedback (Melder)\n    4. Check convergence\n    5. Repeat 2-4 until converged or max rounds\n    \"\"\"\n    \n    def __init__(\n        self,\n        session: SessionManager,\n        melder: Melder,\n        advisor_pool: AdvisorPool,\n        max_rounds: int = 5,\n        timeout: float = 600.0,\n    ):\n        self.session = session\n        self.melder = melder\n        self.pool = advisor_pool\n        self.max_rounds = max_rounds\n        self.timeout = timeout\n        \n        self.current_round = 0\n        self.converged = False\n        self._event_callback: Callable[[OrchestratorEvent], None] | None = None\n        self._advisor_participation: dict[str, bool] = {}\n        self._warnings: list[str] = []\n    \n    def set_event_callback(\n        self,\n        callback: Callable[[OrchestratorEvent], None],\n    ) -\u003e None:\n        \"\"\"Set callback for orchestration events.\"\"\"\n        self._event_callback = callback\n    \n    def _emit(self, event_type: str, **data) -\u003e None:\n        \"\"\"Emit an orchestration event.\"\"\"\n        if self._event_callback:\n            self._event_callback(OrchestratorEvent(event_type, data))\n    \n    async def run(\n        self,\n        task: str,\n        prd: str | None = None,\n    ) -\u003e OrchestrationResult:\n        \"\"\"\n        Run the full convergence loop.\n        \n        Args:\n            task: Task description\n            prd: Optional PRD content\n            \n        Returns:\n            OrchestrationResult with final plan and metadata\n        \"\"\"\n        # Initialize\n        self._advisor_participation = {}\n        self._warnings = []\n        \n        # Save inputs\n        self.session.write_artifact('task.md', task)\n        if prd:\n            self.session.write_artifact('prd.md', prd)\n        \n        # Phase 1: Initial plan\n        self._emit('phase_changed', phase='planning')\n        \n        async for event in self.melder.generate_initial_plan(task, prd):\n            self._emit('melder_stream', provider='melder', event=event)\n        \n        current_plan = self.melder.get_accumulated_content()\n        self.session.update_status(SessionStatus.IN_PROGRESS, current_round=0)\n        self._emit('checkpoint', round=0)\n        \n        # Phase 2+: Feedback loop\n        decision_log = ''\n        \n        for round_num in range(1, self.max_rounds + 1):\n            self.current_round = round_num\n            self._emit('round_started', round=round_num, max_rounds=self.max_rounds)\n            \n            # Collect feedback\n            self._emit('phase_changed', phase='feedback', round=round_num)\n            pool_result = await self.pool.collect_feedback(\n                task=task,\n                prd=prd,\n                current_plan=current_plan,\n                round_num=round_num,\n                timeout=self.timeout,\n            )\n            \n            # Track participation\n            for name in pool_result.feedback:\n                self._advisor_participation[name] = True\n            for name in pool_result.failed:\n                if name not in self._advisor_participation:\n                    self._advisor_participation[name] = False\n            \n            self._warnings.extend(pool_result.warnings)\n            \n            # Check if we have enough feedback\n            if not pool_result.feedback:\n                self._emit('all_advisors_failed')\n                self._warnings.append('All advisors failed - using current plan')\n                break\n            \n            # Synthesize\n            self._emit('phase_changed', phase='synthesis', round=round_num)\n            prev_plan = current_plan\n            \n            async for event in self.melder.synthesize_feedback(\n                task=task,\n                prd=prd,\n                current_plan=current_plan,\n                feedback=pool_result.feedback,\n                round_num=round_num,\n            ):\n                self._emit('melder_stream', provider='melder', event=event)\n            \n            current_plan = self.melder.extract_plan_content()\n            full_response = self.melder.get_accumulated_content()\n            decision_log = self.melder.extract_decision_log()\n            \n            # Check convergence\n            converged, rationale = detect_convergence(\n                full_response, prev_plan, current_plan, round_num\n            )\n            \n            self._emit('convergence_checked',\n                round=round_num,\n                converged=converged,\n                rationale=rationale,\n            )\n            \n            # Update session\n            self.session.update_status(\n                SessionStatus.IN_PROGRESS,\n                current_round=round_num,\n            )\n            self._emit('checkpoint', round=round_num)\n            \n            if converged:\n                self.converged = True\n                self._emit('converged', round=round_num, rationale=rationale)\n                break\n        \n        # Finalize\n        self.session.write_artifact('final-plan.md', current_plan)\n        self.session.update_status(\n            SessionStatus.COMPLETED,\n            convergence={\n                'status': 'converged' if self.converged else 'max_rounds',\n                'rounds': self.current_round,\n            }\n        )\n        \n        self._emit('completed',\n            converged=self.converged,\n            rounds=self.current_round,\n        )\n        \n        return OrchestrationResult(\n            final_plan=current_plan,\n            converged=self.converged,\n            rounds_completed=self.current_round,\n            decision_log=decision_log,\n            advisor_participation=self._advisor_participation,\n            warnings=self._warnings,\n        )\n```\n\n## Acceptance Criteria\n- [ ] Initial plan generates correctly\n- [ ] Feedback loop iterates correctly\n- [ ] Synthesis updates plan each round\n- [ ] Convergence terminates loop early\n- [ ] Max rounds terminates loop\n- [ ] Events emitted at each phase\n- [ ] Checkpoints saved correctly\n- [ ] Final plan saved\n- [ ] All metadata returned in result","notes":"## EXIT CODE SPECIFICATION\n\nThe orchestrator must return correct exit codes:\n\n| Code | Condition | When |\n|------|-----------|------|\n| 0 | Converged successfully | Plan converged within max rounds |\n| 1 | Max rounds reached | Reached limit without convergence |\n| 2 | Preflight failed | Missing CLIs (handled in preflight, not here) |\n| 3 | All advisors failed | No feedback collected in a round |\n| 4 | Melder failed | Plan generation or synthesis error |\n| 5 | Interrupted | User pressed Ctrl+C (handled in signal handler) |\n\n### Implementation\n```python\nclass ExitCode(IntEnum):\n    SUCCESS = 0\n    MAX_ROUNDS = 1\n    PREFLIGHT_FAILED = 2\n    ALL_ADVISORS_FAILED = 3\n    MELDER_FAILED = 4\n    INTERRUPTED = 5\n\ndef determine_exit_code(result: OrchestrationResult) -\u003e ExitCode:\n    if result.interrupted:\n        return ExitCode.INTERRUPTED\n    if result.melder_failed:\n        return ExitCode.MELDER_FAILED\n    if not result.any_advisor_succeeded:\n        return ExitCode.ALL_ADVISORS_FAILED\n    if result.converged:\n        return ExitCode.SUCCESS\n    return ExitCode.MAX_ROUNDS\n```\n\n### Acceptance Criteria Addition\n- [ ] ExitCode enum defined with all codes\n- [ ] Correct exit code returned for each scenario\n- [ ] Exit code propagated to CLI entry point\n- [ ] JSON output includes exit_code field","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:25:27.535761-06:00","created_by":"Michael Fork","updated_at":"2026-01-16T00:05:15.369043-06:00","labels":["core-logic","loop","orchestrator"],"dependencies":[{"issue_id":"meld-eq0.8.1","depends_on_id":"meld-eq0.8","type":"parent-child","created_at":"2026-01-15T23:25:27.54868-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.8.1","depends_on_id":"meld-eq0.6.3","type":"blocks","created_at":"2026-01-15T23:44:29.248663-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.8.1","depends_on_id":"meld-eq0.7.2","type":"blocks","created_at":"2026-01-15T23:44:29.545944-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.8.2","title":"8.2: Implement signal handling and graceful shutdown","description":"# Subtask 8.2: Signal Handling \u0026 Graceful Shutdown\n\n## What to Implement\nHandle Ctrl+C and other signals for graceful shutdown with state preservation.\n\n## Location\n`meld/signals.py`\n\n## Implementation\n\n```python\nimport asyncio\nimport signal\nimport os\nfrom typing import Set, Callable\nfrom contextlib import asynccontextmanager\n\nfrom meld.session import SessionManager, SessionStatus\n\n\nclass SignalHandler:\n    \"\"\"\n    Handles signals for graceful shutdown.\n    \n    On SIGINT (Ctrl+C) or SIGTERM:\n    1. Signals all running subprocesses to terminate\n    2. Waits up to 5 seconds for graceful shutdown\n    3. Force-kills any remaining processes\n    4. Updates session status to 'interrupted'\n    5. Allows resume from last checkpoint\n    \"\"\"\n    \n    # Active subprocess PIDs to kill on shutdown\n    _active_pids: Set[int] = set()\n    \n    # Session to update on interrupt\n    _session: SessionManager | None = None\n    \n    # Callbacks to run on shutdown\n    _shutdown_callbacks: list[Callable] = []\n    \n    # Flag to indicate shutdown is in progress\n    _shutting_down: bool = False\n    \n    @classmethod\n    def register_process(cls, pid: int) -\u003e None:\n        \"\"\"Register a subprocess PID for cleanup on shutdown.\"\"\"\n        cls._active_pids.add(pid)\n    \n    @classmethod\n    def unregister_process(cls, pid: int) -\u003e None:\n        \"\"\"Unregister a subprocess PID.\"\"\"\n        cls._active_pids.discard(pid)\n    \n    @classmethod\n    def set_session(cls, session: SessionManager) -\u003e None:\n        \"\"\"Set session to update on interrupt.\"\"\"\n        cls._session = session\n    \n    @classmethod\n    def add_shutdown_callback(cls, callback: Callable) -\u003e None:\n        \"\"\"Add callback to run during shutdown.\"\"\"\n        cls._shutdown_callbacks.append(callback)\n    \n    @classmethod\n    def is_shutting_down(cls) -\u003e bool:\n        \"\"\"Check if shutdown is in progress.\"\"\"\n        return cls._shutting_down\n    \n    @classmethod\n    async def _shutdown(cls, phase: str | None = None) -\u003e None:\n        \"\"\"\n        Perform graceful shutdown.\n        \n        Args:\n            phase: Current phase when interrupted (for session state)\n        \"\"\"\n        if cls._shutting_down:\n            return\n        cls._shutting_down = True\n        \n        print('\\nInterrupted. Shutting down gracefully...', flush=True)\n        \n        # Signal all processes\n        for pid in list(cls._active_pids):\n            try:\n                os.kill(pid, signal.SIGTERM)\n            except ProcessLookupError:\n                cls._active_pids.discard(pid)\n        \n        # Wait for processes (up to 5 seconds)\n        for _ in range(50):  # 50 * 0.1 = 5 seconds\n            if not cls._active_pids:\n                break\n            \n            # Check which processes are still running\n            for pid in list(cls._active_pids):\n                try:\n                    os.kill(pid, 0)  # Check if alive\n                except ProcessLookupError:\n                    cls._active_pids.discard(pid)\n            \n            await asyncio.sleep(0.1)\n        \n        # Force kill remaining\n        for pid in list(cls._active_pids):\n            try:\n                os.kill(pid, signal.SIGKILL)\n            except ProcessLookupError:\n                pass\n            cls._active_pids.discard(pid)\n        \n        # Update session\n        if cls._session:\n            try:\n                cls._session.update_status(\n                    SessionStatus.INTERRUPTED,\n                    interrupted_at=phase,\n                )\n                print(f'Session saved. Resume with: meld --resume {cls._session.state.id}')\n            except Exception:\n                pass\n        \n        # Run callbacks\n        for callback in cls._shutdown_callbacks:\n            try:\n                if asyncio.iscoroutinefunction(callback):\n                    await callback()\n                else:\n                    callback()\n            except Exception:\n                pass\n\n\ndef setup_signal_handlers(loop: asyncio.AbstractEventLoop | None = None) -\u003e None:\n    \"\"\"\n    Install signal handlers for graceful shutdown.\n    \n    Args:\n        loop: Event loop to use (default: current running loop)\n    \"\"\"\n    def handler(signum, frame):\n        # Get or create event loop\n        try:\n            loop = asyncio.get_running_loop()\n            loop.create_task(SignalHandler._shutdown())\n        except RuntimeError:\n            # No running loop, do synchronous cleanup\n            pass\n    \n    signal.signal(signal.SIGINT, handler)\n    signal.signal(signal.SIGTERM, handler)\n\n\n@asynccontextmanager\nasync def graceful_subprocess(process):\n    \"\"\"\n    Context manager to register/unregister subprocess.\n    \n    Usage:\n        async with graceful_subprocess(process):\n            await process.communicate()\n    \"\"\"\n    if process.pid:\n        SignalHandler.register_process(process.pid)\n    try:\n        yield process\n    finally:\n        if process.pid:\n            SignalHandler.unregister_process(process.pid)\n```\n\n## Integration with Adapters\n\n```python\n# In ProviderAdapter.run()\nasync def run(self, prompt: str, timeout: float = 600.0, ...) -\u003e AdvisorResult:\n    process = await asyncio.create_subprocess_exec(...)\n    \n    async with graceful_subprocess(process):\n        try:\n            stdout, stderr = await asyncio.wait_for(\n                process.communicate(),\n                timeout=timeout,\n            )\n        except asyncio.CancelledError:\n            # Shutdown in progress\n            process.terminate()\n            raise\n```\n\n## CLI Integration\n\n```python\n# In cli.py\ndef run_command(args) -\u003e int:\n    setup_signal_handlers()\n    \n    try:\n        return asyncio.run(run_meld(args))\n    except KeyboardInterrupt:\n        return 5  # Exit code for interrupt\n```\n\n## Acceptance Criteria\n- [ ] Ctrl+C triggers graceful shutdown\n- [ ] All subprocesses terminated\n- [ ] 5 second grace period honored\n- [ ] Force kill after timeout\n- [ ] Session marked as interrupted\n- [ ] Resume command printed\n- [ ] Exit code 5 for interrupt\n- [ ] Works with nested async tasks","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:27:13.436767-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:27:13.436767-06:00","labels":["core-logic","shutdown","signals"],"dependencies":[{"issue_id":"meld-eq0.8.2","depends_on_id":"meld-eq0.8","type":"parent-child","created_at":"2026-01-15T23:27:13.445816-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.8.2","depends_on_id":"meld-eq0.8.1","type":"blocks","created_at":"2026-01-15T23:44:29.735713-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.9","title":"Task 9: TUI Implementation","description":"# Task 9: TUI Implementation\n\n## Overview\nImplement the Textual-based terminal user interface with 4-panel layout showing real-time progress across Melder and all three advisors.\n\n## Why Textual?\n- Purpose-built for terminal apps\n- Native async support\n- Rich widget library\n- Handles terminal resize\n- Good performance for streaming\n\n## Design Goals\n\n### Real-time Visibility\nUsers see all four models working simultaneously:\n- What each is currently outputting\n- How long each has been running\n- Status (waiting/running/done/failed)\n\n### Information Hierarchy\n- Melder panel is largest (top, full width) - it's generating the plan\n- Advisor panels are smaller (bottom, 1/3 width each) - supplementary feedback\n\n### Non-blocking\nTUI updates shouldn't slow down the actual work.\nUse throttling to prevent render thrashing.\n\n## Layout Specification\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                      MELDER (Claude)                        │\n│  [Round 2/5] ▌ Synthesizing feedback...                     │\n│                                                             │\n│  The authentication flow should use OAuth2 with PKCE for    │\n│  mobile clients. Based on advisor feedback, I'm updating    │\n│  the token refresh strategy to use sliding expiration...▌   │\n│                                                             │\n├───────────────────┬───────────────────┬─────────────────────┤\n│      CLAUDE       │      GEMINI       │       CODEX         │\n│    ● 45s          │   ↻ Retry 1/3     │   ◐ 2m 15s          │\n│                   │                   │                     │\n│  ## Summary       │  Connection error │  ## Summary         │\n│  - The auth flow  │  Retrying in 2s...│  - LGTM overall     │\n│    looks solid... │                   │  ...                │\n└───────────────────┴───────────────────┴─────────────────────┘\n                    [Session: 4m 32s | Round 2 | ◐ Active]\n```\n\n## Status Indicators\n\n| Status | Icon | Color | Meaning |\n|--------|------|-------|---------|\n| Waiting | ○ | dim | Not started yet |\n| Running | ◐ | yellow | Currently executing |\n| Streaming | ▌ | cyan | Actively receiving output |\n| Complete | ● | green | Finished successfully |\n| Failed | ✗ | red | Error after retries |\n| Retrying | ↻ | orange | Retry in progress |\n\n## Phase Indicators\nMelder panel header shows current phase:\n- `[Planning]` - Initial plan generation\n- `[Feedback Round N/M]` - Collecting advisor feedback\n- `[Synthesizing]` - Melder incorporating feedback\n- `[Converged]` - Process complete\n\n## Technical Requirements\n\n### Panel Widget\n```python\nclass MeldPanel(Widget):\n    \"\"\"Panel showing streaming output from a model.\"\"\"\n    \n    provider: str\n    status: PanelStatus\n    content: str\n    elapsed: float\n    \n    def update_content(self, text: str) -\u003e None: ...\n    def append_content(self, text: str) -\u003e None: ...\n    def set_status(self, status: PanelStatus) -\u003e None: ...\n```\n\n### Main App\n```python\nclass MeldApp(App):\n    \"\"\"Main Textual application.\"\"\"\n    \n    def compose(self) -\u003e ComposeResult:\n        yield Header()\n        yield MeldPanel(id='melder')\n        with Horizontal():\n            yield MeldPanel(id='claude')\n            yield MeldPanel(id='gemini')\n            yield MeldPanel(id='codex')\n        yield Footer()\n```\n\n### Event Integration\n```python\ndef on_orchestrator_event(self, event: OrchestratorEvent) -\u003e None:\n    match event.type:\n        case 'phase_changed':\n            self.update_phase(event.data['phase'])\n        case 'melder_stream':\n            self.panels['melder'].append_content(event.data)\n        case 'advisor_stream':\n            panel = event.data['provider']\n            self.panels[panel].append_content(event.data)\n```\n\n## Acceptance Criteria\n- [ ] 4-panel layout renders correctly\n- [ ] Melder panel spans full width\n- [ ] Advisor panels each 1/3 width\n- [ ] Status indicators update correctly\n- [ ] Streaming content appears in real-time\n- [ ] Elapsed time updates\n- [ ] Phase indicator shows current state\n- [ ] Terminal resize handled gracefully\n- [ ] No flicker during rapid updates\n- [ ] Footer shows session info\n\n## Performance Notes\n- Throttle updates to 60fps max\n- Batch rapid content updates\n- Use Textual's reactive properties\n- Don't block event loop during render","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:28:01.754547-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:28:01.754547-06:00","labels":["phase-3","textual","tui"],"dependencies":[{"issue_id":"meld-eq0.9","depends_on_id":"meld-eq0","type":"parent-child","created_at":"2026-01-15T23:28:01.761185-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.9","depends_on_id":"meld-eq0.8","type":"blocks","created_at":"2026-01-15T23:45:06.137857-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.9.1","title":"9.1: Create Textual app structure and layout","description":"# Subtask 9.1: Textual App Structure\n\n## What to Implement\nBasic Textual application with 4-panel layout.\n\n## Location\n`meld/tui.py`\n\n## Implementation\n\n```python\nfrom textual.app import App, ComposeResult\nfrom textual.containers import Container, Horizontal, Vertical\nfrom textual.widgets import Header, Footer, Static, RichLog\nfrom textual.reactive import reactive\nfrom textual import on\nfrom rich.text import Text\n\n\nclass StatusBar(Static):\n    \"\"\"Status bar at bottom of screen.\"\"\"\n    \n    session_time = reactive('0:00')\n    current_round = reactive(0)\n    max_rounds = reactive(5)\n    activity = reactive('○')\n    \n    def render(self) -\u003e Text:\n        return Text.assemble(\n            ('Session: ', 'dim'),\n            (self.session_time, 'bold'),\n            (' | ', 'dim'),\n            (f'Round {self.current_round}/{self.max_rounds}', 'bold'),\n            (' | ', 'dim'),\n            (self.activity, 'bold cyan'),\n        )\n\n\nclass MeldPanel(Container):\n    \"\"\"\n    Panel for displaying model output.\n    \n    Shows:\n    - Header with provider name and status\n    - Scrollable content area\n    - Elapsed time\n    \"\"\"\n    \n    DEFAULT_CSS = '''\n    MeldPanel {\n        border: solid ;\n        height: 100%;\n    }\n    \n    MeldPanel.melder {\n        height: 60%;\n    }\n    \n    MeldPanel.advisor {\n        height: 40%;\n        width: 1fr;\n    }\n    \n    .panel-header {\n        height: 1;\n        background: ;\n        padding: 0 1;\n    }\n    \n    .panel-content {\n        height: 1fr;\n        overflow-y: auto;\n    }\n    '''\n    \n    status = reactive('waiting')  # waiting, running, streaming, complete, failed, retrying\n    elapsed_seconds = reactive(0.0)\n    phase_info = reactive('')\n    \n    STATUS_ICONS = {\n        'waiting': ('○', 'dim'),\n        'running': ('◐', 'yellow'),\n        'streaming': ('▌', 'cyan'),\n        'complete': ('●', 'green'),\n        'failed': ('✗', 'red'),\n        'retrying': ('↻', 'yellow'),\n    }\n    \n    def __init__(self, provider: str, **kwargs):\n        super().__init__(**kwargs)\n        self.provider = provider\n        self._content = RichLog(highlight=True, markup=True)\n    \n    def compose(self) -\u003e ComposeResult:\n        with Vertical():\n            yield Static(id='header', classes='panel-header')\n            yield self._content\n    \n    def watch_status(self, status: str) -\u003e None:\n        self._update_header()\n    \n    def watch_elapsed_seconds(self, elapsed: float) -\u003e None:\n        self._update_header()\n    \n    def _update_header(self) -\u003e None:\n        icon, color = self.STATUS_ICONS.get(self.status, ('?', 'white'))\n        \n        minutes = int(self.elapsed_seconds // 60)\n        seconds = int(self.elapsed_seconds % 60)\n        time_str = f'{minutes}m {seconds:02d}s' if minutes else f'{seconds}s'\n        \n        header = self.query_one('#header', Static)\n        header.update(Text.assemble(\n            (self.provider.upper(), 'bold'),\n            ' ',\n            (icon, color),\n            ' ',\n            (time_str, 'dim'),\n            ' ',\n            (self.phase_info, 'italic dim'),\n        ))\n    \n    def append_content(self, text: str) -\u003e None:\n        \"\"\"Append text to content area.\"\"\"\n        self._content.write(text)\n    \n    def clear_content(self) -\u003e None:\n        \"\"\"Clear content area.\"\"\"\n        self._content.clear()\n\n\nclass MeldApp(App):\n    \"\"\"\n    Main Meld TUI application.\n    \n    Layout:\n    - Top: Melder panel (full width, 60% height)\n    - Bottom: Three advisor panels (1/3 width each, 40% height)\n    - Footer: Status bar\n    \"\"\"\n    \n    CSS = '''\n    Screen {\n        layout: vertical;\n    }\n    \n    #advisor-row {\n        height: 40%;\n        layout: horizontal;\n    }\n    \n    #melder-panel {\n        height: 60%;\n    }\n    '''\n    \n    BINDINGS = [\n        ('q', 'quit', 'Quit'),\n        ('ctrl+c', 'quit', 'Quit'),\n    ]\n    \n    def __init__(self):\n        super().__init__()\n        self.panels: dict[str, MeldPanel] = {}\n    \n    def compose(self) -\u003e ComposeResult:\n        yield Header(show_clock=True)\n        \n        # Melder panel (top, full width)\n        melder = MeldPanel('melder', id='melder-panel', classes='melder')\n        self.panels['melder'] = melder\n        yield melder\n        \n        # Advisor panels (bottom row)\n        with Horizontal(id='advisor-row'):\n            for provider in ['claude', 'gemini', 'codex']:\n                panel = MeldPanel(provider, classes='advisor')\n                self.panels[provider] = panel\n                yield panel\n        \n        yield StatusBar(id='status-bar')\n        yield Footer()\n    \n    def update_panel(self, provider: str, text: str) -\u003e None:\n        \"\"\"Append text to a panel.\"\"\"\n        if provider in self.panels:\n            self.panels[provider].append_content(text)\n    \n    def set_panel_status(self, provider: str, status: str) -\u003e None:\n        \"\"\"Update panel status.\"\"\"\n        if provider in self.panels:\n            self.panels[provider].status = status\n    \n    def update_round(self, current: int, max_rounds: int) -\u003e None:\n        \"\"\"Update round display.\"\"\"\n        status_bar = self.query_one('#status-bar', StatusBar)\n        status_bar.current_round = current\n        status_bar.max_rounds = max_rounds\n```\n\n## Acceptance Criteria\n- [ ] App starts without errors\n- [ ] Layout matches specification\n- [ ] Melder panel 60% height, full width\n- [ ] Advisor panels 40% height, 1/3 each\n- [ ] Status bar visible at bottom\n- [ ] 'q' or Ctrl+C quits\n- [ ] Panels scrollable\n- [ ] Terminal resize works","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:28:48.912844-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:28:48.912844-06:00","labels":["layout","textual","tui"],"dependencies":[{"issue_id":"meld-eq0.9.1","depends_on_id":"meld-eq0.9","type":"parent-child","created_at":"2026-01-15T23:28:48.915249-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.9.2","title":"9.2: Implement streaming content display with throttling","description":"# Subtask 9.2: Streaming Display \u0026 Throttling\n\n## What to Implement\nReal-time content streaming with throttling to prevent render thrashing.\n\n## Location\n`meld/tui.py` (continued)\n\n## The Problem\nWithout throttling:\n- Each character triggers a render\n- 1000 chars/sec = 1000 renders/sec\n- UI becomes unresponsive\n\nWith throttling:\n- Batch updates in 16ms windows (~60fps)\n- Smooth, responsive UI\n\n## Implementation\n\n```python\nimport asyncio\nfrom collections import defaultdict\nfrom time import monotonic\n\n\nclass StreamBuffer:\n    \"\"\"\n    Buffers streaming content and flushes periodically.\n    \n    Collects content chunks and flushes to panel\n    at most every 16ms (60fps).\n    \"\"\"\n    \n    def __init__(self, flush_callback, interval_ms: float = 16.0):\n        self.flush_callback = flush_callback\n        self.interval = interval_ms / 1000.0\n        self.buffer: dict[str, list[str]] = defaultdict(list)\n        self.last_flush = monotonic()\n        self._flush_task: asyncio.Task | None = None\n    \n    def append(self, provider: str, text: str) -\u003e None:\n        \"\"\"Add text to buffer for provider.\"\"\"\n        self.buffer[provider].append(text)\n        self._schedule_flush()\n    \n    def _schedule_flush(self) -\u003e None:\n        \"\"\"Schedule flush if not already scheduled.\"\"\"\n        if self._flush_task is None or self._flush_task.done():\n            self._flush_task = asyncio.create_task(self._flush_after_delay())\n    \n    async def _flush_after_delay(self) -\u003e None:\n        \"\"\"Wait for interval then flush.\"\"\"\n        elapsed = monotonic() - self.last_flush\n        remaining = self.interval - elapsed\n        \n        if remaining \u003e 0:\n            await asyncio.sleep(remaining)\n        \n        self._flush()\n    \n    def _flush(self) -\u003e None:\n        \"\"\"Flush all buffered content.\"\"\"\n        self.last_flush = monotonic()\n        \n        for provider, chunks in self.buffer.items():\n            if chunks:\n                content = ''.join(chunks)\n                self.flush_callback(provider, content)\n        \n        self.buffer.clear()\n    \n    def flush_now(self) -\u003e None:\n        \"\"\"Force immediate flush.\"\"\"\n        self._flush()\n\n\nclass MeldApp(App):\n    # ... existing code ...\n    \n    def __init__(self):\n        super().__init__()\n        self.panels: dict[str, MeldPanel] = {}\n        self._stream_buffer = StreamBuffer(self._on_buffer_flush)\n        self._elapsed_timers: dict[str, float] = {}\n        self._timer_task: asyncio.Task | None = None\n    \n    def _on_buffer_flush(self, provider: str, content: str) -\u003e None:\n        \"\"\"Called when stream buffer flushes.\"\"\"\n        if provider in self.panels:\n            self.panels[provider].append_content(content)\n    \n    async def handle_stream_event(\n        self,\n        provider: str,\n        event: StreamEvent,\n    ) -\u003e None:\n        \"\"\"\n        Handle streaming event from orchestrator.\n        \n        Buffers content updates for throttled display.\n        \"\"\"\n        match event.type:\n            case StreamEventType.TEXT:\n                self._stream_buffer.append(provider, event.content)\n                self.panels[provider].status = 'streaming'\n                \n            case StreamEventType.DONE:\n                self._stream_buffer.flush_now()\n                self.panels[provider].status = 'complete'\n                \n            case StreamEventType.ERROR:\n                self._stream_buffer.flush_now()\n                self.panels[provider].append_content(\n                    f'\\n[red]Error: {event.content}[/red]'\n                )\n                self.panels[provider].status = 'failed'\n    \n    async def _update_elapsed_times(self) -\u003e None:\n        \"\"\"Background task to update elapsed times.\"\"\"\n        while True:\n            await asyncio.sleep(1.0)\n            \n            for provider, start_time in self._elapsed_timers.items():\n                if provider in self.panels:\n                    elapsed = monotonic() - start_time\n                    self.panels[provider].elapsed_seconds = elapsed\n    \n    def start_timer(self, provider: str) -\u003e None:\n        \"\"\"Start elapsed timer for provider.\"\"\"\n        self._elapsed_timers[provider] = monotonic()\n        \n        if self._timer_task is None:\n            self._timer_task = asyncio.create_task(self._update_elapsed_times())\n    \n    def stop_timer(self, provider: str) -\u003e None:\n        \"\"\"Stop elapsed timer for provider.\"\"\"\n        if provider in self._elapsed_timers:\n            del self._elapsed_timers[provider]\n```\n\n## Integration with Orchestrator\n\n```python\n# In orchestrator or CLI\nasync def run_with_tui(app: MeldApp, orchestrator: Orchestrator, ...):\n    def on_event(event: OrchestratorEvent):\n        match event.type:\n            case 'phase_changed':\n                phase = event.data['phase']\n                if phase == 'planning':\n                    app.panels['melder'].phase_info = '[Planning]'\n                    app.panels['melder'].status = 'running'\n                    app.start_timer('melder')\n                elif phase == 'feedback':\n                    for p in ['claude', 'gemini', 'codex']:\n                        app.panels[p].status = 'running'\n                        app.start_timer(p)\n                        \n            case 'melder_stream':\n                event_data = event.data['event']\n                asyncio.create_task(\n                    app.handle_stream_event('melder', event_data)\n                )\n    \n    orchestrator.set_event_callback(on_event)\n    \n    async with app.run_async():\n        result = await orchestrator.run(task, prd)\n    \n    return result\n```\n\n## Acceptance Criteria\n- [ ] Content streams in real-time\n- [ ] No visible lag or stutter\n- [ ] Updates capped at ~60fps\n- [ ] Multiple panels update simultaneously\n- [ ] Elapsed timers update each second\n- [ ] Status changes reflected immediately\n- [ ] Error messages displayed in red","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:29:50.111795-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:29:50.111795-06:00","labels":["streaming","throttling","tui"],"dependencies":[{"issue_id":"meld-eq0.9.2","depends_on_id":"meld-eq0.9","type":"parent-child","created_at":"2026-01-15T23:29:50.114427-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.9.2","depends_on_id":"meld-eq0.9.1","type":"blocks","created_at":"2026-01-15T23:45:06.256209-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.9.2","depends_on_id":"meld-eq0.8.1","type":"blocks","created_at":"2026-01-15T23:45:06.350073-06:00","created_by":"Michael Fork"}]}
{"id":"meld-eq0.9.3","title":"9.3: Implement status indicators and phase display","description":"# Subtask 9.3: Status Indicators \u0026 Phase Display\n\n## What to Implement\nVisual status indicators for each panel and phase information display.\n\n## Location\n`meld/tui.py` (continued)\n\n## Status State Machine\n\n```\n            start()\n               ↓\nWAITING ──→ RUNNING ──→ STREAMING ──→ COMPLETE\n               ↓              ↓\n            RETRYING ──→ FAILED\n```\n\n## Implementation\n\n```python\nfrom enum import Enum\nfrom rich.spinner import Spinner\n\n\nclass PanelStatus(str, Enum):\n    WAITING = 'waiting'\n    RUNNING = 'running'\n    STREAMING = 'streaming'\n    COMPLETE = 'complete'\n    FAILED = 'failed'\n    RETRYING = 'retrying'\n\n\n# Status configuration\nSTATUS_CONFIG = {\n    PanelStatus.WAITING: {\n        'icon': '○',\n        'color': 'dim white',\n        'label': 'Waiting',\n    },\n    PanelStatus.RUNNING: {\n        'icon': '◐',\n        'color': 'yellow',\n        'label': 'Running',\n        'animate': True,\n    },\n    PanelStatus.STREAMING: {\n        'icon': '▌',\n        'color': 'cyan',\n        'label': 'Streaming',\n        'animate': True,\n    },\n    PanelStatus.COMPLETE: {\n        'icon': '●',\n        'color': 'green',\n        'label': 'Complete',\n    },\n    PanelStatus.FAILED: {\n        'icon': '✗',\n        'color': 'red',\n        'label': 'Failed',\n    },\n    PanelStatus.RETRYING: {\n        'icon': '↻',\n        'color': 'yellow',\n        'label': 'Retrying',\n        'animate': True,\n    },\n}\n\n\nclass StatusIndicator(Static):\n    \"\"\"Animated status indicator widget.\"\"\"\n    \n    status = reactive(PanelStatus.WAITING)\n    retry_count = reactive(0)\n    max_retries = reactive(0)\n    \n    ANIMATION_FRAMES = ['◐', '◓', '◑', '◒']\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self._frame = 0\n        self._timer = None\n    \n    def watch_status(self, status: PanelStatus) -\u003e None:\n        config = STATUS_CONFIG.get(status, {})\n        \n        # Start/stop animation\n        if config.get('animate'):\n            if self._timer is None:\n                self._timer = self.set_interval(0.2, self._animate)\n        else:\n            if self._timer:\n                self._timer.stop()\n                self._timer = None\n        \n        self._update_display()\n    \n    def _animate(self) -\u003e None:\n        self._frame = (self._frame + 1) % len(self.ANIMATION_FRAMES)\n        self._update_display()\n    \n    def _update_display(self) -\u003e None:\n        config = STATUS_CONFIG.get(self.status, {})\n        \n        # Get icon (animated or static)\n        if config.get('animate'):\n            icon = self.ANIMATION_FRAMES[self._frame]\n        else:\n            icon = config.get('icon', '?')\n        \n        color = config.get('color', 'white')\n        label = config.get('label', '')\n        \n        # Build display text\n        text = Text()\n        text.append(icon, style=color)\n        text.append(' ', style='')\n        text.append(label, style=color)\n        \n        # Show retry info if applicable\n        if self.status == PanelStatus.RETRYING and self.max_retries \u003e 0:\n            text.append(f' ({self.retry_count}/{self.max_retries})', style='dim')\n        \n        self.update(text)\n\n\nclass PhaseDisplay(Static):\n    \"\"\"Shows current phase in Melder panel.\"\"\"\n    \n    phase = reactive('planning')\n    round_num = reactive(0)\n    max_rounds = reactive(5)\n    \n    PHASE_LABELS = {\n        'planning': '[Planning]',\n        'feedback': '[Feedback Round {round}/{max}]',\n        'synthesis': '[Synthesizing]',\n        'converged': '[Converged ✓]',\n        'max_rounds': '[Max Rounds]',\n    }\n    \n    def watch_phase(self, phase: str) -\u003e None:\n        self._update_display()\n    \n    def watch_round_num(self, _: int) -\u003e None:\n        self._update_display()\n    \n    def _update_display(self) -\u003e None:\n        template = self.PHASE_LABELS.get(self.phase, '[Unknown]')\n        \n        if '{round}' in template:\n            label = template.format(\n                round=self.round_num,\n                max=self.max_rounds,\n            )\n        else:\n            label = template\n        \n        # Color based on phase\n        if self.phase == 'converged':\n            style = 'green bold'\n        elif self.phase == 'max_rounds':\n            style = 'yellow'\n        else:\n            style = 'cyan'\n        \n        self.update(Text(label, style=style))\n\n\nclass MeldPanel(Container):\n    \"\"\"Updated panel with status indicator and phase display.\"\"\"\n    \n    def __init__(self, provider: str, is_melder: bool = False, **kwargs):\n        super().__init__(**kwargs)\n        self.provider = provider\n        self.is_melder = is_melder\n        self._status_indicator = StatusIndicator()\n        self._phase_display = PhaseDisplay() if is_melder else None\n        self._content = RichLog(highlight=True, markup=True)\n    \n    def compose(self) -\u003e ComposeResult:\n        with Horizontal(classes='panel-header'):\n            yield Static(self.provider.upper(), classes='panel-title')\n            yield self._status_indicator\n            if self._phase_display:\n                yield self._phase_display\n        yield self._content\n    \n    def set_status(self, status: PanelStatus) -\u003e None:\n        self._status_indicator.status = status\n    \n    def set_retry_info(self, current: int, max_retries: int) -\u003e None:\n        self._status_indicator.retry_count = current\n        self._status_indicator.max_retries = max_retries\n    \n    def set_phase(self, phase: str, round_num: int = 0, max_rounds: int = 5) -\u003e None:\n        if self._phase_display:\n            self._phase_display.phase = phase\n            self._phase_display.round_num = round_num\n            self._phase_display.max_rounds = max_rounds\n```\n\n## Acceptance Criteria\n- [ ] All 6 status states render correctly\n- [ ] Running/streaming states animate\n- [ ] Animation smooth (~5fps)\n- [ ] Retry counter shows attempt info\n- [ ] Phase display updates correctly\n- [ ] Colors match specification\n- [ ] Icons visible in all terminals","status":"open","priority":1,"issue_type":"task","owner":"mjfork@users.noreply.github.com","created_at":"2026-01-15T23:30:58.697735-06:00","created_by":"Michael Fork","updated_at":"2026-01-15T23:30:58.697735-06:00","labels":["indicators","status","tui"],"dependencies":[{"issue_id":"meld-eq0.9.3","depends_on_id":"meld-eq0.9","type":"parent-child","created_at":"2026-01-15T23:30:58.701965-06:00","created_by":"Michael Fork"},{"issue_id":"meld-eq0.9.3","depends_on_id":"meld-eq0.9.1","type":"blocks","created_at":"2026-01-15T23:45:06.454244-06:00","created_by":"Michael Fork"}]}
